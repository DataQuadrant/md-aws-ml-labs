{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie genre prediction with Object2Vec Algorithm\n",
    "### MD: modified to work on a local container\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Install and import dependencies](#Install-and-import-dependencies)\n",
    "3. [Preprocessing](#Preprocessing)\n",
    "  1. [Build the vocabulary](#Build-the-vocabulary)\n",
    "  2. [Split data into train, validation and test](#Split-data-into-train,-validation-and-test)\n",
    "  3. [Negative sampling](#Negative-sampling)\n",
    "  4. [Tokenization](#Tokenization)\n",
    "  5. [Download pretrained word embeddings](#Download-pretrained-word-embeddings)\n",
    "4. [Sagemaker Training](#Sagemaker-Training)\n",
    "  1. [Upload data to S3](#Upload-data-to-S3)\n",
    "  1. [Training hyperparameters](#Training-hyperparameters)\n",
    "5. [Evaluation with Batch inference](#Evaluation-with-Batch-inference)\n",
    "6. [Online inference demo](#Online-inference-demo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, we will explore how ObjectToVec algorithm can be used in a multi label prediction setting \n",
    "to predict the genre of a movie from its plot description. We will be using a dataset provided from imdb.\n",
    "\n",
    "\n",
    "At a high level, the network architecture that we use for this task is illustrated in the diagram below.\n",
    "\n",
    "<img src=\"image.png\" width=\"500\">\n",
    "\n",
    "We cast the problem of multi-label prediction as a binary classification problem. A positive example is the tuple of movie plot description, and a movie genre that applies to the movie in the labeled data. If a movie has multiple genres, we create multiple positive examples for the movie, one for each genre. A negative example is a pair where the genre does not apply to the movie. The negative examples are generated by picking a random subset of genres which do not apply to the movie, as determined by the labeled dataset.\n",
    "\n",
    "Let us first start with downloading the data.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "Important: Before you begin downloading, please read the following README file using your browser and make sure you are okay with the license.\n",
    "ftp://ftp.fu-berlin.de/pub/misc/movies/database/frozendata/README\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-05-23 13:38:55--  ftp://ftp.fu-berlin.de/pub/misc/movies/database/frozendata/genres.list.gz\n",
      "           => ‘genres.list.gz’\n",
      "Resolving ftp.fu-berlin.de (ftp.fu-berlin.de)... 130.133.3.130\n",
      "Connecting to ftp.fu-berlin.de (ftp.fu-berlin.de)|130.133.3.130|:21... connected.\n",
      "Logging in as anonymous ... Logged in!\n",
      "==> SYST ... done.    ==> PWD ... done.\n",
      "==> TYPE I ... done.  ==> CWD (1) /pub/misc/movies/database/frozendata ... done.\n",
      "==> SIZE genres.list.gz ... 20525974\n",
      "==> PASV ... done.    ==> RETR genres.list.gz ... done.\n",
      "Length: 20525974 (20M) (unauthoritative)\n",
      "\n",
      "genres.list.gz      100%[===================>]  19.57M  1.51MB/s    in 19s     \n",
      "\n",
      "2020-05-23 13:39:18 (1.03 MB/s) - ‘genres.list.gz’ saved [20525974]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !wget ftp://ftp.fu-berlin.de/pub/misc/movies/database/frozendata/genres.list.gz\n",
    "# !gunzip genres.list.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-05-23 13:39:43--  ftp://ftp.fu-berlin.de/pub/misc/movies/database/frozendata/plot.list.gz\n",
      "           => ‘plot.list.gz’\n",
      "Resolving ftp.fu-berlin.de (ftp.fu-berlin.de)... 130.133.3.130\n",
      "Connecting to ftp.fu-berlin.de (ftp.fu-berlin.de)|130.133.3.130|:21... connected.\n",
      "Logging in as anonymous ... Logged in!\n",
      "==> SYST ... done.    ==> PWD ... done.\n",
      "==> TYPE I ... done.  ==> CWD (1) /pub/misc/movies/database/frozendata ... done.\n",
      "==> SIZE plot.list.gz ... 159742723\n",
      "==> PASV ... done.    ==> RETR plot.list.gz ... done.\n",
      "Length: 159742723 (152M) (unauthoritative)\n",
      "\n",
      "plot.list.gz        100%[===================>] 152.34M  2.00MB/s    in 1m 55s  \n",
      "\n",
      "2020-05-23 13:41:43 (1.33 MB/s) - ‘plot.list.gz’ saved [159742723]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !wget ftp://ftp.fu-berlin.de/pub/misc/movies/database/frozendata/plot.list.gz\n",
    "# !gunzip plot.list.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/ec2-user/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.8.tar.gz (981 kB)\n",
      "\u001b[K     |████████████████████████████████| 981 kB 335 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from langdetect) (1.11.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.8-py3-none-any.whl size=993191 sha256=2a9dc7ab8939bb14500daa8be0a522fe7d855c20abd66bb15ed5f0c4473ae627\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-fmehe9hd/wheels/53/88/5d/b239dc55d773b01fdd2059606b1a8f4b64548848b8f6e381c3\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.8\n"
     ]
    }
   ],
   "source": [
    "! {sys.prefix}/bin/pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/ec2-user/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.5.zip (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 507 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting click\n",
      "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "\u001b[K     |████████████████████████████████| 82 kB 399 kB/s eta 0:00:011\n",
      "\u001b[?25hCollecting joblib\n",
      "  Downloading joblib-0.15.1-py3-none-any.whl (298 kB)\n",
      "\u001b[K     |████████████████████████████████| 298 kB 488 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex\n",
      "  Downloading regex-2020.5.14-cp36-cp36m-manylinux2010_x86_64.whl (675 kB)\n",
      "\u001b[K     |████████████████████████████████| 675 kB 1.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm\n",
      "  Downloading tqdm-4.46.0-py2.py3-none-any.whl (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 1.8 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434676 sha256=690abbe3bbb2c9cf4946a5b7ac88ce2e5ecc1de3299f9264c541f939584863a2\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-n38s_135/wheels/de/5e/42/64abaeca668161c3e2cecc24f864a8fc421e3d07a104fc8a51\n",
      "Successfully built nltk\n",
      "Installing collected packages: click, joblib, regex, tqdm, nltk\n",
      "Successfully installed click-7.1.2 joblib-0.15.1 nltk-3.5 regex-2020.5.14 tqdm-4.46.0\n"
     ]
    }
   ],
   "source": [
    "! {sys.prefix}/bin/pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.5.12\n",
      "  latest version: 4.8.3\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/ec2-user/anaconda3\n",
      "\n",
      "  added / updated specs: \n",
      "    - sqlite\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    openssl-1.1.1g             |       h516909a_0         2.1 MB  conda-forge\n",
      "    certifi-2020.4.5.1         |   py37hc8dfbb8_0         151 KB  conda-forge\n",
      "    python_abi-3.7             |          1_cp37m           4 KB  conda-forge\n",
      "    ca-certificates-2020.4.5.1 |       hecc5488_0         146 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         2.4 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "    python_abi:      3.7-1_cp37m           conda-forge\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "    ca-certificates: 2019.11.28-hecc5488_0 conda-forge --> 2020.4.5.1-hecc5488_0     conda-forge\n",
      "    certifi:         2019.11.28-py37_0     conda-forge --> 2020.4.5.1-py37hc8dfbb8_0 conda-forge\n",
      "    openssl:         1.1.1d-h516909a_0     conda-forge --> 1.1.1g-h516909a_0         conda-forge\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "openssl-1.1.1g       | 2.1 MB    | ##################################### | 100% \n",
      "certifi-2020.4.5.1   | 151 KB    | ##################################### | 100% \n",
      "python_abi-3.7       | 4 KB      | ##################################### | 100% \n",
      "ca-certificates-2020 | 146 KB    | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "!conda upgrade -y sqlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/ec2-user/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Requirement already satisfied: jsonlines in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (1.2.0)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from jsonlines) (1.11.0)\n"
     ]
    }
   ],
   "source": [
    "! {sys.prefix}/bin/pip install jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from itertools import chain, islice\n",
    "\n",
    "import boto3\n",
    "import jsonlines\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "import seaborn as sns\n",
    "\n",
    "from langdetect import detect\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TreebankWordTokenizer, sent_tokenize\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "from sagemaker.session import s3_input\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "arn:aws:iam::558157414092:role/service-role/AmazonSageMaker-ExecutionRole-20200523T082014\n"
    }
   ],
   "source": [
    "# # execute this on aws sagemaker\n",
    "# role = get_execution_role()\n",
    "\n",
    "# use this if running sagemaker locally\n",
    "def resolve_sm_role():\n",
    "    client = boto3.client('iam', region_name='us-east-2')\n",
    "    response_roles = client.list_roles(\n",
    "        PathPrefix='/',\n",
    "        # Marker='string',\n",
    "        MaxItems=999\n",
    "    )\n",
    "    for role in response_roles['Roles']:\n",
    "        if role['RoleName'].startswith('AmazonSageMaker-ExecutionRole-'):\n",
    "#             print('Resolved SageMaker IAM Role to: ' + str(role))\n",
    "            return role['Arn']\n",
    "    raise Exception('Could not resolve what should be the SageMaker role to be used')\n",
    "\n",
    "# this is the role created by sagemaker notebook on aws\n",
    "role_arn = resolve_sm_role()\n",
    "print(role_arn)\n",
    "role=role_arn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'list'>\n[0, 0, 0, 0, 0]\n"
    }
   ],
   "source": [
    "row = [0] * 5\n",
    "print(type(row))\n",
    "print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['Action', 'Adult', 'Adventure', 'Animation', 'Biography', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Family', 'Fantasy', 'Game-Show', 'History', 'Horror', 'Lifestyle', 'Music', 'Musical', 'Mystery', 'News', 'Reality-TV', 'Reality-tv', 'Romance', 'Sci-Fi', 'Sci-fi', 'Short', 'Sport', 'Talk-Show', 'Thriller', 'War', 'Western']\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                         short_title  Action  Adult  \\\n0                                    \"!Next?\" (1994)       0      0   \n1                                 \"#1 Single\" (2006)       0      0   \n2                            \"#15SecondScare\" (2015)       0      0   \n3  \"#15SecondScare\" (2015) {Who Wants to Play wit...       0      0   \n4                         \"#1MinuteNightmare\" (2014)       0      0   \n\n   Adventure  Animation  Biography  Comedy  Crime  Documentary  Drama  ...  \\\n0          0          0          0       0      0            1      0  ...   \n1          0          0          0       0      0            0      0  ...   \n2          0          0          0       0      0            0      0  ...   \n3          0          0          0       0      0            0      1  ...   \n4          0          0          0       0      0            0      0  ...   \n\n   Reality-tv  Romance  Sci-Fi  Sci-fi  Short  Sport  Talk-Show  Thriller  \\\n0           0        0       0       0      0      0          0         0   \n1           0        0       0       0      0      0          0         0   \n2           0        0       0       0      1      0          0         1   \n3           0        0       0       0      1      0          0         1   \n4           0        0       0       0      0      0          0         0   \n\n   War  Western  \n0    0        0  \n1    0        0  \n2    0        0  \n3    0        0  \n4    0        0  \n\n[5 rows x 31 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>short_title</th>\n      <th>Action</th>\n      <th>Adult</th>\n      <th>Adventure</th>\n      <th>Animation</th>\n      <th>Biography</th>\n      <th>Comedy</th>\n      <th>Crime</th>\n      <th>Documentary</th>\n      <th>Drama</th>\n      <th>...</th>\n      <th>Reality-tv</th>\n      <th>Romance</th>\n      <th>Sci-Fi</th>\n      <th>Sci-fi</th>\n      <th>Short</th>\n      <th>Sport</th>\n      <th>Talk-Show</th>\n      <th>Thriller</th>\n      <th>War</th>\n      <th>Western</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>\"!Next?\" (1994)</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>\"#1 Single\" (2006)</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>\"#15SecondScare\" (2015)</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>\"#15SecondScare\" (2015) {Who Wants to Play wit...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>\"#1MinuteNightmare\" (2014)</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 31 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "def get_genres(filename):\n",
    "    \n",
    "    genres = defaultdict(list)\n",
    "    unique_genres = set()\n",
    "    \n",
    "    with open(filename, \"r\", errors='ignore') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('\"'):\n",
    "                \n",
    "                data = line.split('\\t')\n",
    "                \n",
    "                movie = data[0]\n",
    "                genre = data[-1].strip()\n",
    "                \n",
    "                genres[movie].append(genre)\n",
    "                unique_genres.add(genre)\n",
    "                \n",
    "    unique_genres = sorted(unique_genres)\n",
    "    print(unique_genres)\n",
    "    \n",
    "    # md: do a one hot encoding for movies and genres\n",
    "    data = []\n",
    "    for movie in genres:\n",
    "        \n",
    "        # md: create a list with dimension equal to number of genres, each element equal to 0\n",
    "        row = [0]*len(unique_genres)\n",
    "        \n",
    "        for g in genres[movie]:\n",
    "            row[unique_genres.index(g)] = 1\n",
    "            \n",
    "        row.insert(0, movie)\n",
    "        data.append(row)\n",
    "        \n",
    "    genres_df = pd.DataFrame(data)\n",
    "    genres_df.columns = ['short_title'] + unique_genres\n",
    "    return genres_df\n",
    "    \n",
    "genres_df = get_genres(\"genres.list\")\n",
    "genres_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "            short_title                                              title  \\\n0  \"#7DaysLater\" (2013)                               \"#7DaysLater\" (2013)   \n1   \"#BlackLove\" (2015)       \"#BlackLove\" (2015) {Crash the Party (#1.9)}   \n2   \"#BlackLove\" (2015)  \"#BlackLove\" (2015) {Making Lemonade Out of Le...   \n3   \"#BlackLove\" (2015)      \"#BlackLove\" (2015) {Miss Independent (#1.5)}   \n4   \"#BlackLove\" (2015)     \"#BlackLove\" (2015) {Sealing the Deal (#1.10)}   \n\n                                                plot  \n0   #7dayslater is an interactive comedy series f...  \n1   With just one week left in the workshops, the...  \n2   All of the women start making strides towards...  \n3   All five of these women are independent and s...  \n4   Despite having gone through a life changing p...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>short_title</th>\n      <th>title</th>\n      <th>plot</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>\"#7DaysLater\" (2013)</td>\n      <td>\"#7DaysLater\" (2013)</td>\n      <td>#7dayslater is an interactive comedy series f...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>\"#BlackLove\" (2015)</td>\n      <td>\"#BlackLove\" (2015) {Crash the Party (#1.9)}</td>\n      <td>With just one week left in the workshops, the...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>\"#BlackLove\" (2015)</td>\n      <td>\"#BlackLove\" (2015) {Making Lemonade Out of Le...</td>\n      <td>All of the women start making strides towards...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>\"#BlackLove\" (2015)</td>\n      <td>\"#BlackLove\" (2015) {Miss Independent (#1.5)}</td>\n      <td>All five of these women are independent and s...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>\"#BlackLove\" (2015)</td>\n      <td>\"#BlackLove\" (2015) {Sealing the Deal (#1.10)}</td>\n      <td>Despite having gone through a life changing p...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "def get_plots(filename):\n",
    "    \n",
    "    with open(filename, \"r\", errors='ignore') as f:\n",
    "        data = []\n",
    "        inside = False\n",
    "        plot = ''\n",
    "        full_title = ''\n",
    "        for line in f:\n",
    "            if line.startswith(\"MV:\") and not inside:\n",
    "                inside = True\n",
    "                full_title = line.split(\"MV:\")[1].strip()\n",
    "\n",
    "            elif line.startswith(\"PL:\") and inside:\n",
    "                plot += line.split(\"PL:\")[1].replace(\"\\n\", \"\")\n",
    "\n",
    "            elif line.startswith(\"MV:\") and inside:\n",
    "                short_title = full_title.split('{')[0].strip()\n",
    "                data.append((short_title, full_title, plot))\n",
    "                plot = ''\n",
    "                inside = False\n",
    "    plots_df = pd.DataFrame(data)\n",
    "    plots_df.columns = ['short_title', 'title', 'plot']\n",
    "    return plots_df\n",
    "\n",
    "plots_df = get_plots(\"plot.list\")\n",
    "plots_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now join the genre and the plot dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>plot</th>\n",
       "      <th>Action</th>\n",
       "      <th>Adult</th>\n",
       "      <th>Adventure</th>\n",
       "      <th>Animation</th>\n",
       "      <th>Biography</th>\n",
       "      <th>Comedy</th>\n",
       "      <th>Crime</th>\n",
       "      <th>Documentary</th>\n",
       "      <th>...</th>\n",
       "      <th>Reality-tv</th>\n",
       "      <th>Romance</th>\n",
       "      <th>Sci-Fi</th>\n",
       "      <th>Sci-fi</th>\n",
       "      <th>Short</th>\n",
       "      <th>Sport</th>\n",
       "      <th>Talk-Show</th>\n",
       "      <th>Thriller</th>\n",
       "      <th>War</th>\n",
       "      <th>Western</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"#7DaysLater\" (2013)</td>\n",
       "      <td>#7dayslater is an interactive comedy series f...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"#BlackLove\" (2015) {Crash the Party (#1.9)}</td>\n",
       "      <td>With just one week left in the workshops, the...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"#BlackLove\" (2015) {Making Lemonade Out of Le...</td>\n",
       "      <td>All of the women start making strides towards...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"#BlackLove\" (2015) {Miss Independent (#1.5)}</td>\n",
       "      <td>All five of these women are independent and s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"#BlackLove\" (2015) {Sealing the Deal (#1.10)}</td>\n",
       "      <td>Despite having gone through a life changing p...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                               \"#7DaysLater\" (2013)   \n",
       "1       \"#BlackLove\" (2015) {Crash the Party (#1.9)}   \n",
       "2  \"#BlackLove\" (2015) {Making Lemonade Out of Le...   \n",
       "3      \"#BlackLove\" (2015) {Miss Independent (#1.5)}   \n",
       "4     \"#BlackLove\" (2015) {Sealing the Deal (#1.10)}   \n",
       "\n",
       "                                                plot  Action  Adult  \\\n",
       "0   #7dayslater is an interactive comedy series f...       0      0   \n",
       "1   With just one week left in the workshops, the...       0      0   \n",
       "2   All of the women start making strides towards...       0      0   \n",
       "3   All five of these women are independent and s...       0      0   \n",
       "4   Despite having gone through a life changing p...       0      0   \n",
       "\n",
       "   Adventure  Animation  Biography  Comedy  Crime  Documentary  ...  \\\n",
       "0          0          0          0       1      0            0  ...   \n",
       "1          0          0          0       0      0            0  ...   \n",
       "2          0          0          0       0      0            0  ...   \n",
       "3          0          0          0       0      0            0  ...   \n",
       "4          0          0          0       0      0            0  ...   \n",
       "\n",
       "   Reality-tv  Romance  Sci-Fi  Sci-fi  Short  Sport  Talk-Show  Thriller  \\\n",
       "0           0        0       0       0      0      0          0         0   \n",
       "1           0        0       0       0      0      0          0         0   \n",
       "2           0        0       0       0      0      0          0         0   \n",
       "3           0        0       0       0      0      0          0         0   \n",
       "4           0        0       0       0      0      0          0         0   \n",
       "\n",
       "   War  Western  \n",
       "0    0        0  \n",
       "1    0        0  \n",
       "2    0        0  \n",
       "3    0        0  \n",
       "4    0        0  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = plots_df.merge(genres_df, how='inner', on='short_title')\n",
    "data_df.dropna(inplace=True)\n",
    "data_df.drop('short_title', axis=1, inplace=True)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Action',\n",
       " 'Adult',\n",
       " 'Adventure',\n",
       " 'Animation',\n",
       " 'Biography',\n",
       " 'Comedy',\n",
       " 'Crime',\n",
       " 'Documentary',\n",
       " 'Drama',\n",
       " 'Family',\n",
       " 'Fantasy',\n",
       " 'Game-Show',\n",
       " 'History',\n",
       " 'Horror',\n",
       " 'Lifestyle',\n",
       " 'Music',\n",
       " 'Musical',\n",
       " 'Mystery',\n",
       " 'News',\n",
       " 'Reality-TV',\n",
       " 'Reality-tv',\n",
       " 'Romance',\n",
       " 'Sci-Fi',\n",
       " 'Sci-fi',\n",
       " 'Short',\n",
       " 'Sport',\n",
       " 'Talk-Show',\n",
       " 'Thriller',\n",
       " 'War',\n",
       " 'Western']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genres = list(data_df.columns)[2:]\n",
    "genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Action', 13620),\n",
       " ('Adult', 129),\n",
       " ('Adventure', 11353),\n",
       " ('Animation', 12944),\n",
       " ('Biography', 1580),\n",
       " ('Comedy', 37354),\n",
       " ('Crime', 16777),\n",
       " ('Documentary', 13882),\n",
       " ('Drama', 51150),\n",
       " ('Family', 17127),\n",
       " ('Fantasy', 8488),\n",
       " ('Game-Show', 2316),\n",
       " ('History', 3165),\n",
       " ('Horror', 2826),\n",
       " ('Lifestyle', 0),\n",
       " ('Music', 3198),\n",
       " ('Musical', 779),\n",
       " ('Mystery', 12813),\n",
       " ('News', 4719),\n",
       " ('Reality-TV', 13748),\n",
       " ('Reality-tv', 1),\n",
       " ('Romance', 21557),\n",
       " ('Sci-Fi', 9504),\n",
       " ('Sci-fi', 0),\n",
       " ('Short', 858),\n",
       " ('Sport', 2406),\n",
       " ('Talk-Show', 6516),\n",
       " ('Thriller', 9511),\n",
       " ('War', 1534),\n",
       " ('Western', 2841)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = []\n",
    "for genre in genres:\n",
    "    counts.append((genre, data_df[genre].sum()))\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genre</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Action</td>\n",
       "      <td>13620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adult</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adventure</td>\n",
       "      <td>11353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Animation</td>\n",
       "      <td>12944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Biography</td>\n",
       "      <td>1580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Comedy</td>\n",
       "      <td>37354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Crime</td>\n",
       "      <td>16777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Documentary</td>\n",
       "      <td>13882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Drama</td>\n",
       "      <td>51150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Family</td>\n",
       "      <td>17127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Fantasy</td>\n",
       "      <td>8488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Game-Show</td>\n",
       "      <td>2316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>History</td>\n",
       "      <td>3165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Horror</td>\n",
       "      <td>2826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Lifestyle</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Music</td>\n",
       "      <td>3198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Musical</td>\n",
       "      <td>779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Mystery</td>\n",
       "      <td>12813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>News</td>\n",
       "      <td>4719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Reality-TV</td>\n",
       "      <td>13748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Reality-tv</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Romance</td>\n",
       "      <td>21557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Sci-Fi</td>\n",
       "      <td>9504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Sci-fi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Short</td>\n",
       "      <td>858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Sport</td>\n",
       "      <td>2406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Talk-Show</td>\n",
       "      <td>6516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Thriller</td>\n",
       "      <td>9511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>War</td>\n",
       "      <td>1534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Western</td>\n",
       "      <td>2841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          genre  count\n",
       "0        Action  13620\n",
       "1         Adult    129\n",
       "2     Adventure  11353\n",
       "3     Animation  12944\n",
       "4     Biography   1580\n",
       "5        Comedy  37354\n",
       "6         Crime  16777\n",
       "7   Documentary  13882\n",
       "8         Drama  51150\n",
       "9        Family  17127\n",
       "10      Fantasy   8488\n",
       "11    Game-Show   2316\n",
       "12      History   3165\n",
       "13       Horror   2826\n",
       "14    Lifestyle      0\n",
       "15        Music   3198\n",
       "16      Musical    779\n",
       "17      Mystery  12813\n",
       "18         News   4719\n",
       "19   Reality-TV  13748\n",
       "20   Reality-tv      1\n",
       "21      Romance  21557\n",
       "22       Sci-Fi   9504\n",
       "23       Sci-fi      0\n",
       "24        Short    858\n",
       "25        Sport   2406\n",
       "26    Talk-Show   6516\n",
       "27     Thriller   9511\n",
       "28          War   1534\n",
       "29      Western   2841"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distribution = pd.DataFrame(counts, columns=['genre', 'count'])\n",
    "distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the genres with 0 movies\n",
    "data_df.drop('Lifestyle', axis=1, inplace=True)\n",
    "data_df.drop('Sci-fi', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we select all the movies whose description are in English. Note that this will take about 12 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['plot_lang'] = data_df.apply(lambda row: detect(row['plot']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>plot</th>\n",
       "      <th>Action</th>\n",
       "      <th>Adult</th>\n",
       "      <th>Adventure</th>\n",
       "      <th>Animation</th>\n",
       "      <th>Biography</th>\n",
       "      <th>Comedy</th>\n",
       "      <th>Crime</th>\n",
       "      <th>Documentary</th>\n",
       "      <th>...</th>\n",
       "      <th>Reality-tv</th>\n",
       "      <th>Romance</th>\n",
       "      <th>Sci-Fi</th>\n",
       "      <th>Short</th>\n",
       "      <th>Sport</th>\n",
       "      <th>Talk-Show</th>\n",
       "      <th>Thriller</th>\n",
       "      <th>War</th>\n",
       "      <th>Western</th>\n",
       "      <th>plot_lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"#7DaysLater\" (2013)</td>\n",
       "      <td>#7dayslater is an interactive comedy series f...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"#BlackLove\" (2015) {Crash the Party (#1.9)}</td>\n",
       "      <td>With just one week left in the workshops, the...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"#BlackLove\" (2015) {Making Lemonade Out of Le...</td>\n",
       "      <td>All of the women start making strides towards...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"#BlackLove\" (2015) {Miss Independent (#1.5)}</td>\n",
       "      <td>All five of these women are independent and s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"#BlackLove\" (2015) {Sealing the Deal (#1.10)}</td>\n",
       "      <td>Despite having gone through a life changing p...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                               \"#7DaysLater\" (2013)   \n",
       "1       \"#BlackLove\" (2015) {Crash the Party (#1.9)}   \n",
       "2  \"#BlackLove\" (2015) {Making Lemonade Out of Le...   \n",
       "3      \"#BlackLove\" (2015) {Miss Independent (#1.5)}   \n",
       "4     \"#BlackLove\" (2015) {Sealing the Deal (#1.10)}   \n",
       "\n",
       "                                                plot  Action  Adult  \\\n",
       "0   #7dayslater is an interactive comedy series f...       0      0   \n",
       "1   With just one week left in the workshops, the...       0      0   \n",
       "2   All of the women start making strides towards...       0      0   \n",
       "3   All five of these women are independent and s...       0      0   \n",
       "4   Despite having gone through a life changing p...       0      0   \n",
       "\n",
       "   Adventure  Animation  Biography  Comedy  Crime  Documentary  ...  \\\n",
       "0          0          0          0       1      0            0  ...   \n",
       "1          0          0          0       0      0            0  ...   \n",
       "2          0          0          0       0      0            0  ...   \n",
       "3          0          0          0       0      0            0  ...   \n",
       "4          0          0          0       0      0            0  ...   \n",
       "\n",
       "   Reality-tv  Romance  Sci-Fi  Short  Sport  Talk-Show  Thriller  War  \\\n",
       "0           0        0       0      0      0          0         0    0   \n",
       "1           0        0       0      0      0          0         0    0   \n",
       "2           0        0       0      0      0          0         0    0   \n",
       "3           0        0       0      0      0          0         0    0   \n",
       "4           0        0       0      0      0          0         0    0   \n",
       "\n",
       "   Western  plot_lang  \n",
       "0        0         en  \n",
       "1        0         en  \n",
       "2        0         en  \n",
       "3        0         en  \n",
       "4        0         en  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en    131169\n",
       "nl       145\n",
       "fr       113\n",
       "de        36\n",
       "es        10\n",
       "it         7\n",
       "no         3\n",
       "da         3\n",
       "ca         2\n",
       "sv         2\n",
       "sl         1\n",
       "pt         1\n",
       "tl         1\n",
       "hu         1\n",
       "Name: plot_lang, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df['plot_lang'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### select only en types of records and save them to a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_df[data_df.plot_lang.isin(['en'])]\n",
    "df.to_csv(\"movies_genres_en.csv\", sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from now on we can read data from csv and save time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"movies_genres_en.csv\", delimiter='\\t', encoding='utf-8', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the vocabulary\n",
    "\n",
    "Lets define a few functions to tokenize our data and build the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter()"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = Counter()\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131169"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_plot_summary(summary):\n",
    "    for sent in sent_tokenize(summary):\n",
    "        for token in tokenizer.tokenize(sent):\n",
    "            yield token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello wonderful world']\n",
      "hello\n",
      "wonderful\n",
      "world\n"
     ]
    }
   ],
   "source": [
    "tmp_01 = sent_tokenize('hello wonderful world')\n",
    "print(tmp_01)\n",
    "\n",
    "for sent in tmp_01:\n",
    "    for token in tokenizer.tokenize(sent):\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNKNOWN = '<unk>'\n",
    "\n",
    "def build_vocab(data, max_vocab_size=None):\n",
    "    \n",
    "    vocab = Counter()\n",
    "    total = len(data)\n",
    "    \n",
    "    for i, row in enumerate(data.itertuples()):\n",
    "        \n",
    "        vocab.update(tokenize_plot_summary(row.plot))\n",
    "        \n",
    "        if (i+1)%1000 == 0:\n",
    "            sys.stdout.write(\".\")\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "    final_vocab = {word:i for i, (word, count) in enumerate(vocab.most_common(max_vocab_size))}\n",
    "    final_vocab[UNKNOWN]=len(final_vocab)+1\n",
    "    return final_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..................................................................................................................................."
     ]
    }
   ],
   "source": [
    "vocab = build_vocab(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  226171\n",
      "Saved vocabulary file to vocab.json\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocab size: \", len(vocab))\n",
    "with open(\"vocab.json\", \"w\") as f:\n",
    "    json.dump(vocab, f)\n",
    "    print(\"Saved vocabulary file to vocab.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start From Here If Data is Already Processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"movies_genres_en.csv\", delimiter='\\t', encoding='utf-8', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(131169, 31)\n"
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"vocab.json\", \"r\") as read_file:\n",
    "    vocab = json.load(read_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into train, validation and test\n",
    "\n",
    "Now we show how to prepare the data for training. First we define a function to convert a dataframe into a jsonlines format which can be used by the algorithm to train.\n",
    "\n",
    "First we split the dataframe into train, validation and test partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([['\"#7DaysLater\" (2013)',\n        \" #7dayslater is an interactive comedy series featuring an ensemble cast of YouTube celebrities. Each week the audience writes the brief via social media for an all-new episode featuring a well-known guest-star. Seven days later that week's episode premieres on TV and across multiple platforms.\",\n        0, ..., 0, 0, 'en'],\n       ['\"#BlackLove\" (2015) {Crash the Party (#1.9)}',\n        ' With just one week left in the workshops, the women consider the idea of \"The One.\" The ladies are stunned when Jahmil finally comes to a decision about Bentley and if he\\'s the one for her. Jack challenges Tennesha to express her feelings of love towards Errol, but can she put herself out there and face possible rejection?',\n        0, ..., 0, 0, 'en'],\n       ['\"#BlackLove\" (2015) {Making Lemonade Out of Lemons (#1.2)}',\n        \" All of the women start making strides towards finding their own version of a happy ending. Tennesha and Errol decide to become exclusive, but Laree just isn't ready to tell Karl she loves him, even though he has expressed that sentiment to her. Cynthia finds it hard to venture out on her own after her tumultuous separation, and Monet finally finds peace in her divorce when she decides to throw a 'freedom' party to celebrate!\",\n        0, ..., 0, 0, 'en'],\n       ...,\n       ['\"yevitne\" (2014) {(#1.4)}',\n        \" The new head of the crime squad travels to Mysen to learn about the latest progress of the investigation. According to Helen Zana's death has no connection with the investigation. Philip requests to change adoptive parents because Henning has completely rejected him since he confessed the truth.\",\n        0, ..., 0, 0, 'en'],\n       ['\"yevitne\" (2014) {(#1.6)}',\n        \" Henning is struggling between life and death. Philip decides to confess everything to Helen. If Henning's accident was orchestrated, it means there is a mole somewhere. Helen opens to Ron, and mentions the other witness. It is a mistake. Ron managed to find the identity of Philip and kidnapped him. But Philip had time to call Helen and tell him that he recognized the killer of the sandpit.\",\n        0, ..., 0, 0, 'en'],\n       ['\"mit milli\" (2006)',\n        \" Summer, Southern Turkey, dreams of losing their virginity... Four friends graduate from college and go on the holiday they've long been dreaming of. Inexperienced, they try their luck with tourists but to no avail...until they meet some local girls.\",\n        0, ..., 0, 0, 'en']], dtype=object)"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "131169"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "data_y = df.drop(['title', 'plot', 'plot_lang'], axis=1).values\n",
    "# print(data_y)\n",
    "tmp = np.argmax(data_y, axis=1)\n",
    "len(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(131169,)\n<class 'sklearn.model_selection._split.StratifiedShuffleSplit'>\n(87883,)\n<class 'sklearn.model_selection._split.StratifiedShuffleSplit'>\n"
    }
   ],
   "source": [
    "def split(df, test_size):\n",
    "    data = df.values\n",
    "    data_y = df.drop(['title', 'plot', 'plot_lang'], axis=1).values\n",
    "    \n",
    "    # StratifiedShuffleSplit does not work with one hot encoded / multiple labels. \n",
    "    # Doing the split on basis of arg max labels.\n",
    "    data_y = np.argmax(data_y, axis=1)\n",
    "    print(data_y.shape)\n",
    "    \n",
    "    stratified_split = StratifiedShuffleSplit(n_splits=2, test_size=test_size, random_state=42)\n",
    "    print(type(stratified_split))\n",
    "    \n",
    "    for train_index, test_index in stratified_split.split(data, data_y):\n",
    "        train, test = df.iloc[train_index], df.iloc[test_index]\n",
    "    return train, test\n",
    "\n",
    "train, test = split(df, 0.33)\n",
    "#Split the train further into train and validation\n",
    "train, validation = split(train, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(70306, 31)\n(43286, 31)\n"
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                                   title  \\\n42360                     \"Grange Hill\" (1978) {(#4.16)}   \n34969  \"Evil Lives Here\" (2016) {My Brother's Secret ...   \n71718                    \"Neighbours\" (1985) {(#1.7178)}   \n8291   \"Australian Story\" (1996) {A True Calling (#4....   \n85090  \"Rote Rosen\" (2006) {Eine neue Allianz (#1.2012)}   \n\n                                                    plot  Action  Adult  \\\n42360   Cathy's bunking off Cathy's group have been f...       0      0   \n34969   Danyall White always thought her brother, Ric...       0      0   \n71718   Both frustrated in love and school, Tyler tak...       0      0   \n8291    Millions of television viewers around the wor...       0      0   \n85090   Nora assumes Carla is mistaken. Nora wants to...       0      0   \n\n       Adventure  Animation  Biography  Comedy  Crime  Documentary  ...  \\\n42360          0          0          0       1      0            0  ...   \n34969          0          0          1       0      0            0  ...   \n71718          0          0          0       0      0            0  ...   \n8291           0          0          0       0      0            1  ...   \n85090          0          0          0       0      0            0  ...   \n\n       Reality-tv  Romance  Sci-Fi  Short  Sport  Talk-Show  Thriller  War  \\\n42360           0        0       0      0      0          0         0    0   \n34969           0        0       0      0      0          0         0    0   \n71718           0        1       0      0      0          0         0    0   \n8291            0        0       0      0      0          0         0    0   \n85090           0        0       0      0      0          0         0    0   \n\n       Western  plot_lang  \n42360        0         en  \n34969        0         en  \n71718        0         en  \n8291         0         en  \n85090        0         en  \n\n[5 rows x 31 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>plot</th>\n      <th>Action</th>\n      <th>Adult</th>\n      <th>Adventure</th>\n      <th>Animation</th>\n      <th>Biography</th>\n      <th>Comedy</th>\n      <th>Crime</th>\n      <th>Documentary</th>\n      <th>...</th>\n      <th>Reality-tv</th>\n      <th>Romance</th>\n      <th>Sci-Fi</th>\n      <th>Short</th>\n      <th>Sport</th>\n      <th>Talk-Show</th>\n      <th>Thriller</th>\n      <th>War</th>\n      <th>Western</th>\n      <th>plot_lang</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>42360</th>\n      <td>\"Grange Hill\" (1978) {(#4.16)}</td>\n      <td>Cathy's bunking off Cathy's group have been f...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>en</td>\n    </tr>\n    <tr>\n      <th>34969</th>\n      <td>\"Evil Lives Here\" (2016) {My Brother's Secret ...</td>\n      <td>Danyall White always thought her brother, Ric...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>en</td>\n    </tr>\n    <tr>\n      <th>71718</th>\n      <td>\"Neighbours\" (1985) {(#1.7178)}</td>\n      <td>Both frustrated in love and school, Tyler tak...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>en</td>\n    </tr>\n    <tr>\n      <th>8291</th>\n      <td>\"Australian Story\" (1996) {A True Calling (#4....</td>\n      <td>Millions of television viewers around the wor...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>en</td>\n    </tr>\n    <tr>\n      <th>85090</th>\n      <td>\"Rote Rosen\" (2006) {Eine neue Allianz (#1.2012)}</td>\n      <td>Nora assumes Carla is mistaken. Nora wants to...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>en</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 31 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                                   title  \\\n3759              \"Air Warriors\" (2014) {Harrier (#5.1)}   \n66259  \"Mission: Impossible\" (1966) {The Bargain (#3....   \n53300   \"Jjang!\" (2012) {U-KISS/ZE:A Interviews (#1.55)}   \n39564                                  \"Fun Farm\" (2014)   \n43992  \"Happy Tree Friends\" (1999/II) {Change of Hear...   \n\n                                                    plot  Action  Adult  \\\n3759    It's sleek, powerful, fast, and innovative, w...       0      0   \n66259   A former dictator, now in exile in Miami, pla...       1      0   \n53300   Happy Holidays from JJANG! This week on JJANG...       0      0   \n39564   52x7' Fun Farm is a very unique place. A farm...       0      0   \n43992   An emergency heart transplant for Disco Bear ...       0      0   \n\n       Adventure  Animation  Biography  Comedy  Crime  Documentary  ...  \\\n3759           0          0          0       0      0            1  ...   \n66259          1          0          0       0      1            0  ...   \n53300          0          0          0       0      0            0  ...   \n39564          0          1          0       0      0            0  ...   \n43992          0          1          0       1      0            0  ...   \n\n       Reality-tv  Romance  Sci-Fi  Short  Sport  Talk-Show  Thriller  War  \\\n3759            0        0       0      0      0          0         0    0   \n66259           0        0       0      0      0          0         1    0   \n53300           0        0       0      0      0          0         0    0   \n39564           0        0       0      0      0          0         0    0   \n43992           0        0       0      0      0          0         0    0   \n\n       Western  plot_lang  \n3759         0         en  \n66259        0         en  \n53300        0         en  \n39564        0         en  \n43992        0         en  \n\n[5 rows x 31 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>plot</th>\n      <th>Action</th>\n      <th>Adult</th>\n      <th>Adventure</th>\n      <th>Animation</th>\n      <th>Biography</th>\n      <th>Comedy</th>\n      <th>Crime</th>\n      <th>Documentary</th>\n      <th>...</th>\n      <th>Reality-tv</th>\n      <th>Romance</th>\n      <th>Sci-Fi</th>\n      <th>Short</th>\n      <th>Sport</th>\n      <th>Talk-Show</th>\n      <th>Thriller</th>\n      <th>War</th>\n      <th>Western</th>\n      <th>plot_lang</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3759</th>\n      <td>\"Air Warriors\" (2014) {Harrier (#5.1)}</td>\n      <td>It's sleek, powerful, fast, and innovative, w...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>en</td>\n    </tr>\n    <tr>\n      <th>66259</th>\n      <td>\"Mission: Impossible\" (1966) {The Bargain (#3....</td>\n      <td>A former dictator, now in exile in Miami, pla...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>en</td>\n    </tr>\n    <tr>\n      <th>53300</th>\n      <td>\"Jjang!\" (2012) {U-KISS/ZE:A Interviews (#1.55)}</td>\n      <td>Happy Holidays from JJANG! This week on JJANG...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>en</td>\n    </tr>\n    <tr>\n      <th>39564</th>\n      <td>\"Fun Farm\" (2014)</td>\n      <td>52x7' Fun Farm is a very unique place. A farm...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>en</td>\n    </tr>\n    <tr>\n      <th>43992</th>\n      <td>\"Happy Tree Friends\" (1999/II) {Change of Hear...</td>\n      <td>An emergency heart transplant for Disco Bear ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>en</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 31 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                                   title  \\\n42360                     \"Grange Hill\" (1978) {(#4.16)}   \n34969  \"Evil Lives Here\" (2016) {My Brother's Secret ...   \n71718                    \"Neighbours\" (1985) {(#1.7178)}   \n8291   \"Australian Story\" (1996) {A True Calling (#4....   \n85090  \"Rote Rosen\" (2006) {Eine neue Allianz (#1.2012)}   \n\n                                                    plot  Action  Adult  \\\n42360   Cathy's bunking off Cathy's group have been f...       0      0   \n34969   Danyall White always thought her brother, Ric...       0      0   \n71718   Both frustrated in love and school, Tyler tak...       0      0   \n8291    Millions of television viewers around the wor...       0      0   \n85090   Nora assumes Carla is mistaken. Nora wants to...       0      0   \n\n       Adventure  Animation  Biography  Comedy  Crime  Documentary  ...  \\\n42360          0          0          0       1      0            0  ...   \n34969          0          0          1       0      0            0  ...   \n71718          0          0          0       0      0            0  ...   \n8291           0          0          0       0      0            1  ...   \n85090          0          0          0       0      0            0  ...   \n\n       Reality-tv  Romance  Sci-Fi  Short  Sport  Talk-Show  Thriller  War  \\\n42360           0        0       0      0      0          0         0    0   \n34969           0        0       0      0      0          0         0    0   \n71718           0        1       0      0      0          0         0    0   \n8291            0        0       0      0      0          0         0    0   \n85090           0        0       0      0      0          0         0    0   \n\n       Western  plot_lang  \n42360        0         en  \n34969        0         en  \n71718        0         en  \n8291         0         en  \n85090        0         en  \n\n[5 rows x 31 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>plot</th>\n      <th>Action</th>\n      <th>Adult</th>\n      <th>Adventure</th>\n      <th>Animation</th>\n      <th>Biography</th>\n      <th>Comedy</th>\n      <th>Crime</th>\n      <th>Documentary</th>\n      <th>...</th>\n      <th>Reality-tv</th>\n      <th>Romance</th>\n      <th>Sci-Fi</th>\n      <th>Short</th>\n      <th>Sport</th>\n      <th>Talk-Show</th>\n      <th>Thriller</th>\n      <th>War</th>\n      <th>Western</th>\n      <th>plot_lang</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>42360</th>\n      <td>\"Grange Hill\" (1978) {(#4.16)}</td>\n      <td>Cathy's bunking off Cathy's group have been f...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>en</td>\n    </tr>\n    <tr>\n      <th>34969</th>\n      <td>\"Evil Lives Here\" (2016) {My Brother's Secret ...</td>\n      <td>Danyall White always thought her brother, Ric...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>en</td>\n    </tr>\n    <tr>\n      <th>71718</th>\n      <td>\"Neighbours\" (1985) {(#1.7178)}</td>\n      <td>Both frustrated in love and school, Tyler tak...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>en</td>\n    </tr>\n    <tr>\n      <th>8291</th>\n      <td>\"Australian Story\" (1996) {A True Calling (#4....</td>\n      <td>Millions of television viewers around the wor...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>en</td>\n    </tr>\n    <tr>\n      <th>85090</th>\n      <td>\"Rote Rosen\" (2006) {Eine neue Allianz (#1.2012)}</td>\n      <td>Nora assumes Carla is mistaken. Nora wants to...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>en</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 31 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative sampling\n",
    "\n",
    "The object2vec algorithm is setup as a binary classification problem. The true examples are the movie, genre pairs present in the dataset. In order to train the algorithm, we also need to provide negative examples. One option is to add all the genres to which the movie does not belong. However this strategy will create a highly skewed dataset with large percentage of negative example, as there are 27 classes present. Instead we choose to have 5 negative examples per positive example, as has been reported in related works like word2vec.\n",
    "\n",
    "Lets look at the class distribution and figure out the how much we should sample the negative examples to achieve a balanced distribution of positive and negative examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Index(['title', 'plot', 'Action', 'Adult', 'Adventure', 'Animation',\n       'Biography', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Family',\n       'Fantasy', 'Game-Show', 'History', 'Horror', 'Music', 'Musical',\n       'Mystery', 'News', 'Reality-TV', 'Reality-tv', 'Romance', 'Sci-Fi',\n       'Short', 'Sport', 'Talk-Show', 'Thriller', 'War', 'Western',\n       'plot_lang'],\n      dtype='object')"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['Action',\n 'Adult',\n 'Adventure',\n 'Animation',\n 'Biography',\n 'Comedy',\n 'Crime',\n 'Documentary',\n 'Drama',\n 'Family',\n 'Fantasy',\n 'Game-Show',\n 'History',\n 'Horror',\n 'Music',\n 'Musical',\n 'Mystery',\n 'News',\n 'Reality-TV',\n 'Reality-tv',\n 'Romance',\n 'Sci-Fi',\n 'Short',\n 'Sport',\n 'Talk-Show',\n 'Thriller',\n 'War',\n 'Western']"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "genres = list(train.columns)[2:-1]\n",
    "genres\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Number of genres:  28\n"
    }
   ],
   "source": [
    "print (\"Number of genres: \", len(genres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'Action': 'sum',\n 'Adult': 'sum',\n 'Adventure': 'sum',\n 'Animation': 'sum',\n 'Biography': 'sum',\n 'Comedy': 'sum',\n 'Crime': 'sum',\n 'Documentary': 'sum',\n 'Drama': 'sum',\n 'Family': 'sum',\n 'Fantasy': 'sum',\n 'Game-Show': 'sum',\n 'History': 'sum',\n 'Horror': 'sum',\n 'Music': 'sum',\n 'Musical': 'sum',\n 'Mystery': 'sum',\n 'News': 'sum',\n 'Reality-TV': 'sum',\n 'Reality-tv': 'sum',\n 'Romance': 'sum',\n 'Sci-Fi': 'sum',\n 'Short': 'sum',\n 'Sport': 'sum',\n 'Talk-Show': 'sum',\n 'Thriller': 'sum',\n 'War': 'sum',\n 'Western': 'sum'}"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "\n",
    "\n",
    "# create a dictionary for df aggregation, the values should be the funtion for aggregation\n",
    "agg = {genre:'sum' for genre in genres}\n",
    "type(agg)\n",
    "agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Action          7297\nAdult             70\nAdventure       6103\nAnimation       6924\nBiography        851\nComedy         20052\nCrime           8982\nDocumentary     7447\nDrama          27341\nFamily          9117\nFantasy         4466\nGame-Show       1218\nHistory         1737\nHorror          1516\nMusic           1681\nMusical          428\nMystery         6828\nNews            2544\nReality-TV      7360\nReality-tv         0\nRomance        11631\nSci-Fi          5105\nShort            472\nSport           1282\nTalk-Show       3510\nThriller        5129\nWar              839\nWestern         1503\ndtype: int64"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "agg_by_genre = train.agg(agg)\n",
    "agg_by_genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "151433"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "total_positive_samples = agg_by_genre.sum()\n",
    "total_positive_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1817135"
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "total_negative_samples = len(train)*len(genres) - total_positive_samples\n",
    "total_negative_samples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.4166806538864751"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "NEGATIVE_TO_POSITIVE_RATIO = 5\n",
    "sampling_percent = NEGATIVE_TO_POSITIVE_RATIO * total_positive_samples / total_negative_samples\n",
    "sampling_percent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "total positive examples:  151433\ntotal negative samples 1817135\nnegative sampling needed:  0.4166806538864751\n"
    }
   ],
   "source": [
    "\n",
    "print(\"total positive examples: \", total_positive_samples)\n",
    "print(\"total negative samples\", total_negative_samples)\n",
    "print(\"negative sampling needed: \", sampling_percent )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Now we can proceed to create the tokenized jsonlines dataset for training, validation and test partitions. We will use negative sampling of 0.4 for the training set, and add all the negatives for validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package stopwords to\n[nltk_data]     /home/ec2-user/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize(df, vocab, filename, negative_frac=1.0, use_stopwords=False):\n",
    "    # Rename the columns so that they are valid python identifiers\n",
    "    df = df.rename(lambda x:x.replace(\"-\", \"_\") ,axis='columns')\n",
    "\n",
    "    genres = list(df.columns)[2:-1]\n",
    "    max_seq_length = 0\n",
    "    total = len(df)\n",
    "    stop_words = set()\n",
    "\n",
    "    if use_stopwords:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    with jsonlines.open(filename, mode='w') as writer:\n",
    "        for j, row in enumerate(df.itertuples()):\n",
    "            tokens = [token for token in tokenize_plot_summary(row.plot) if token not in stop_words]\n",
    "\n",
    "            plot_token_ids = [vocab[token] if token in vocab else vocab[UNKNOWN] for token in tokens]\n",
    "            for i, genre in enumerate(genres):\n",
    "                label = getattr(row, genre)\n",
    "\n",
    "                # here we also consider generating of negative samples \n",
    "                if label == 1 or np.random.rand() < negative_frac:\n",
    "                    # All positive examples and fraction of negative examples are picked.\n",
    "                    writer.write({\"in0\": plot_token_ids, \"in1\": [i], \"label\": label})\n",
    "                    \n",
    "            max_seq_length = max(len(plot_token_ids), max_seq_length)\n",
    "            if (j+1)%1000==0:\n",
    "                sys.stdout.write(\".\")\n",
    "                sys.stdout.flush()\n",
    "        print(\"Finished tokenizing data. Max sequence length of the tokenized data: \", max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "......................................................................Finished tokenizing data. Max sequence length of the tokenized data:  1192\n"
    }
   ],
   "source": [
    "tokenize(df=train, vocab=vocab, filename=\"tokenized_movie_genres_train.jsonl\", negative_frac=0.4, use_stopwords=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": ".................Finished tokenizing data. Max sequence length of the tokenized data:  1465\n"
    }
   ],
   "source": [
    "tokenize(df=validation, vocab=vocab, filename=\"tokenized_movie_genres_validation.jsonl\", use_stopwords=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "...........................................Finished tokenizing data. Max sequence length of the tokenized data:  998\n"
    }
   ],
   "source": [
    "tokenize(df=test, vocab=vocab, filename=\"tokenized_movie_genres_test.jsonl\", use_stopwords=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For better performance, the training dataset needs to be shuffled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "!shuf tokenized_movie_genres_train.jsonl > tokenized_movie_genres_train_shuffled.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download pretrained word embeddings\n",
    "\n",
    "We will make use of pretrained word embeddings from https://nlp.stanford.edu/projects/glove/. \n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "Important: Before you begin downloading, please read the following  and make sure you are okay with the license.\n",
    "https://opendatacommons.org/licenses/pddl/1.0/\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "--2020-05-23 17:22:21--  http://nlp.stanford.edu/data/glove.840B.300d.zip\nResolving nlp.stanford.edu (nlp.stanford.edu)...171.64.67.140\nConnecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80...connected.\nHTTP request sent, awaiting response...302 Found\nLocation: https://nlp.stanford.edu/data/glove.840B.300d.zip [following]\n--2020-05-23 17:22:22--  https://nlp.stanford.edu/data/glove.840B.300d.zip\nConnecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443...connected.\nHTTP request sent, awaiting response...301 Moved Permanently\nLocation: http://downloads.cs.stanford.edu/nlp/data/glove.840B.300d.zip [following]\n--2020-05-23 17:22:25--  http://downloads.cs.stanford.edu/nlp/data/glove.840B.300d.zip\nResolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)...171.64.64.22\nConnecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80...connected.\nHTTP request sent, awaiting response...200 OK\nLength: 2176768927 (2.0G) [application/zip]\nSaving to: ‘/tmp/glove/glove.840B.300d.zip’\n\nglove.840B.300d.zip 100%[===================>]   2.03G  1.70MB/s    in 29m 50s \n\n2020-05-23 17:52:16 (1.16 MB/s) - ‘/tmp/glove/glove.840B.300d.zip’ saved [2176768927/2176768927]\n\nArchive:  /tmp/glove/glove.840B.300d.zip\n  inflating: /tmp/glove/glove.840B.300d.txt\n"
    }
   ],
   "source": [
    "# !mkdir /tmp/glove\n",
    "# !wget -P /tmp/glove/ http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
    "# !unzip -d /tmp/glove /tmp/glove/glove.840B.300d.zip\n",
    "# !rm /tmp/glove/glove.840B.300d.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sagemaker Training\n",
    "\n",
    "Let us start with defining some configurations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "bucket='md-backup-bucket-01' # customize to your bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'object2vec-movie-genre-prediction'\n",
    "\n",
    "container = get_image_uri(boto3.Session().region_name, 'object2vec')\n",
    "container\n",
    "\n",
    "train_s3_path = \"s3://{}/{}/data/train/\".format(bucket, prefix)\n",
    "validation_s3_path = \"s3://{}/{}/data/validation/\".format(bucket, prefix)\n",
    "test_s3_path = \"s3://{}/{}/data/test/\".format(bucket, prefix)\n",
    "auxiliary_s3_path = \"s3://{}/{}/data/auxiliary/\".format(bucket, prefix)\n",
    "prediction_s3_path = \"s3://{}/{}/predictions/\".format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'404615174143.dkr.ecr.us-east-2.amazonaws.com/object2vec:1'"
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "container\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Completed 342.5 MiB/346.0 MiB (564.2 KiB/s) with 1 file(s) remainingCompleted 342.8 MiB/346.0 MiB (563.9 KiB/s) with 1 file(s) remainingCompleted 343.0 MiB/346.0 MiB (561.3 KiB/s) with 1 file(s) remainingCompleted 343.3 MiB/346.0 MiB (560.6 KiB/s) with 1 file(s) remainingCompleted 343.5 MiB/346.0 MiB (559.5 KiB/s) with 1 file(s) remainingCompleted 343.8 MiB/346.0 MiB (559.6 KiB/s) with 1 file(s) remainingCompleted 344.0 MiB/346.0 MiB (559.6 KiB/s) with 1 file(s) remainingCompleted 344.3 MiB/346.0 MiB (556.9 KiB/s) with 1 file(s) remainingCompleted 344.5 MiB/346.0 MiB (554.6 KiB/s) with 1 file(s) remainingCompleted 344.8 MiB/346.0 MiB (548.0 KiB/s) with 1 file(s) remainingCompleted 345.0 MiB/346.0 MiB (545.1 KiB/s) with 1 file(s) remainingCompleted 345.3 MiB/346.0 MiB (541.5 KiB/s) with 1 file(s) remainingCompleted 345.5 MiB/346.0 MiB (529.2 KiB/s) with 1 file(s) remainingCompleted 345.8 MiB/346.0 MiB (500.6 KiB/s) with 1 file(s) remainingCompleted 346.0 MiB/346.0 MiB (490.6 KiB/s) with 1 file(s) remainingupload: ./tokenized_movie_genres_train_shuffled.jsonl to s3://md-backup-bucket-01/object2vec-movie-genre-prediction/data/train/tokenized_movie_genres_train_shuffled.jsonl\nCompleted 185.2 MiB/192.8 MiB (536.1 KiB/s) with 1 file(s) remainingCompleted 185.5 MiB/192.8 MiB (536.6 KiB/s) with 1 file(s) remainingCompleted 185.8 MiB/192.8 MiB (536.6 KiB/s) with 1 file(s) remainingCompleted 186.0 MiB/192.8 MiB (536.2 KiB/s) with 1 file(s) remainingCompleted 186.2 MiB/192.8 MiB (536.5 KiB/s) with 1 file(s) remainingCompleted 186.5 MiB/192.8 MiB (534.8 KiB/s) with 1 file(s) remainingCompleted 186.8 MiB/192.8 MiB (534.5 KiB/s) with 1 file(s) remainingCompleted 187.0 MiB/192.8 MiB (534.7 KiB/s) with 1 file(s) remainingCompleted 187.2 MiB/192.8 MiB (533.5 KiB/s) with 1 file(s) remainingCompleted 187.5 MiB/192.8 MiB (533.8 KiB/s) with 1 file(s) remainingCompleted 187.8 MiB/192.8 MiB (533.9 KiB/s) with 1 file(s) remainingCompleted 188.0 MiB/192.8 MiB (534.1 KiB/s) with 1 file(s) remainingCompleted 188.2 MiB/192.8 MiB (533.8 KiB/s) with 1 file(s) remainingCompleted 188.5 MiB/192.8 MiB (532.8 KiB/s) with 1 file(s) remainingCompleted 188.8 MiB/192.8 MiB (533.4 KiB/s) with 1 file(s) remainingCompleted 188.8 MiB/192.8 MiB (531.8 KiB/s) with 1 file(s) remainingCompleted 189.1 MiB/192.8 MiB (532.0 KiB/s) with 1 file(s) remainingCompleted 189.3 MiB/192.8 MiB (532.4 KiB/s) with 1 file(s) remainingCompleted 189.6 MiB/192.8 MiB (523.0 KiB/s) with 1 file(s) remainingCompleted 189.8 MiB/192.8 MiB (487.8 KiB/s) with 1 file(s) remainingCompleted 190.1 MiB/192.8 MiB (478.7 KiB/s) with 1 file(s) remainingCompleted 190.3 MiB/192.8 MiB (466.1 KiB/s) with 1 file(s) remainingCompleted 190.6 MiB/192.8 MiB (461.0 KiB/s) with 1 file(s) remainingCompleted 190.8 MiB/192.8 MiB (459.5 KiB/s) with 1 file(s) remainingCompleted 191.1 MiB/192.8 MiB (458.6 KiB/s) with 1 file(s) remainingCompleted 191.3 MiB/192.8 MiB (456.8 KiB/s) with 1 file(s) remainingCompleted 191.6 MiB/192.8 MiB (402.2 KiB/s) with 1 file(s) remainingCompleted 191.8 MiB/192.8 MiB (401.6 KiB/s) with 1 file(s) remainingCompleted 192.1 MiB/192.8 MiB (401.1 KiB/s) with 1 file(s) remainingCompleted 192.3 MiB/192.8 MiB (401.1 KiB/s) with 1 file(s) remainingCompleted 192.6 MiB/192.8 MiB (401.3 KiB/s) with 1 file(s) remainingCompleted 192.8 MiB/192.8 MiB (398.2 KiB/s) with 1 file(s) remainingupload: ./tokenized_movie_genres_validation.jsonl to s3://md-backup-bucket-01/object2vec-movie-genre-prediction/data/validation/tokenized_movie_genres_validation.jsonl\nCompleted 475.3 MiB/477.8 MiB (623.8 KiB/s) with 1 file(s) remainingCompleted 475.5 MiB/477.8 MiB (623.8 KiB/s) with 1 file(s) remainingCompleted 475.8 MiB/477.8 MiB (623.7 KiB/s) with 1 file(s) remainingCompleted 476.0 MiB/477.8 MiB (623.4 KiB/s) with 1 file(s) remainingCompleted 476.3 MiB/477.8 MiB (622.7 KiB/s) with 1 file(s) remainingCompleted 476.5 MiB/477.8 MiB (620.3 KiB/s) with 1 file(s) remainingCompleted 476.8 MiB/477.8 MiB (595.7 KiB/s) with 1 file(s) remainingCompleted 477.0 MiB/477.8 MiB (544.5 KiB/s) with 1 file(s) remainingCompleted 477.3 MiB/477.8 MiB (523.8 KiB/s) with 1 file(s) remainingCompleted 477.5 MiB/477.8 MiB (510.8 KiB/s) with 1 file(s) remainingCompleted 477.8 MiB/477.8 MiB (498.7 KiB/s) with 1 file(s) remainingupload: ./tokenized_movie_genres_test.jsonl to s3://md-backup-bucket-01/object2vec-movie-genre-prediction/data/test/tokenized_movie_genres_test.jsonl\n"
    }
   ],
   "source": [
    "!aws s3 cp tokenized_movie_genres_train_shuffled.jsonl {train_s3_path}\n",
    "!aws s3 cp tokenized_movie_genres_validation.jsonl {validation_s3_path}\n",
    "!aws s3 cp tokenized_movie_genres_test.jsonl {test_s3_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp vocab.json {auxiliary_s3_path}\n",
    "!aws s3 cp /tmp/glove/glove.840B.300d.txt {auxiliary_s3_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training hyperparameters\n",
    "\n",
    "The object2vec is a customizable algorithm and hence it has quite a few hyperparameters. Lets review some of the important ones:\n",
    "\n",
    "* **enc_dim**: The dimension of the encoder. Both the movie plot description and genre embeddings are mapped to this dimension. \n",
    "* **mlp_dim**: The dimension of the output from multilayer perceptron (MLP) layers.\n",
    "* **mlp_activation**: Type of activation function for the multilayer perceptron (MLP) layer.\n",
    "* **mlp_layers**: The number of multilayer perceptron (MLP) layers in the network.\n",
    "* **output_layer**: The type of output layer. We choose 'softmax' as it is a classification problem.\n",
    "* **bucket_width**: The allowed difference between data sequence length when bucketing is enabled. Bucketing is enabled when a non-zero value is specified for this parameter.\n",
    "* **num_classes**: The number of classes for classification training, which is 2 for our case.\n",
    "\n",
    "The **enc0** encodes the movie plot description which is a sequence, and **enc1** encodes the movie genre which is a single token. The encoder parameters:\n",
    "\n",
    "* **max_seq_len**: The maximum sequence length that will be considered. Any input tokens beyond max_seq_len will be truncated and ignored. We choose a value of 500 for enc\n",
    "* **network**: Network model. We choose hcnn for both enc0 and enc1.\n",
    "* **cnn_filter_width**: The filter width of the hcnn encoder.\n",
    "* **layers**: The number of layers. We choose 2 layers for enc0, as we want to capture richer structures in the movie plot description which is a sequence input. For enc1, we choose 1 layer.\n",
    "* **token_embedding_dim**: The output dimension of  token embedding layer. We choose a dimension of 300 for encoder 0, consistent with the dimension of the glove embdeddings. For enc1, we choose 10.\n",
    "* **pretrained_embedding_file**: The filename of pretrained token embedding file present in the auxiliary data channel. We use the glove embeddings for enc0. For enc1, the embeddings will be learned by the algorithm.\n",
    "* **freeze_pretrained_embedding**: Whether to freeze  pretrained embedding weights. We set this to True for enc0.\n",
    "* **vocab_file**: The vocabulary file for mapping pretrained token embeddings to vocabulary IDs. This is specified only for enc0, as we use pretrained embeddings only for enc0.\n",
    "* **vocab_size**: The vocabulary size of the tokens. For enc0, it is the number of words appearing the dataset. For enc1, it is the number of genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    " 'enc_dim': 4096, \n",
    " 'mlp_dim': 512, \n",
    " 'mlp_activation': 'relu', \n",
    " 'mlp_layers': 2, \n",
    " 'output_layer': 'softmax',\n",
    " 'bucket_width': 10, \n",
    " 'num_classes': 2,\n",
    " \n",
    " 'mini_batch_size': 256,\n",
    " \n",
    " 'enc0_max_seq_len': 500,\n",
    " 'enc1_max_seq_len': 2,\n",
    " \n",
    " 'enc0_network': 'hcnn',\n",
    " 'enc1_network': 'hcnn',\n",
    "    \n",
    " 'enc0_layers': '2',\n",
    " 'enc1_layers': '1',\n",
    "    \n",
    " 'enc0_cnn_filter_width': 2,\n",
    " 'enc1_cnn_filter_width': 1,\n",
    " \n",
    " 'enc0_token_embedding_dim': 300,\n",
    " 'enc1_token_embedding_dim': 10,\n",
    " \n",
    " 'enc0_pretrained_embedding_file' : \"glove.840B.300d.txt\",\n",
    " \n",
    " 'enc0_freeze_pretrained_embedding': 'true',\n",
    " \n",
    " 'enc0_vocab_file': 'vocab.json',\n",
    " 'enc1_vocab_file': '',\n",
    " \n",
    " 'enc0_vocab_size': len(vocab),\n",
    " 'enc1_vocab_size': len(genres),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "Note that the training will take approximately 1.5 hours to complete on the ml.p2.8xlarge instance type\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o2v = sagemaker.estimator.Estimator(container,\n",
    "                                    get_execution_role(), \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.p3.8xlarge',\n",
    "                                    output_path=\"s3://{}/{}/output\".format(bucket, prefix),\n",
    "                                   )\n",
    "o2v.set_hyperparameters(**hyperparameters)\n",
    "input_data = {\n",
    "    \"train\": s3_input(train_s3_path, content_type=\"application/jsonlines\"),\n",
    "    \"validation\": s3_input(validation_s3_path, content_type=\"application/jsonlines\"),\n",
    "    \"auxiliary\": s3_input(auxiliary_s3_path)\n",
    "}\n",
    "o2v.fit(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with Batch inference\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "Note that the batch inference will take approximately 30 minutes to complete on the ml.p2.8xlarge instance type\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = o2v.transformer(instance_count=1, \n",
    "                              instance_type=\"ml.p3.8xlarge\", \n",
    "                              output_path=prediction_s3_path)\n",
    "transformer.transform(data=test_s3_path, content_type=\"application/jsonlines\", split_type=\"Line\")\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the predictions from s3 to perform the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp --recursive {prediction_s3_path} ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(filename, predictions, genre_dict, threshold=0.5):\n",
    "    metrics = {g:{\"genre\": g, \"tp\":0, \"tn\":0, \"fp\":0, \"fn\":0} for g in genre_dict.values()}\n",
    "    with jsonlines.open(filename, \"r\") as reader, jsonlines.open(predictions, \"r\") as preds:\n",
    "        for row, preds in zip(reader, preds):\n",
    "            prediction = preds[\"scores\"][1] > threshold\n",
    "            label = row[\"label\"]\n",
    "            g = genre_dict[row[\"in1\"][0]]\n",
    "            if prediction == 1:\n",
    "                if label == prediction:\n",
    "                    metrics[g][\"tp\"] +=1\n",
    "                else:\n",
    "                    metrics[g][\"fp\"]+=1\n",
    "            elif prediction == 0:\n",
    "                if label == prediction:\n",
    "                    metrics[g][\"tn\"]+=1\n",
    "                else:\n",
    "                    metrics[g][\"fn\"]+=1\n",
    "    summary = pd.DataFrame(list(metrics.values())).set_index('genre')\n",
    "    summary['accuracy'] = summary.apply (lambda row: (row.tp + row.tn) / (row.tp + row.tn + row.fp + row.fn),axis=1)\n",
    "    summary['precision'] = summary.apply (lambda row: row.tp / (row.tp + row.fp),axis=1)\n",
    "    summary['recall'] = summary.apply (lambda row: row.tp / (row.tp + row.fn),axis=1)\n",
    "    summary['f1'] = summary.apply (lambda row: 2*(row.precision * row.recall) /(row.precision + row.recall),axis=1)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_dict = {i:genre for i, genre in enumerate(genres)}\n",
    "summary =evaluate(\"tokenized_movie_genres_test.jsonl\", \"tokenized_movie_genres_test.jsonl.out\", genre_dict, threshold=0.6)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_sum = summary[\"tp\"].sum()\n",
    "fp_sum = summary[\"fp\"].sum()\n",
    "tn_sum = summary[\"tn\"].sum()\n",
    "fn_sum = summary[\"fn\"].sum()\n",
    "precision = (tp_sum) / (tp_sum + fp_sum)\n",
    "recall = (tp_sum) / (tp_sum + fn_sum)\n",
    "\n",
    "print(\"Accuracy: \", (tp_sum + tn_sum) / (tp_sum + fp_sum + tn_sum + fn_sum))\n",
    "print(\"Micro Precision: \", precision)\n",
    "print(\"Micro Recall: \", recall)\n",
    "print(\"Micro F1: \", 2*precision*recall/(precision + recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compared the performance with [fastText](https://fasttext.cc/). Fasttext does not perform multi-label predictions, so to do a fair comparison we trained 28 binary classification models with fastText for each of the movie genres and combined the results of each predictor. While training the fastText models we set **wordNgrams** to 2, **dim** to 300 and  **pretrainedVectors** to the glove embeddings.\n",
    "\n",
    "<img src=\"comparison.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online inference demo\n",
    "\n",
    "In this section we setup a online inference endpoint and perform inference for a few recently released movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = o2v.deploy(initial_instance_count=1, instance_type=\"ml.m4.xlarge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_genre_predictions(movie_summary, genre_dict, vocab, predictor, threshold=0.5):\n",
    "    plot_token_ids = [vocab[token] if token in vocab else vocab[UNKNOWN] for token in tokenize_plot_summary(movie_summary)]\n",
    "    batch = [{\"in0\": plot_token_ids, \"in1\": [genre_id]} for genre_id in range(len(genre_dict))]\n",
    "    request = {\"instances\": batch}\n",
    "    response = predictor.predict(data=json.dumps(request))\n",
    "    scores = [score[\"scores\"] for score in json.loads(response)[\"predictions\"]]\n",
    "    predictions = [genre_dict[i] for i, score in enumerate(scores) if score[1] > threshold]\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "star_trek = \"Ten years before Kirk, Spock and the Enterprise, theUSS Discovery discovers new worlds and lifeforms \\\n",
    "as one Starfleet officer learns to understand all things alien.\"\n",
    "\n",
    "get_movie_genre_predictions(star_trek, genre_dict, vocab, predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nun = \"A priest with a haunted past and a novice on the threshold of her final vows are sent by the Vatican \\\n",
    "to investigate the death of a young nun in Romania and confront a malevolent force in the form of a demonic nun.\"\n",
    "get_movie_genre_predictions(nun, genre_dict, vocab, predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fantastic_beasts = \"The second installment of the 'Fantastic Beasts' series set in J.K. Rowling's Wizarding World \\\n",
    "featuring the adventures of magizoologist Newt Scamander.\"\n",
    "get_movie_genre_predictions(fantastic_beasts, genre_dict, vocab, predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}