{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import io\n",
    "import sagemaker.amazon.common as smac\n",
    "\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "import sagemaker\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# installing a library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "! echo {sys.prefix}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not good\n",
    "! conda install --yes --prefix {sys.prefix} mxnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good\n",
    "! /home/ec2-user/anaconda3/envs/python3/bin/pip install mxnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# super good\n",
    "import sys\n",
    "! {sys.prefix}/bin/pip install mxnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bookeh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# understand zip() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = [1, 2, 3]\n",
    "letters = ['a', 'b', 'c']\n",
    "zipped = zip(numbers, letters)\n",
    "zipped  # Holds an iterator object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(zipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zipped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# understand map() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python program to demonstrate working \n",
    "# of map. \n",
    "  \n",
    "# Return double of n \n",
    "def addition(n): \n",
    "    return n + n \n",
    "  \n",
    "# We double all numbers using map() \n",
    "numbers = (1, 2, 3, 4) \n",
    "result = map(addition, numbers) \n",
    "print(list(result)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# copy to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker as sage\n",
    "import os.path\n",
    "\n",
    "bucket = 'md-ml-labs-bucket'\n",
    "prefix = 'ufo_dataset'\n",
    "print(os.getcwd())\n",
    "# # use this when in browser\n",
    "data_path = os.path.join(os.getcwd(),'data', 'ufo_fullset.csv')\n",
    "print(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sage.Session()\n",
    "s3_data_path = sess.upload_data(\n",
    "    path=data_path, \n",
    "    bucket=bucket, \n",
    "    key_prefix=prefix)\n",
    "print(s3_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# copy data from s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "bucket = 'md-ml-labs-bucket'\n",
    "s3_client = boto3.client('s3')\n",
    "obj = s3_client.get_object(\n",
    "    Bucket=bucket,\n",
    "    Key='ufo_dataset/ufo_fullset.csv'\n",
    ")\n",
    "df = pd.read_csv(obj['Body'])\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get execution role when local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "arn:aws:iam::868024899531:role/service-role/AmazonSageMaker-ExecutionRole-20200523T071121\n"
    }
   ],
   "source": [
    "# # execute this on aws sagemaker\n",
    "# role = get_execution_role()\n",
    "\n",
    "# use this if running sagemaker locally\n",
    "def resolve_sm_role():\n",
    "    client = boto3.client('iam', region_name='us-east-2')\n",
    "    response_roles = client.list_roles(\n",
    "        PathPrefix='/',\n",
    "        # Marker='string',\n",
    "        MaxItems=999\n",
    "    )\n",
    "    for role in response_roles['Roles']:\n",
    "        if role['RoleName'].startswith('AmazonSageMaker-ExecutionRole-'):\n",
    "#             print('Resolved SageMaker IAM Role to: ' + str(role))\n",
    "            return role['Arn']\n",
    "    raise Exception('Could not resolve what should be the SageMaker role to be used')\n",
    "\n",
    "# this is the role created by sagemaker notebook on aws\n",
    "role_arn = resolve_sm_role()\n",
    "print(role_arn)\n",
    "role=role_arn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change pandas options to show all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "f_path = '/home/ec2-user/SageMaker/AWS-ML-Certification/__my_study/sm-xgboost_direct_marketing/bank-additional/bank-additional-full.csv'\n",
    "data = pd.read_csv(f_path, sep=';')\n",
    "pd.set_option('display.max_columns', 500)     # Make sure we can see all of the columns\n",
    "pd.set_option('display.max_rows', 50)         # Keep the output on one page\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notify when a cell execution ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "32512"
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import os\n",
    "duration = 1  # seconds\n",
    "freq = 440  # Hz\n",
    "os.system('play -nq -t alsa synth {} sine {}'.format(duration, freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\u0007\n"
    }
   ],
   "source": [
    "print('\\007')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show python scripts inside notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\u001b[37m#     Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.\u001b[39;49;00m\n\u001b[37m#\u001b[39;49;00m\n\u001b[37m#     Licensed under the Apache License, Version 2.0 (the \"License\").\u001b[39;49;00m\n\u001b[37m#     You may not use this file except in compliance with the License.\u001b[39;49;00m\n\u001b[37m#     A copy of the License is located at\u001b[39;49;00m\n\u001b[37m#\u001b[39;49;00m\n\u001b[37m#         https://aws.amazon.com/apache-2-0/\u001b[39;49;00m\n\u001b[37m#\u001b[39;49;00m\n\u001b[37m#     or in the \"license\" file accompanying this file. This file is distributed\u001b[39;49;00m\n\u001b[37m#     on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\u001b[39;49;00m\n\u001b[37m#     express or implied. See the License for the specific language governing\u001b[39;49;00m\n\u001b[37m#     permissions and limitations under the License.\u001b[39;49;00m\n\n\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36m__future__\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m print_function\n\n\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n\n\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n\n\u001b[37m# import tensorflow.keras as keras\u001b[39;49;00m\n\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mkeras\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdatasets\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m mnist\n\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mkeras\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmodels\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Sequential\n\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mkeras\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mlayers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Dense, Dropout, Flatten\n\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mkeras\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mlayers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Conv2D, MaxPooling2D\n\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mkeras\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m backend \u001b[34mas\u001b[39;49;00m K\n\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mmath\u001b[39;49;00m\n\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtf\u001b[39;49;00m\n\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mhorovod\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mtensorflow\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mkeras\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mhvd\u001b[39;49;00m\n\n\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n    \n    num_gpus = \u001b[36mint\u001b[39;49;00m(os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_NUM_GPUS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n\n    parser = argparse.ArgumentParser()\n\n    \u001b[37m# Data, model, and output directories. These are required.\u001b[39;49;00m\n    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--output-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_OUTPUT_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m)\n    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAIN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--test\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TEST\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n\n    args, _ = parser.parse_known_args()\n\n    \u001b[37m# Horovod: initialize Horovod.\u001b[39;49;00m\n    hvd.init()\n\n    \u001b[37m# Horovod: pin GPU to be used to process local rank (one GPU per process)\u001b[39;49;00m\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = \u001b[34mTrue\u001b[39;49;00m\n    config.gpu_options.visible_device_list = \u001b[36mstr\u001b[39;49;00m(hvd.local_rank())\n    K.set_session(tf.Session(config=config))\n\n    batch_size = \u001b[34m128\u001b[39;49;00m\n    num_classes = \u001b[34m10\u001b[39;49;00m\n\n    \u001b[37m# Horovod: adjust number of epochs based on number of GPUs.\u001b[39;49;00m\n    epochs = \u001b[36mint\u001b[39;49;00m(math.ceil(\u001b[34m12.0\u001b[39;49;00m / hvd.size()))\n\n    \u001b[37m# Input image dimensions\u001b[39;49;00m\n    img_rows, img_cols = \u001b[34m28\u001b[39;49;00m, \u001b[34m28\u001b[39;49;00m\n\n    \u001b[37m# The data, shuffled and split between train and test sets\u001b[39;49;00m\n\n    x_train = np.load(os.path.join(args.train, \u001b[33m'\u001b[39;49;00m\u001b[33mtrain.npz\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))[\u001b[33m'\u001b[39;49;00m\u001b[33mdata\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n    y_train = np.load(os.path.join(args.train, \u001b[33m'\u001b[39;49;00m\u001b[33mtrain.npz\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))[\u001b[33m'\u001b[39;49;00m\u001b[33mlabels\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mTrain dataset loaded from: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(os.path.join(args.train, \u001b[33m'\u001b[39;49;00m\u001b[33mtrain.npz\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)))\n\n    x_test = np.load(os.path.join(args.test, \u001b[33m'\u001b[39;49;00m\u001b[33mtest.npz\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))[\u001b[33m'\u001b[39;49;00m\u001b[33mdata\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n    y_test = np.load(os.path.join(args.test, \u001b[33m'\u001b[39;49;00m\u001b[33mtest.npz\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))[\u001b[33m'\u001b[39;49;00m\u001b[33mlabels\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mTest dataset loaded from: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(os.path.join(args.test, \u001b[33m'\u001b[39;49;00m\u001b[33mtest.npz\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)))\n\n\n    \u001b[34mif\u001b[39;49;00m K.image_data_format() == \u001b[33m'\u001b[39;49;00m\u001b[33mchannels_first\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n        x_train = x_train.reshape(x_train.shape[\u001b[34m0\u001b[39;49;00m], \u001b[34m1\u001b[39;49;00m, img_rows, img_cols)\n        x_test = x_test.reshape(x_test.shape[\u001b[34m0\u001b[39;49;00m], \u001b[34m1\u001b[39;49;00m, img_rows, img_cols)\n        input_shape = (\u001b[34m1\u001b[39;49;00m, img_rows, img_cols)\n    \u001b[34melse\u001b[39;49;00m:\n        x_train = x_train.reshape(x_train.shape[\u001b[34m0\u001b[39;49;00m], img_rows, img_cols, \u001b[34m1\u001b[39;49;00m)\n        x_test = x_test.reshape(x_test.shape[\u001b[34m0\u001b[39;49;00m], img_rows, img_cols, \u001b[34m1\u001b[39;49;00m)\n        input_shape = (img_rows, img_cols, \u001b[34m1\u001b[39;49;00m)\n\n    x_train = x_train.astype(\u001b[33m'\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n    x_test = x_test.astype(\u001b[33m'\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n    x_train /= \u001b[34m255\u001b[39;49;00m\n    x_test /= \u001b[34m255\u001b[39;49;00m\n    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mx_train shape:\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, x_train.shape)\n    \u001b[36mprint\u001b[39;49;00m(x_train.shape[\u001b[34m0\u001b[39;49;00m], \u001b[33m'\u001b[39;49;00m\u001b[33mtrain samples\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n    \u001b[36mprint\u001b[39;49;00m(x_test.shape[\u001b[34m0\u001b[39;49;00m], \u001b[33m'\u001b[39;49;00m\u001b[33mtest samples\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n\n    \u001b[37m# Convert class vectors to binary class matrices\u001b[39;49;00m\n    y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n    y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n\n    model = Sequential()\n    model.add(Conv2D(\u001b[34m32\u001b[39;49;00m, kernel_size=(\u001b[34m3\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m),\n                     activation=\u001b[33m'\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n                     input_shape=input_shape))\n    model.add(Conv2D(\u001b[34m64\u001b[39;49;00m, (\u001b[34m3\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m), activation=\u001b[33m'\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n    model.add(MaxPooling2D(pool_size=(\u001b[34m2\u001b[39;49;00m, \u001b[34m2\u001b[39;49;00m)))\n    model.add(Dropout(\u001b[34m0.25\u001b[39;49;00m))\n    model.add(Flatten())\n    model.add(Dense(\u001b[34m128\u001b[39;49;00m, activation=\u001b[33m'\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n    model.add(Dropout(\u001b[34m0.5\u001b[39;49;00m))\n    model.add(Dense(num_classes, activation=\u001b[33m'\u001b[39;49;00m\u001b[33msoftmax\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n\n    \u001b[37m# Horovod: adjust learning rate based on number of GPUs.\u001b[39;49;00m\n    opt = tf.keras.optimizers.Adadelta(\u001b[34m1.0\u001b[39;49;00m * hvd.size())\n\n    \u001b[37m# Horovod: add Horovod Distributed Optimizer.\u001b[39;49;00m\n    opt = hvd.DistributedOptimizer(opt)\n\n    model.compile(loss=tf.keras.losses.categorical_crossentropy,\n                  optimizer=opt,\n                  metrics=[\u001b[33m'\u001b[39;49;00m\u001b[33maccuracy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n\n    callbacks = [\n        \u001b[37m# Horovod: broadcast initial variable states from rank 0 to all other processes.\u001b[39;49;00m\n        \u001b[37m# This is necessary to ensure consistent initialization of all workers when\u001b[39;49;00m\n        \u001b[37m# training is started with random weights or restored from a checkpoint.\u001b[39;49;00m\n        hvd.callbacks.BroadcastGlobalVariablesCallback(\u001b[34m0\u001b[39;49;00m),\n    ]\n\n    \u001b[37m# Horovod: save checkpoints only on worker 0 to prevent other workers from corrupting them.\u001b[39;49;00m\n    \u001b[34mif\u001b[39;49;00m hvd.rank() == \u001b[34m0\u001b[39;49;00m:\n        callbacks.append(tf.keras.callbacks.ModelCheckpoint(\u001b[33m'\u001b[39;49;00m\u001b[33m./checkpoint-\u001b[39;49;00m\u001b[33m{epoch}\u001b[39;49;00m\u001b[33m.h5\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n\n    model.fit(x_train, y_train,\n              batch_size=batch_size,\n              callbacks=callbacks,\n              epochs=epochs,\n              verbose=\u001b[34m1\u001b[39;49;00m,\n              validation_data=(x_test, y_test))\n    score = model.evaluate(x_test, y_test, verbose=\u001b[34m0\u001b[39;49;00m)\n\n    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mTest loss:\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, score[\u001b[34m0\u001b[39;49;00m])\n    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mTest accuracy:\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, score[\u001b[34m1\u001b[39;49;00m])\n\n    \u001b[37m# Horovod: Save model only on worker 0 (i.e. master)\u001b[39;49;00m\n    \u001b[34mif\u001b[39;49;00m hvd.rank() == \u001b[34m0\u001b[39;49;00m:\n        model.save(os.path.join(args.model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.h5\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n"
    }
   ],
   "source": [
    "! pygmentize /home/ec2-user/SageMaker/AWS-ML-Certification/__my_study/sagemaker/tensorflow_script_mode_horovod/mnist_hvd.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5-final"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}