{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Embedding with Amazon SageMaker Object2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Introduction](#Introduction)\n",
    "2. [Background](#Background)\n",
    "  1. [Embedding documents using Object2Vec](#Embedding-documents-using-Object2Vec)\n",
    "3. [Download and preprocess Wikipedia data](#Download-and-preprocess-Wikipedia-data)\n",
    "  1. [Install and load dependencies](#Install-and-load-dependencies)\n",
    "  2. [Build vocabulary and tokenize datasets](#Build-vocabulary-and-tokenize-datasets)\n",
    "  3. [Upload preprocessed data to S3](#Upload-preprocessed-data-to-S3)\n",
    "4. [Define SageMaker session, Object2Vec image, S3 input and output paths](#Define-SageMaker-session,-Object2Vec-image,-S3-input-and-output-paths)\n",
    "5. [Train and deploy doc2vec](#Train-and-deploy-doc2vec)\n",
    "  1. [Learning performance boost with new features](#Learning-performance-boost-with-new-features)\n",
    "  2. [Training speedup with sparse gradient update](#Training-speedup-with-sparse-gradient-update)\n",
    "6. [Apply learned embeddings to document retrieval task](#Apply-learned-embeddings-to-document-retrieval-task)\n",
    "  1. [Comparison with the StarSpace algorithm](#Comparison-with-the-StarSpace-algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we introduce four new features to Object2Vec, a general-purpose neural embedding algorithm: negative sampling, sparse gradient update, weight-sharing, and comparator operator customization. The new features together broaden the applicability of Object2Vec, improve its training speed and accuracy, and provide users with greater flexibility. See [Introduction to the Amazon SageMaker Object2Vec](https://aws.amazon.com/blogs/machine-learning/introduction-to-amazon-sagemaker-object2vec/) if you arenâ€™t already familiar with Object2Vec.\n",
    "\n",
    "We demonstrate how these new features extend the applicability of Object2Vec to a new Document Embedding use-case: A customer has a large collection of documents. Instead of storing these documents in its raw format or as sparse bag-of-words vectors, to achieve training efficiency in the various downstream tasks, she would like to instead embed all documents in a common low-dimensional space, so that the semantic distance between these documents are preserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object2Vec is a highly customizable multi-purpose algorithm that can learn embeddings of pairs of objects. \n",
    "# The embeddings are learned such that it preserves their pairwise similarities in the original space.\n",
    "\n",
    "# - Similarity is user-defined: users need to provide the algorithm with pairs of objects that they define as similar (1) or dissimilar (0); alternatively, the users can define similarity in a continuous sense (provide a real-valued similarity score).\n",
    "\n",
    "# - The learned embeddings can be used to efficiently compute nearest neighbors of objects, as well as to visualize natural clusters of related objects in the embedding space. In addition, the embeddings can also be used as features of the corresponding objects in downstream supervised tasks such as classification or regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding documents using Object2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We demonstrate how, with the new features, Object2Vec can be used to embed a large collection of documents into vectors in the same latent space.\n",
    "\n",
    "Similar to the widely used Word2Vec algorithm for word embedding, a natural approach to document embedding is to preprocess documents as (sentence, context) pairs, where the sentence and its matching context come from the same document. The matching context is the entire document with the given sentence removed. The idea is to embed both sentence and context into a low dimensional space such that their mutual similarity is maximized, since they belong to the same document and therefore should be semantically related. The learned encoder for the context can then be used to encode new documents into the same embedding space. In order to train the encoders for sentences and documents, we also need negative (sentence, context) pairs so that the model can learn to discriminate between semantically similar and dissimilar pairs. It is easy to generate such negatives by pairing sentences with documents that they do not belong to. Since there are many more negative pairs than positives in naturally occurring data, we typically resort to random sampling techniques to achieve a balance between positive and negative pairs in the training data. The figure below shows pictorially how the positive pairs and negative pairs are generated from unlabeled data for the purpose of learning embeddings for documents (and sentences)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"doc_embedding_illustration.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We show how Object2Vec with the new *negative sampling feature* can be applied to the document embedding use-case. In addition, we show how the other new features, namely, *weight-sharing*, *customization of comparator operator*, and *sparse gradient update*, together enhance the algorithm's performance and user-experience in and beyond this use-case. Sections [Learning performance boost with new features](#Learning-performance-boost-with-new-features) and [Training speedup with sparse gradient update](#Training-speedup-with-sparse-gradient-update) in this notebook provide a detailed introduction to the new features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and preprocess Wikipedia data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please be aware of the following requirements about the acknowledgment, copyright and availability, cited from the [data source description page](https://github.com/facebookresearch/StarSpace/blob/master/LICENSE.md).\n",
    "\n",
    "> Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading wikipedia data\n",
      "wikipedia_train250k.txt\n",
      "wikipedia_dev10k.txt\n",
      "wikipedia_dev_basedocs.txt\n",
      "wikipedia_test_basedocs.txt\n",
      "wikipedia_test10k.txt\n",
      "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.creationtime'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.dev'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.ino'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.nlink'\n",
      "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.creationtime'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.dev'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.ino'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.nlink'\n",
      "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.creationtime'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.dev'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.ino'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.nlink'\n",
      "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.creationtime'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.dev'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.ino'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.nlink'\n",
      "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.creationtime'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.dev'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.ino'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.nlink'\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "DATANAME=\"wikipedia\"\n",
    "DATADIR=\"./wiki\"\n",
    "\n",
    "mkdir -p \"${DATADIR}\"\n",
    "\n",
    "if [ ! -f \"${DATADIR}/${DATANAME}_train250k.txt\" ]\n",
    "then\n",
    "    echo \"Downloading wikipedia data\"\n",
    "    wget --quiet -c \"https://dl.fbaipublicfiles.com/starspace/wikipedia_train250k.tgz\" -O \"${DATADIR}/${DATANAME}_train.tar.gz\"\n",
    "    tar -xzvf \"${DATADIR}/${DATANAME}_train.tar.gz\" -C \"${DATADIR}\"\n",
    "    wget --quiet -c \"https://dl.fbaipublicfiles.com/starspace/wikipedia_devtst.tgz\" -O \"${DATADIR}/${DATANAME}_test.tar.gz\"\n",
    "    tar -xzvf \"${DATADIR}/${DATANAME}_test.tar.gz\" -C \"${DATADIR}\"\n",
    "fi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = './wiki'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wikipedia_dev10k.txt\t    wikipedia_test_basedocs.txt  wikipedia_train.tar.gz\n",
      "wikipedia_dev_basedocs.txt  wikipedia_test.tar.gz\n",
      "wikipedia_test10k.txt\t    wikipedia_train250k.txt\n"
     ]
    }
   ],
   "source": [
    "!ls ./wiki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install and load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jsonlines in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (1.2.0)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from jsonlines) (1.11.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "! {sys.prefix}/bin/pip install jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: please run on python 3 kernel\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "import math\n",
    "import scipy\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import string\n",
    "import json, jsonlines\n",
    "\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "from itertools import chain, islice\n",
    "\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "## sagemaker api\n",
    "import sagemaker, boto3\n",
    "from sagemaker.session import s3_input\n",
    "from sagemaker.predictor import json_serializer, json_deserializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build vocabulary and tokenize datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS_SYMBOL = \"<s>\"\n",
    "EOS_SYMBOL = \"</s>\"\n",
    "UNK_SYMBOL = \"<unk>\"\n",
    "PAD_SYMBOL = \"<pad>\"\n",
    "PAD_ID = 0\n",
    "TOKEN_SEPARATOR = \" \"\n",
    "VOCAB_SYMBOLS = [PAD_SYMBOL, UNK_SYMBOL, BOS_SYMBOL, EOS_SYMBOL]\n",
    "\n",
    "\n",
    "##### utility functions for preprocessing\n",
    "def get_article_iter_from_file(fname):\n",
    "    with open(fname) as f:\n",
    "        for article in f:\n",
    "            yield article\n",
    "\n",
    "def get_article_iter_from_channel(channel, datadir='/tmp/wiki'):\n",
    "    if channel == 'train':\n",
    "        fname = os.path.join(datadir, 'wikipedia_train250k.txt')\n",
    "        return get_article_iter_from_file(fname)\n",
    "    else:\n",
    "        iterlist = []\n",
    "        suffix_list = ['train250k.txt', 'test10k.txt', 'dev10k.txt', 'test_basedocs.txt']\n",
    "        for suffix in suffix_list:\n",
    "            fname = os.path.join(datadir, 'wikipedia_'+suffix)\n",
    "            iterlist.append(get_article_iter_from_file(fname))\n",
    "        return chain.from_iterable(iterlist)\n",
    "\n",
    "\n",
    "def readlines_from_article(article):\n",
    "    return article.strip().split('\\t')\n",
    "\n",
    "\n",
    "def sentence_to_integers(sentence, word_dict, trim_size=None):\n",
    "    \"\"\"\n",
    "    Converts a string of tokens to a list of integers\n",
    "    \"\"\"\n",
    "    if not trim_size:\n",
    "        return [word_dict[token] if token in word_dict else 0 for token in get_tokens_from_sentence(sentence)]\n",
    "    else:\n",
    "        integer_list = []\n",
    "        for token in get_tokens_from_sentence(sentence):\n",
    "            if len(integer_list) < trim_size:\n",
    "                if token in word_dict:\n",
    "                    integer_list.append(word_dict[token])\n",
    "                else:\n",
    "                    integer_list.append(0)\n",
    "            else:\n",
    "                break\n",
    "        return integer_list\n",
    "\n",
    "\n",
    "def get_tokens_from_sentence(sent):\n",
    "    \"\"\"\n",
    "    Yields tokens from input string.\n",
    "\n",
    "    :param line: Input string.\n",
    "    :return: Iterator over tokens.\n",
    "    \"\"\"\n",
    "    for token in sent.split():\n",
    "        if len(token) > 0:\n",
    "            yield normalize_token(token)\n",
    "\n",
    "\n",
    "def get_tokens_from_article(article):\n",
    "    iterlist = []\n",
    "    for sent in readlines_from_article(article):\n",
    "        iterlist.append(get_tokens_from_sentence(sent))\n",
    "    return chain.from_iterable(iterlist)\n",
    "\n",
    "\n",
    "def normalize_token(token):\n",
    "    token = token.lower()\n",
    "    if all(s.isdigit() or s in string.punctuation for s in token):\n",
    "        tok = list(token)\n",
    "        for i in range(len(tok)):\n",
    "            if tok[i].isdigit():\n",
    "                tok[i] = '0'\n",
    "        token = \"\".join(tok)\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to build vocabulary\n",
    "\n",
    "def build_vocab(channel, num_words=50000, min_count=1, use_reserved_symbols=True, sort=True):\n",
    "    \"\"\"\n",
    "    Creates a vocabulary mapping from words to ids. Increasing integer ids are assigned by word frequency,\n",
    "    using lexical sorting as a tie breaker. The only exception to this are special symbols such as the padding symbol\n",
    "    (PAD).\n",
    "\n",
    "    :param num_words: Maximum number of words in the vocabulary.\n",
    "    :param min_count: Minimum occurrences of words to be included in the vocabulary.\n",
    "    :return: word-to-id mapping.\n",
    "    \"\"\"\n",
    "    vocab_symbols_set = set(VOCAB_SYMBOLS)\n",
    "    raw_vocab = Counter()\n",
    "    for article in get_article_iter_from_channel(channel):\n",
    "        article_wise_vocab_list = list()\n",
    "        for token in get_tokens_from_article(article):\n",
    "            if token not in vocab_symbols_set:\n",
    "                article_wise_vocab_list.append(token)\n",
    "        raw_vocab.update(article_wise_vocab_list)\n",
    "\n",
    "    print(\"Initial vocabulary: {} types\".format(len(raw_vocab)))\n",
    "\n",
    "    # For words with the same count, they will be ordered reverse alphabetically.\n",
    "    # Not an issue since we only care for consistency\n",
    "    pruned_vocab = sorted(((c, w) for w, c in raw_vocab.items() if c >= min_count), reverse=True)\n",
    "    print(\"Pruned vocabulary: {} types (min frequency {})\".format(len(pruned_vocab), min_count))\n",
    "\n",
    "    # truncate the vocabulary to fit size num_words (only includes the most frequent ones)\n",
    "    vocab = islice((w for c, w in pruned_vocab), num_words)\n",
    "\n",
    "    if sort:\n",
    "        # sort the vocabulary alphabetically\n",
    "        vocab = sorted(vocab)\n",
    "    if use_reserved_symbols:\n",
    "        vocab = chain(VOCAB_SYMBOLS, vocab)\n",
    "\n",
    "    word_to_id = {word: idx for idx, word in enumerate(vocab)}\n",
    "\n",
    "    print(\"Final vocabulary: {} types\".format(len(word_to_id)))\n",
    "\n",
    "    if use_reserved_symbols:\n",
    "        # Important: pad symbol becomes index 0\n",
    "        assert word_to_id[PAD_SYMBOL] == PAD_ID\n",
    "    \n",
    "    return word_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial vocabulary: 1257162 types\n",
      "Pruned vocabulary: 267518 types (min frequency 5)\n",
      "Final vocabulary: 267522 types\n"
     ]
    }
   ],
   "source": [
    "# build vocab dictionary\n",
    "\n",
    "def build_vocabulary_file(vocab_fname, channel, num_words=50000, min_count=1, \n",
    "                          use_reserved_symbols=True, sort=True, force=False):\n",
    "    if not os.path.exists(vocab_fname) or force:\n",
    "        w_dict = build_vocab(channel, num_words=num_words, min_count=min_count, \n",
    "                             use_reserved_symbols=True, sort=True)\n",
    "        with open(vocab_fname, \"w\") as write_file:\n",
    "            json.dump(w_dict, write_file)\n",
    "\n",
    "channel = 'train'\n",
    "min_count = 5\n",
    "vocab_fname = os.path.join(datadir, 'wiki-vocab-{}250k-mincount-{}.json'.format(channel, min_count))\n",
    "\n",
    "build_vocabulary_file(vocab_fname, channel, num_words=500000, min_count=min_count, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vocab file ./wiki/wiki-vocab-train250k-mincount-5.json ...\n",
      "The vocabulary size is 267522\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading vocab file {} ...\".format(vocab_fname))\n",
    "\n",
    "with open(vocab_fname) as f:\n",
    "    w_dict = json.load(f)\n",
    "    print(\"The vocabulary size is {}\".format(len(w_dict.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to build training data \n",
    "# Tokenize wiki articles to (sentence, document) pairs\n",
    "def generate_sent_article_pairs_from_single_article(article, word_dict):\n",
    "    sent_list = readlines_from_article(article)\n",
    "    art_len = len(sent_list)\n",
    "    idx = random.randint(0, art_len-1)\n",
    "    wrapper_text_idx = list(range(idx)) + list(range((idx+1) % art_len, art_len))\n",
    "    wrapper_text_list = sent_list[:idx] + sent_list[(idx+1) % art_len : art_len]\n",
    "    wrapper_tokens = []\n",
    "    for sent1 in wrapper_text_list:\n",
    "        wrapper_tokens += sentence_to_integers(sent1, word_dict)\n",
    "    sent_tokens = sentence_to_integers(sent_list[idx], word_dict)\n",
    "    yield {'in0':sent_tokens, 'in1':wrapper_tokens, 'label':1}\n",
    "\n",
    "\n",
    "def generate_sent_article_pairs_from_single_file(fname, word_dict):\n",
    "    with open(fname) as reader:\n",
    "        iter_list = []\n",
    "        for article in reader:\n",
    "            iter_list.append(generate_sent_article_pairs_from_single_article(article, word_dict))\n",
    "    return chain.from_iterable(iter_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished generating train250k data of size 250000\n"
     ]
    }
   ],
   "source": [
    "# Build training data\n",
    "\n",
    "# Generate integer positive labeled data\n",
    "train_prefix = 'train250k'\n",
    "fname = \"wikipedia_{}.txt\".format(train_prefix)\n",
    "outfname = os.path.join(datadir, '{}_tokenized.jsonl'.format(train_prefix))\n",
    "counter = 0\n",
    "\n",
    "with jsonlines.open(outfname, 'w') as writer:\n",
    "    for sample in generate_sent_article_pairs_from_single_file(os.path.join(datadir, fname), w_dict):\n",
    "        writer.write(sample)\n",
    "        counter += 1\n",
    "        \n",
    "print(\"Finished generating {} data of size {}\".format(train_prefix, counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle training data\n",
    "!shuf {outfname} > {train_prefix}_tokenized_shuf.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to generate dev/test data (with both positive and negative labels)\n",
    "\n",
    "def generate_pos_neg_samples_from_single_article(word_dict, article_idx, article_buffer, negative_sampling_rate=1):\n",
    "    sample_list = []\n",
    "    # generate positive samples\n",
    "    sent_list = readlines_from_article(article_buffer[article_idx])\n",
    "    art_len = len(sent_list)\n",
    "    idx = random.randint(0, art_len-1)\n",
    "    wrapper_text_idx = list(range(idx)) + list(range((idx+1) % art_len, art_len))\n",
    "    wrapper_text_list = sent_list[:idx] + sent_list[(idx+1) % art_len : art_len]\n",
    "    wrapper_tokens = []\n",
    "    for sent1 in wrapper_text_list:\n",
    "        wrapper_tokens += sentence_to_integers(sent1, word_dict)\n",
    "    sent_tokens = sentence_to_integers(sent_list[idx], word_dict)\n",
    "    sample_list.append({'in0':sent_tokens, 'in1':wrapper_tokens, 'label':1})\n",
    "    # generate negative sample\n",
    "    buff_len = len(article_buffer)\n",
    "    sampled_inds = np.random.choice(list(range(article_idx)) + list(range((article_idx+1) % buff_len, buff_len)), \n",
    "                                    size=negative_sampling_rate)\n",
    "    for n_idx in sampled_inds:\n",
    "        other_article = article_buffer[n_idx]\n",
    "        context_list = readlines_from_article(other_article)\n",
    "        context_tokens = []\n",
    "        for sent2 in context_list:\n",
    "            context_tokens += sentence_to_integers(sent2, word_dict)\n",
    "        sample_list.append({'in0': sent_tokens, 'in1':context_tokens, 'label':0})\n",
    "    return sample_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dev and test data\n",
    "for data in ['dev10k', 'test10k']:\n",
    "    fname = os.path.join(datadir,'wikipedia_{}.txt'.format(data))\n",
    "    test_nsr = 5\n",
    "    outfname = '{}_tokenized-nsr{}.jsonl'.format(data, test_nsr)\n",
    "    article_buffer = list(get_article_iter_from_file(fname))\n",
    "    sample_buffer = []\n",
    "    for article_idx in range(len(article_buffer)):\n",
    "        sample_buffer += generate_pos_neg_samples_from_single_article(w_dict, article_idx, \n",
    "                                                                      article_buffer, \n",
    "                                                                      negative_sampling_rate=test_nsr)\n",
    "    with jsonlines.open(outfname, 'w') as writer:\n",
    "        writer.write_all(sample_buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload preprocessed data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "TRAIN_DATA=\"train250k_tokenized_shuf.jsonl\"\n",
    "DEV_DATA=\"dev10k_tokenized-nsr{}.jsonl\".format(test_nsr)\n",
    "TEST_DATA=\"test10k_tokenized-nsr{}.jsonl\".format(test_nsr)\n",
    "\n",
    "# NOTE: define your s3 bucket and key here\n",
    "bucket = 'md-labs-bucket'\n",
    "S3_KEY = 'md-labs-object2vec-doc2vec'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./train250k_tokenized_shuf.jsonl to s3://md-labs-bucket/md-labs-object2vec-doc2vec/input/train/train250k_tokenized_shuf.jsonl\n",
      "upload: ./dev10k_tokenized-nsr5.jsonl to s3://md-labs-bucket/md-labs-object2vec-doc2vec/input/validation/dev10k_tokenized-nsr5.jsonl\n",
      "upload: ./test10k_tokenized-nsr5.jsonl to s3://md-labs-bucket/md-labs-object2vec-doc2vec/input/test/test10k_tokenized-nsr5.jsonl\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$TRAIN_DATA\" \"$DEV_DATA\" \"$TEST_DATA\" \"$bucket\" \"$S3_KEY\"\n",
    "\n",
    "aws s3 cp \"$1\" s3://$4/$5/input/train/\n",
    "aws s3 cp \"$2\" s3://$4/$5/input/validation/\n",
    "aws s3 cp \"$3\" s3://$4/$5/input/test/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Sagemaker session, Object2Vec image, S3 input and output paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your notebook is running on region 'us-east-2'\n",
      "Your IAM role: 'arn:aws:iam::868024899531:role/service-role/AmazonSageMaker-ExecutionRole-20200530T112594'\n",
      "The image uri used is '404615174143.dkr.ecr.us-east-2.amazonaws.com/object2vec:1'\n",
      "Using s3 buceket: md-labs-bucket and key prefix: md-labs-object2vec-doc2vec\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "print(\"Your notebook is running on region '{}'\".format(region))\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    " \n",
    "# role = get_execution_role()\n",
    "role = 'arn:aws:iam::868024899531:role/service-role/AmazonSageMaker-ExecutionRole-20200530T112594'\n",
    "\n",
    "print(\"Your IAM role: '{}'\".format(role))\n",
    "\n",
    "container = get_image_uri(region, 'object2vec')\n",
    "print(\"The image uri used is '{}'\".format(container))\n",
    "\n",
    "print(\"Using s3 buceket: {} and key prefix: {}\".format(bucket, S3_KEY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define input channels\n",
    "\n",
    "s3_input_path = os.path.join('s3://', bucket, S3_KEY, 'input')\n",
    "\n",
    "s3_train = s3_input(os.path.join(s3_input_path, 'train', TRAIN_DATA), \n",
    "                    distribution='ShardedByS3Key', content_type='application/jsonlines')\n",
    "\n",
    "s3_valid = s3_input(os.path.join(s3_input_path, 'validation', DEV_DATA), \n",
    "                    distribution='ShardedByS3Key', content_type='application/jsonlines')\n",
    "\n",
    "s3_test = s3_input(os.path.join(s3_input_path, 'test', TEST_DATA), \n",
    "                   distribution='ShardedByS3Key', content_type='application/jsonlines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define output path\n",
    "output_path = os.path.join('s3://', bucket, S3_KEY, 'models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and deploy doc2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combine four new features into our training of Object2Vec:\n",
    "\n",
    "- Negative sampling: With the new `negative_sampling_rate` hyperparameter, users of Object2Vec only need to provide positively labeled data pairs, and the algorithm automatically samples for negative data internally during training.\n",
    "\n",
    "- Weight-sharing of embedding layer: The new `tied_token_embedding_weight` hyperparameter gives user the flexibility to share the embedding weights for both encoders, and it improves the performance of the algorithm in this use-case\n",
    "\n",
    "- The new `comparator_list` hyperparameter gives users the flexibility to mix-and-match different operators so that they can tune the algorithm towards optimal performance for their applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning performance boost with new features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Table 1_ below shows the effect of these features on these two metrics evaluated on a test set obtained from the same data creation process. \n",
    "\n",
    "We see that when negative sampling and weight-sharing of embedding layer is on, and when we use a customized comparator operator (Hadamard product), the model has improved test performance. When all these features are combined together (last row of Table 1), the algorithm has the best performance as measured by accuracy and cross-entropy.\n",
    "\n",
    "\n",
    "### Table 1\n",
    "\n",
    "|negative_sampling_rate|weight-sharing|comparator operator| Test accuracy | Test cross-entropy|\n",
    "| :-------------       | :----------: | :-----------:     | :----------:  | ----------:       |\n",
    "|  off                 | off          | default           | 0.167         |  23               |\n",
    "|  3                 | off          | default             | 0.92          |  0.21             |\n",
    "|  5                 | off          | default             | 0.92          |   0.19            |\n",
    "|  off               | on           | default           | 0.167         |  23               |\n",
    "|  3                 | on           | default           | 0.93         |  0.18               |\n",
    "|  5                 | on           | default           | 0.936         |  0.17               |\n",
    "|  off               | on           | customized        | 0.17         |  23               |\n",
    "|  3                 | on           | customized        | 0.93         |  0.18               |\n",
    "|  5                 | on           | customized        | 0.94         |  0.17               |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- The new `token_embedding_storage_type` hyperparameter flags the use of sparse gradient update, which takes advantage of the sparse input format of Object2Vec. We tested and summarized the training speedup with different GPU and `max_seq_len` configurations in the table below. In a word, we see 2-20 times speed up on different machine and algorithm configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training speedup with sparse gradient update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Table 2_ below shows the training speeds up with sparse gradient update feature turned on, as a function of number of GPUs used for training.\n",
    "\n",
    "### Table 2\n",
    "\n",
    "|num_gpus|Throughput (samples/sec) with dense storage|Throughput with sparse storage|max_seq_len (in0/in1)|Speedup X-times  |\n",
    "| :------------- | :----------: | :-----------:| :----------: | ----------: |\n",
    "|  1             | 5k           | 14k          | 50           |  2.8        |\n",
    "|  2             | 2.7k         | 23k          | 50           |  8.5        |\n",
    "|  3             | 2k           | 23~26k       | 50           |  10         |\n",
    "|  4             | 2k           | 23k          | 50           |  10         |\n",
    "|  8             | 1.1k         | 19k~20k      | 50           |  20         |\n",
    "|  1             | 1.1k         | 2k           | 500          |  2          |\n",
    "|  2             | 1.5k         | 3.6k         | 500          |  2.4        |\n",
    "|  4             | 1.6k         | 6k           | 500          |  3.75       |\n",
    "|  6             | 1.3k         | 6.7k         | 500          |  5.15       |\n",
    "|  8             | 1.1k        | 5.6k         | 500          |  5          |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training hyperparameters\n",
    "\n",
    "hyperparameters = {\n",
    "      \"_kvstore\": \"device\",\n",
    "      \"_num_gpus\": 'auto',\n",
    "      \"_num_kv_servers\": \"auto\",\n",
    "      \"bucket_width\": 0,\n",
    "      \"dropout\": 0.4,\n",
    "      \"early_stopping_patience\": 2,\n",
    "      \"early_stopping_tolerance\": 0.01,\n",
    "      \"enc0_layers\": \"auto\",\n",
    "      \"enc0_max_seq_len\": 50,\n",
    "      \"enc0_network\": \"pooled_embedding\",\n",
    "      \"enc0_pretrained_embedding_file\": \"\",\n",
    "      \"enc0_token_embedding_dim\": 300,\n",
    "      \"enc0_vocab_size\": 267522,\n",
    "      \"enc1_network\": \"enc0\",\n",
    "      \"enc_dim\": 300,\n",
    "      \"epochs\": 20,\n",
    "      \"learning_rate\": 0.01,\n",
    "      \"mini_batch_size\": 512,\n",
    "      \"mlp_activation\": \"relu\",\n",
    "      \"mlp_dim\": 512,\n",
    "      \"mlp_layers\": 2,\n",
    "      \"num_classes\": 2,\n",
    "      \"optimizer\": \"adam\",\n",
    "      \"output_layer\": \"softmax\",\n",
    "      \"weight_decay\": 0\n",
    "}\n",
    "\n",
    "\n",
    "hyperparameters['negative_sampling_rate'] = 3\n",
    "hyperparameters['tied_token_embedding_weight'] = \"true\"\n",
    "hyperparameters['comparator_list'] = \"hadamard\"\n",
    "hyperparameters['token_embedding_storage_type'] = 'row_sparse'\n",
    "\n",
    "    \n",
    "# get estimator\n",
    "doc2vec = sagemaker.estimator.Estimator(container,\n",
    "                                          role, \n",
    "                                          train_instance_count=1, \n",
    "                                          train_instance_type='ml.p2.xlarge',\n",
    "                                          output_path=output_path,\n",
    "                                          sagemaker_session=sess)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", num_examples: 153600, 14528.4 samples/sec, epoch time so far: 0:00:10.572371\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:01:57 INFO 140303559739200] #011Training metrics: perplexity: 1.142 cross_entropy: 0.133 accuracy: 0.950 \u001b[0m\n",
      "\u001b[34m[05/31/2020 17:02:01 INFO 140303559739200] Epoch: 3, batches: 400, num_examples: 204800, 14542.3 samples/sec, epoch time so far: 0:00:14.083019\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:02:01 INFO 140303559739200] #011Training metrics: perplexity: 1.142 cross_entropy: 0.133 accuracy: 0.950 \u001b[0m\n",
      "\u001b[34m[05/31/2020 17:02:04 INFO 140303559739200] Epoch: 3, batches: 500, num_examples: 256000, 14538.5 samples/sec, epoch time so far: 0:00:17.608369\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:02:04 INFO 140303559739200] #011Training metrics: perplexity: 1.140 cross_entropy: 0.131 accuracy: 0.950 \u001b[0m\n",
      "\u001b[34m[05/31/2020 17:02:08 INFO 140303559739200] Epoch: 3, batches: 600, num_examples: 307200, 14527.8 samples/sec, epoch time so far: 0:00:21.145596\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:02:08 INFO 140303559739200] #011Training metrics: perplexity: 1.141 cross_entropy: 0.132 accuracy: 0.950 \u001b[0m\n",
      "\u001b[34m[05/31/2020 17:02:11 INFO 140303559739200] Epoch: 3, batches: 700, num_examples: 358400, 14505.0 samples/sec, epoch time so far: 0:00:24.708695\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:02:11 INFO 140303559739200] #011Training metrics: perplexity: 1.141 cross_entropy: 0.132 accuracy: 0.950 \u001b[0m\n",
      "\u001b[34m[05/31/2020 17:02:15 INFO 140303559739200] Epoch: 3, batches: 800, num_examples: 409600, 14486.5 samples/sec, epoch time so far: 0:00:28.274652\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:02:15 INFO 140303559739200] #011Training metrics: perplexity: 1.141 cross_entropy: 0.132 accuracy: 0.950 \u001b[0m\n",
      "\u001b[34m[05/31/2020 17:02:18 INFO 140303559739200] Epoch: 3, batches: 900, num_examples: 460800, 14476.9 samples/sec, epoch time so far: 0:00:31.830107\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:02:18 INFO 140303559739200] #011Training metrics: perplexity: 1.141 cross_entropy: 0.132 accuracy: 0.950 \u001b[0m\n",
      "\u001b[34m[05/31/2020 17:02:22 INFO 140303559739200] Epoch: 3, batches: 1000, num_examples: 512000, 14461.8 samples/sec, epoch time so far: 0:00:35.403512\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:02:22 INFO 140303559739200] #011Training metrics: perplexity: 1.141 cross_entropy: 0.132 accuracy: 0.950 \u001b[0m\n",
      "\u001b[34m[05/31/2020 17:02:26 INFO 140303559739200] Epoch: 3, batches: 1100, num_examples: 563200, 14453.1 samples/sec, epoch time so far: 0:00:38.967381\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:02:26 INFO 140303559739200] #011Training metrics: perplexity: 1.141 cross_entropy: 0.132 accuracy: 0.950 \u001b[0m\n",
      "\u001b[34m[05/31/2020 17:02:29 INFO 140303559739200] Epoch: 3, batches: 1200, num_examples: 614400, 14450.2 samples/sec, epoch time so far: 0:00:42.518564\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:02:29 INFO 140303559739200] #011Training metrics: perplexity: 1.142 cross_entropy: 0.133 accuracy: 0.950 \u001b[0m\n",
      "\u001b[34m[05/31/2020 17:02:33 INFO 140303559739200] Epoch: 3, batches: 1300, num_examples: 665600, 14448.3 samples/sec, epoch time so far: 0:00:46.067546\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:02:33 INFO 140303559739200] #011Training metrics: perplexity: 1.142 cross_entropy: 0.133 accuracy: 0.950 \u001b[0m\n",
      "\u001b[34m[05/31/2020 17:02:36 INFO 140303559739200] Epoch: 3, batches: 1400, num_examples: 716800, 14448.4 samples/sec, epoch time so far: 0:00:49.611029\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:02:36 INFO 140303559739200] #011Training metrics: perplexity: 1.142 cross_entropy: 0.133 accuracy: 0.950 \u001b[0m\n",
      "\u001b[34m[05/31/2020 17:02:40 INFO 140303559739200] Epoch: 3, batches: 1500, num_examples: 768000, 14447.0 samples/sec, epoch time so far: 0:00:53.159957\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:02:40 INFO 140303559739200] #011Training metrics: perplexity: 1.143 cross_entropy: 0.133 accuracy: 0.950 \u001b[0m\n",
      "\u001b[34m[05/31/2020 17:02:43 INFO 140303559739200] Epoch: 3, batches: 1600, num_examples: 819200, 14448.2 samples/sec, epoch time so far: 0:00:56.699188\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:02:43 INFO 140303559739200] #011Training metrics: perplexity: 1.143 cross_entropy: 0.134 accuracy: 0.950 \u001b[0m\n",
      "\u001b[34m[05/31/2020 17:02:47 INFO 140303559739200] Epoch: 3, batches: 1700, num_examples: 870400, 14451.0 samples/sec, epoch time so far: 0:01:00.230961\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:02:47 INFO 140303559739200] #011Training metrics: perplexity: 1.144 cross_entropy: 0.134 accuracy: 0.949 \u001b[0m\n",
      "\u001b[34m[05/31/2020 17:02:50 INFO 140303559739200] Epoch: 3, batches: 1800, num_examples: 921600, 14445.3 samples/sec, epoch time so far: 0:01:03.799448\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:02:50 INFO 140303559739200] #011Training metrics: perplexity: 1.144 cross_entropy: 0.135 accuracy: 0.949 \u001b[0m\n",
      "\u001b[34m[05/31/2020 17:02:54 INFO 140303559739200] Epoch: 3, batches: 1900, num_examples: 972800, 14443.1 samples/sec, epoch time so far: 0:01:07.354124\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:02:54 INFO 140303559739200] #011Training metrics: perplexity: 1.144 cross_entropy: 0.135 accuracy: 0.949 \u001b[0m\n",
      "\u001b[34m[05/31/2020 17:02:56 INFO 140303559739200] **************\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:02:56 INFO 140303559739200] Completed Epoch: 3, time taken: 0:01:09.338602\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:02:56 INFO 140303559739200] Epoch 3 Training metrics:   perplexity: 1.144 cross_entropy: 0.135 accuracy: 0.949 \u001b[0m\n",
      "\u001b[34m[05/31/2020 17:02:56 INFO 140303559739200] #quality_metric: host=algo-1, epoch=3, train cross_entropy <loss>=0.134697643724\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:02:56 INFO 140303559739200] #quality_metric: host=algo-1, epoch=3, train accuracy <score>=0.949187795565\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:02:59 INFO 140303559739200] Epoch 3 Validation metrics: perplexity: 1.205 cross_entropy: 0.187 accuracy: 0.933 \u001b[0m\n",
      "\u001b[34m[05/31/2020 17:02:59 INFO 140303559739200] #quality_metric: host=algo-1, epoch=3, validation cross_entropy <loss>=0.186666987331\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:02:59 INFO 140303559739200] #quality_metric: host=algo-1, epoch=3, validation accuracy <score>=0.933312367585\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:02:59 INFO 140303559739200] **************\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:02:59 INFO 140303559739200] patience losses: [0.1829337920930426, 0.19140251126077215]\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:02:59 INFO 140303559739200] min patience losses: 0.182933792093\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:02:59 INFO 140303559739200] current loss: 0.186666987331\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:02:59 INFO 140303559739200] absolute loss difference: 0.00373319523819\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:02:59 INFO 140303559739200] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"early_stop.time\": {\"count\": 1, \"max\": 0.4730224609375, \"sum\": 0.4730224609375, \"min\": 0.4730224609375}, \"update.time\": {\"count\": 1, \"max\": 72473.6020565033, \"sum\": 72473.6020565033, \"min\": 72473.6020565033}}, \"EndTime\": 1590944579.532205, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"ObjectToVec\"}, \"StartTime\": 1590944506.83469}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:02:59 INFO 140303559739200] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1956, \"sum\": 1956.0, \"min\": 1956}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1956, \"sum\": 1956.0, \"min\": 1956}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1001472, \"sum\": 1001472.0, \"min\": 1001472}, \"Total Batches Seen\": {\"count\": 1, \"max\": 7821, \"sum\": 7821.0, \"min\": 7821}, \"Total Records Seen\": {\"count\": 1, \"max\": 4004352, \"sum\": 4004352.0, \"min\": 4004352}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1001472, \"sum\": 1001472.0, \"min\": 1001472}, \"Reset Count\": {\"count\": 1, \"max\": 4, \"sum\": 4.0, \"min\": 4}}, \"EndTime\": 1590944579.532494, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"ObjectToVec\", \"epoch\": 3}, \"StartTime\": 1590944507.058575}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:02:59 INFO 140303559739200] #throughput_metric: host=algo-1, train throughput=13818.3464048 records/second\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:03:03 INFO 140303559739200] Epoch: 4, batches: 100, num_examples: 51200, 14335.6 samples/sec, epoch time so far: 0:00:03.571530\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:03:03 INFO 140303559739200] #011Training metrics: perplexity: 1.123 cross_entropy: 0.116 accuracy: 0.958 \u001b[0m\n",
      "\u001b[34m[05/31/2020 17:03:06 INFO 140303559739200] Epoch: 4, batches: 200, num_examples: 102400, 14375.3 samples/sec, epoch time so far: 0:00:07.123348\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:03:06 INFO 140303559739200] #011Training metrics: perplexity: 1.124 cross_entropy: 0.117 accuracy: 0.957 \u001b[0m\n",
      "\u001b[34m[05/31/2020 17:03:10 INFO 140303559739200] Epoch: 4, batches: 300, num_examples: 153600, 14400.2 samples/sec, epoch time so far: 0:00:10.666487\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:03:10 INFO 140303559739200] #011Training metrics: perplexity: 1.125 cross_entropy: 0.118 accuracy: 0.956 \u001b[0m\n",
      "\u001b[34m[05/31/2020 17:03:13 INFO 140303559739200] Epoch: 4, batches: 400, num_examples: 204800, 14387.5 samples/sec, epoch time so far: 0:00:14.234613\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:03:13 INFO 140303559739200] #011Training metrics: perplexity: 1.125 cross_entropy: 0.118 accuracy: 0.956 \u001b[0m\n",
      "\u001b[34m[05/31/2020 17:03:17 INFO 140303559739200] Epoch: 4, batches: 500, num_examples: 256000, 14397.2 samples/sec, epoch time so far: 0:00:17.781264\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:03:17 INFO 140303559739200] #011Training metrics: perplexity: 1.125 cross_entropy: 0.118 accuracy: 0.956 \u001b[0m\n",
      "\u001b[34m[05/31/2020 17:03:21 INFO 140303559739200] Epoch: 4, batches: 600, num_examples: 307200, 14402.2 samples/sec, epoch time so far: 0:00:21.330039\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:03:21 INFO 140303559739200] #011Training metrics: perplexity: 1.126 cross_entropy: 0.119 accuracy: 0.956 \u001b[0m\n",
      "\u001b[34m[05/31/2020 17:03:24 INFO 140303559739200] Epoch: 4, batches: 700, num_examples: 358400, 14415.5 samples/sec, epoch time so far: 0:00:24.862136\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:03:24 INFO 140303559739200] #011Training metrics: perplexity: 1.127 cross_entropy: 0.119 accuracy: 0.956 \u001b[0m\n",
      "\u001b[34m[05/31/2020 17:03:28 INFO 140303559739200] Epoch: 4, batches: 800, num_examples: 409600, 14404.4 samples/sec, epoch time so far: 0:00:28.435819\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:03:28 INFO 140303559739200] #011Training metrics: perplexity: 1.127 cross_entropy: 0.120 accuracy: 0.955 \u001b[0m\n",
      "\u001b[34m[05/31/2020 17:03:31 INFO 140303559739200] Epoch: 4, batches: 900, num_examples: 460800, 14412.2 samples/sec, epoch time so far: 0:00:31.972989\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:03:31 INFO 140303559739200] #011Training metrics: perplexity: 1.127 cross_entropy: 0.120 accuracy: 0.955 \u001b[0m\n",
      "\u001b[34m[05/31/2020 17:03:35 INFO 140303559739200] Epoch: 4, batches: 1000, num_examples: 512000, 14414.2 samples/sec, epoch time so far: 0:00:35.520458\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:03:35 INFO 140303559739200] #011Training metrics: perplexity: 1.128 cross_entropy: 0.120 accuracy: 0.955 \u001b[0m\n",
      "\u001b[34m[05/31/2020 17:03:38 INFO 140303559739200] Epoch: 4, batches: 1100, num_examples: 563200, 14418.0 samples/sec, epoch time so far: 0:00:39.062393\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:03:38 INFO 140303559739200] #011Training metrics: perplexity: 1.128 cross_entropy: 0.121 accuracy: 0.955 \u001b[0m\n",
      "\u001b[34m[05/31/2020 17:03:42 INFO 140303559739200] Epoch: 4, batches: 1200, num_examples: 614400, 14414.8 samples/sec, epoch time so far: 0:00:42.622943\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:03:42 INFO 140303559739200] #011Training metrics: perplexity: 1.128 cross_entropy: 0.120 accuracy: 0.955 \u001b[0m\n",
      "\u001b[34m[05/31/2020 17:03:45 INFO 140303559739200] Epoch: 4, batches: 1300, num_examples: 665600, 14413.8 samples/sec, epoch time so far: 0:00:46.177983\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:03:45 INFO 140303559739200] #011Training metrics: perplexity: 1.128 cross_entropy: 0.121 accuracy: 0.955 \u001b[0m\n",
      "\u001b[34m[05/31/2020 17:03:49 INFO 140303559739200] Epoch: 4, batches: 1400, num_examples: 716800, 14411.8 samples/sec, epoch time so far: 0:00:49.737040\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:03:49 INFO 140303559739200] #011Training metrics: perplexity: 1.129 cross_entropy: 0.121 accuracy: 0.955 \u001b[0m\n",
      "\u001b[34m[05/31/2020 17:03:53 INFO 140303559739200] Epoch: 4, batches: 1500, num_examples: 768000, 14404.4 samples/sec, epoch time so far: 0:00:53.316913\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:03:53 INFO 140303559739200] #011Training metrics: perplexity: 1.129 cross_entropy: 0.121 accuracy: 0.955 \u001b[0m\n",
      "\u001b[34m[05/31/2020 17:03:56 INFO 140303559739200] Epoch: 4, batches: 1600, num_examples: 819200, 14402.0 samples/sec, epoch time so far: 0:00:56.881141\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:03:56 INFO 140303559739200] #011Training metrics: perplexity: 1.129 cross_entropy: 0.121 accuracy: 0.955 \u001b[0m\n",
      "\u001b[34m[05/31/2020 17:04:00 INFO 140303559739200] Epoch: 4, batches: 1700, num_examples: 870400, 14402.7 samples/sec, epoch time so far: 0:01:00.433044\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:04:00 INFO 140303559739200] #011Training metrics: perplexity: 1.129 cross_entropy: 0.121 accuracy: 0.955 \u001b[0m\n",
      "\u001b[34m[05/31/2020 17:04:03 INFO 140303559739200] Epoch: 4, batches: 1800, num_examples: 921600, 14398.2 samples/sec, epoch time so far: 0:01:04.008063\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:04:03 INFO 140303559739200] #011Training metrics: perplexity: 1.129 cross_entropy: 0.122 accuracy: 0.955 \u001b[0m\n",
      "\u001b[34m[05/31/2020 17:04:07 INFO 140303559739200] Epoch: 4, batches: 1900, num_examples: 972800, 14394.8 samples/sec, epoch time so far: 0:01:07.580140\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:04:07 INFO 140303559739200] #011Training metrics: perplexity: 1.130 cross_entropy: 0.122 accuracy: 0.955 \u001b[0m\n",
      "\u001b[34m[05/31/2020 17:04:09 INFO 140303559739200] **************\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:04:09 INFO 140303559739200] Completed Epoch: 4, time taken: 0:01:09.562600\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:04:09 INFO 140303559739200] Epoch 4 Training metrics:   perplexity: 1.130 cross_entropy: 0.122 accuracy: 0.955 \u001b[0m\n",
      "\u001b[34m[05/31/2020 17:04:09 INFO 140303559739200] #quality_metric: host=algo-1, epoch=4, train cross_entropy <loss>=0.12190338383\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:04:09 INFO 140303559739200] #quality_metric: host=algo-1, epoch=4, train accuracy <score>=0.954619799655\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:04:12 INFO 140303559739200] Epoch 4 Validation metrics: perplexity: 1.217 cross_entropy: 0.197 accuracy: 0.935 \u001b[0m\n",
      "\u001b[34m[05/31/2020 17:04:12 INFO 140303559739200] #quality_metric: host=algo-1, epoch=4, validation cross_entropy <loss>=0.196761327464\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:04:12 INFO 140303559739200] #quality_metric: host=algo-1, epoch=4, validation accuracy <score>=0.935282044492\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:04:12 INFO 140303559739200] **************\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:04:12 INFO 140303559739200] patience losses: [0.19140251126077215, 0.18666698733123682]\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:04:12 INFO 140303559739200] min patience losses: 0.186666987331\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:04:12 INFO 140303559739200] current loss: 0.196761327464\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:04:12 INFO 140303559739200] absolute loss difference: 0.010094340133\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:04:12 INFO 140303559739200] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:04:12 INFO 140303559739200] Bad epochs exceeded patience. Stopping training early!\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:04:12 INFO 140303559739200] Early stopping criterion met! Stopping training at epoch: 4\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"early_stop.time\": {\"count\": 1, \"max\": 0.6651878356933594, \"sum\": 0.6651878356933594, \"min\": 0.6651878356933594}, \"update.time\": {\"count\": 1, \"max\": 72661.59987449646, \"sum\": 72661.59987449646, \"min\": 72661.59987449646}}, \"EndTime\": 1590944652.422972, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"ObjectToVec\"}, \"StartTime\": 1590944579.532303}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:04:12 INFO 140303559739200] Early stop condition met. Stopping training.\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:04:12 INFO 140303559739200] #progress_metric: host=algo-1, completed 100 % epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1956, \"sum\": 1956.0, \"min\": 1956}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1956, \"sum\": 1956.0, \"min\": 1956}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1001472, \"sum\": 1001472.0, \"min\": 1001472}, \"Total Batches Seen\": {\"count\": 1, \"max\": 9777, \"sum\": 9777.0, \"min\": 9777}, \"Total Records Seen\": {\"count\": 1, \"max\": 5005824, \"sum\": 5005824.0, \"min\": 5005824}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1001472, \"sum\": 1001472.0, \"min\": 1001472}, \"Reset Count\": {\"count\": 1, \"max\": 5, \"sum\": 5.0, \"min\": 5}}, \"EndTime\": 1590944652.423364, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"ObjectToVec\", \"epoch\": 4}, \"StartTime\": 1590944579.761337}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:04:12 INFO 140303559739200] #throughput_metric: host=algo-1, train throughput=13782.575579 records/second\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:04:12 WARNING 140303559739200] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:04:12 INFO 140303559739200] Best model based on epoch 1. Best loss: 0.183\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"finalize.time\": {\"count\": 1, \"max\": 1.5308856964111328, \"sum\": 1.5308856964111328, \"min\": 1.5308856964111328}}, \"EndTime\": 1590944652.425272, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"ObjectToVec\"}, \"StartTime\": 1590944652.423069}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:04:12 INFO 140303559739200] Serializing model to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:04:12 INFO 140303559739200] Saved checkpoint to \"/tmp/tmpxxHPmj/state-0001.params\"\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:04:16 INFO 140303559739200] Finished scoring on 60416 examples from 118 batches, each of size 512.\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:04:16 INFO 140303559739200] Test Metrics:perplexity: 1.201 cross_entropy: 0.183 accuracy: 0.929 \u001b[0m\n",
      "\u001b[34m[05/31/2020 17:04:16 INFO 140303559739200] Test Metric names:['perplexity', 'cross_entropy', 'accuracy']\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:04:16 INFO 140303559739200] Test Metric values:[1.2011785292310246, 0.18337926154924652, 0.928859904661017]\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:04:16 INFO 140303559739200] Test Metric names and values:[('perplexity', 1.2011785292310246), ('cross_entropy', 0.18337926154924652), ('accuracy', 0.928859904661017)]\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 118, \"sum\": 118.0, \"min\": 118}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 118, \"sum\": 118.0, \"min\": 118}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 60416, \"sum\": 60416.0, \"min\": 60416}, \"Total Batches Seen\": {\"count\": 1, \"max\": 118, \"sum\": 118.0, \"min\": 118}, \"Total Records Seen\": {\"count\": 1, \"max\": 60416, \"sum\": 60416.0, \"min\": 60416}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 60416, \"sum\": 60416.0, \"min\": 60416}, \"Reset Count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1590944656.767671, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"test_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"ObjectToVec\"}, \"StartTime\": 1590944653.659784}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:04:16 INFO 140303559739200] #test_score (algo-1) : ('perplexity', 1.2011785292310246)\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:04:16 INFO 140303559739200] #test_score (algo-1) : ('cross_entropy', 0.18337926154924652)\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:04:16 INFO 140303559739200] #test_score (algo-1) : ('accuracy', 0.928859904661017)\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:04:16 INFO 140303559739200] #quality_metric: host=algo-1, test cross_entropy <loss>=0.183379261549\u001b[0m\n",
      "\u001b[34m[05/31/2020 17:04:16 INFO 140303559739200] #quality_metric: host=algo-1, test accuracy <score>=0.928859904661\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 502765.3269767761, \"sum\": 502765.3269767761, \"min\": 502765.3269767761}, \"model.score.time\": {\"count\": 1, \"max\": 3107.7938079833984, \"sum\": 3107.7938079833984, \"min\": 3107.7938079833984}, \"model.serialize.time\": {\"count\": 1, \"max\": 1233.994960784912, \"sum\": 1233.994960784912, \"min\": 1233.994960784912}, \"setuptime\": {\"count\": 1, \"max\": 321.2149143218994, \"sum\": 321.2149143218994, \"min\": 321.2149143218994}}, \"EndTime\": 1590944656.807613, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"ObjectToVec\"}, \"StartTime\": 1590944652.42534}\n",
      "\u001b[0m\n",
      "\n",
      "2020-05-31 17:04:21 Uploading - Uploading generated training model\n",
      "2020-05-31 17:05:12 Completed - Training job completed\n",
      "Training seconds: 658\n",
      "Billable seconds: 658\n"
     ]
    }
   ],
   "source": [
    "# set hyperparameters\n",
    "doc2vec.set_hyperparameters(**hyperparameters)\n",
    "\n",
    "# fit estimator with data\n",
    "doc2vec.fit({'train': s3_train, 'validation':s3_valid, 'test':s3_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------"
     ]
    },
    {
     "ename": "ClientError",
     "evalue": "An error occurred (ExpiredTokenException) when calling the DescribeEndpoint operation: The security token included in the request is expired",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-cabd0615ddec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                         content_type='application/json')\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mpredictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc2vec_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeploy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_instance_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ml.m4.xlarge'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/model.py\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(self, initial_instance_count, instance_type, accelerator_type, endpoint_name, update_endpoint, tags, kms_key, wait, data_capture_config)\u001b[0m\n\u001b[1;32m    484\u001b[0m                 \u001b[0mkms_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkms_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m                 \u001b[0mdata_capture_config_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_capture_config_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m             )\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mendpoint_from_production_variants\u001b[0;34m(self, name, production_variants, tags, kms_key, wait, data_capture_config_dict)\u001b[0m\n\u001b[1;32m   2829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2830\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_endpoint_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2831\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2833\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexpand_role\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrole\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mcreate_endpoint\u001b[0;34m(self, endpoint_name, config_name, tags, wait)\u001b[0m\n\u001b[1;32m   2361\u001b[0m         )\n\u001b[1;32m   2362\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mendpoint_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mwait_for_endpoint\u001b[0;34m(self, endpoint, poll)\u001b[0m\n\u001b[1;32m   2608\u001b[0m             \u001b[0mdict\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mReturn\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDescribeEndpoint\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mAPI\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2609\u001b[0m         \"\"\"\n\u001b[0;32m-> 2610\u001b[0;31m         \u001b[0mdesc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wait_until\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_deploy_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2611\u001b[0m         \u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"EndpointStatus\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_wait_until\u001b[0;34m(callable_fn, poll)\u001b[0m\n\u001b[1;32m   3560\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3561\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3562\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3563\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2608\u001b[0m             \u001b[0mdict\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mReturn\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDescribeEndpoint\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mAPI\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2609\u001b[0m         \"\"\"\n\u001b[0;32m-> 2610\u001b[0;31m         \u001b[0mdesc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wait_until\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_deploy_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2611\u001b[0m         \u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"EndpointStatus\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_deploy_done\u001b[0;34m(sagemaker_client, endpoint_name)\u001b[0m\n\u001b[1;32m   3537\u001b[0m     \u001b[0min_progress_statuses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"Creating\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Updating\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3539\u001b[0;31m     \u001b[0mdesc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msagemaker_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEndpointName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3540\u001b[0m     \u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"EndpointStatus\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    275\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    584\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (ExpiredTokenException) when calling the DescribeEndpoint operation: The security token included in the request is expired"
     ]
    }
   ],
   "source": [
    "# deploy model\n",
    "\n",
    "doc2vec_model = doc2vec.create_model(\n",
    "                        serializer=json_serializer,\n",
    "                        deserializer=json_deserializer,\n",
    "                        content_type='application/json')\n",
    "\n",
    "predictor = doc2vec_model.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply learned embeddings to document retrieval task\n",
    "\n",
    "After training the model, we can use the encoders in Object2Vec to map new articles and sentences into a shared embedding space. Then we evaluate the quality of these embeddings with a downstream document retrieval task.\n",
    "\n",
    "In the retrieval task, given a sentence query, the trained algorithm needs to find its best matching document (the ground-truth document is the one that contains it) from a pool of documents, where the pool contains 10,000 other non ground-truth documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tokenized_articles_from_single_file(fname, word_dict):\n",
    "    for article in get_article_iter_from_file(fname):\n",
    "        integer_article = []\n",
    "        for sent in readlines_from_article(article):\n",
    "            integer_article += sentence_to_integers(sent, word_dict)\n",
    "        yield integer_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsonline(fname):\n",
    "    \"\"\"\n",
    "    Reads jsonline files and returns iterator\n",
    "    \"\"\"\n",
    "    with jsonlines.open(fname) as reader:\n",
    "        for line in reader:\n",
    "            yield line\n",
    "\n",
    "def send_payload(predictor, payload):\n",
    "    return predictor.predict(payload)\n",
    "\n",
    "def write_to_jsonlines(data, fname):\n",
    "    with jsonlines.open(fname, 'a') as writer:\n",
    "        data = data['predictions']\n",
    "        writer.write_all(data)\n",
    "\n",
    "\n",
    "def eval_and_write(predictor, fname, to_fname,  batch_size):\n",
    "    if os.path.exists(to_fname):\n",
    "        print(\"Removing exisiting embedding file {}\".format(to_fname))\n",
    "        os.remove(to_fname)\n",
    "    print(\"Getting embedding of data in {} and store to {}...\".format(fname, to_fname))\n",
    "    test_data_content = list(read_jsonline(fname))\n",
    "    n_test = len(test_data_content)\n",
    "    n_batches = math.ceil(n_test / float(batch_size))\n",
    "    start = 0\n",
    "    for idx in range(n_batches):\n",
    "        if idx % 10 == 0:\n",
    "            print(\"Inference on the {}-th batch\".format(idx+1))\n",
    "        end = (start + batch_size) if (start + batch_size) <= n_test else n_test\n",
    "        payload = {'instances': test_data_content[start:end]}\n",
    "        data = send_payload(predictor, payload)\n",
    "        write_to_jsonlines(data, to_fname)\n",
    "        start = end\n",
    "\n",
    "def get_embeddings(predictor, test_data_content, batch_size):\n",
    "    n_test = len(test_data_content)\n",
    "    n_batches = math.ceil(n_test / float(batch_size))\n",
    "    start = 0\n",
    "    embeddings = []\n",
    "    for idx in range(n_batches):\n",
    "        if idx % 10 == 0:\n",
    "            print(\"Inference the {}-th batch\".format(idx+1))\n",
    "        end = (start + batch_size) if (start + batch_size) <= n_test else n_test\n",
    "        payload = {'instances': test_data_content[start:end]}\n",
    "        data = send_payload(predictor, payload)\n",
    "        embeddings += data['predictions']\n",
    "        start = end\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basedocs_fpath = os.path.join(datadir, 'wikipedia_test_basedocs.txt')\n",
    "test_fpath = '{}_tokenized-nsr{}.jsonl'.format('test10k', test_nsr)\n",
    "eval_basedocs = 'test_basedocs_tokenized_in0.jsonl'\n",
    "basedocs_emb = 'test_basedocs_embeddings.jsonl'\n",
    "sent_doc_emb = 'test10k_embeddings_pairs.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import numpy as np\n",
    "basedocs_emb = 'test_basedocs_embeddings.jsonl'\n",
    "sent_doc_emb = 'test10k_embeddings_pairs.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "# tokenize basedocs\n",
    "with jsonlines.open(eval_basedocs, 'w') as writer:\n",
    "    for data in generate_tokenized_articles_from_single_file(basedocs_fpath, w_dict):\n",
    "        writer.write({'in0': data})\n",
    "\n",
    "# get basedocs embedding\n",
    "eval_and_write(predictor, eval_basedocs, basedocs_emb, batch_size)\n",
    "\n",
    "\n",
    "# get embeddings for sentence and ground-truth article pairs\n",
    "sentences = []\n",
    "gt_articles = []\n",
    "for data in read_jsonline(test_fpath):\n",
    "    if data['label'] == 1:\n",
    "        sentences.append({'in0': data['in0']})\n",
    "        gt_articles.append({'in0': data['in1']})\n",
    "        \n",
    "sent_emb = get_embeddings(predictor, sentences, batch_size)\n",
    "doc_emb = get_embeddings(predictor, gt_articles, batch_size)\n",
    "\n",
    "with jsonlines.open(sent_doc_emb, 'w') as writer:\n",
    "    for (sent, doc) in zip(sent_emb, doc_emb):\n",
    "        writer.write({'sent': sent['embeddings'], 'doc': doc['embeddings']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del w_dict\n",
    "del sent_emb, doc_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The blocks below evaluate the performance of Object2Vec model on the document retrieval task.\n",
    "\n",
    "We use two metrics hits@k and mean rank to evaluate the retrieval performance. Note that the ground-truth documents in the pool have the query sentence removed from them -- else the task would have been trivial.\n",
    "\n",
    "* hits@k:  It calculates the fraction of queries where its best-matching (ground-truth) document is contained in top k retrieved documents by the algorithm.\n",
    "* mean rank: It is the average rank of the best-matching documents, as determined by the algorithm, over all queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct normalized basedocs, sentences, and ground-truth docs embedding matrix\n",
    "\n",
    "basedocs = []\n",
    "with jsonlines.open(basedocs_emb) as reader:\n",
    "    for line in reader:\n",
    "        basedocs.append(np.array(line['embeddings'])) \n",
    "\n",
    "\n",
    "sent_embs = []\n",
    "gt_doc_embs = []\n",
    "\n",
    "with jsonlines.open(sent_doc_emb) as reader2:\n",
    "    for line2 in reader2:\n",
    "        sent_embs.append(line2['sent'])\n",
    "        gt_doc_embs.append(line2['doc'])\n",
    "\n",
    "basedocs_emb_mat = normalize(np.array(basedocs).T, axis=0)\n",
    "sent_emb_mat = normalize(np.array(sent_embs), axis=1)\n",
    "gt_emb_mat = normalize(np.array(gt_doc_embs).T, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunk_query_rank(sent_emb_mat, basedocs_emb_mat, gt_emb_mat, largest_k):\n",
    "    # this is a memory-consuming step if chunk is large\n",
    "    dot_with_basedocs = np.matmul(sent_emb_mat, basedocs_emb_mat)\n",
    "    dot_with_gt = np.diag(np.matmul(sent_emb_mat, gt_emb_mat))\n",
    "    final_ranking_scores = np.insert(dot_with_basedocs, 0, dot_with_gt, axis=1)\n",
    "    query_rankings = list()\n",
    "    largest_k_list = list()\n",
    "    for row in final_ranking_scores:\n",
    "        ranking_ind = np.argsort(row) # sorts row in increasing order of similarity score\n",
    "        num_scores = len(ranking_ind)\n",
    "        query_rankings.append(num_scores-list(ranking_ind).index(0))\n",
    "        largest_k_list.append(np.array(ranking_ind[-largest_k:]).astype(int))\n",
    "    return query_rankings, largest_k_list\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Note: We evaluate the learned embeddings on chunks of test sentences-document pairs to save run-time memory; this is to make sure that our code works on the smallest notebook instance *ml.t2.medium*. If you have a larger notebook instance, you can increase the chunk_size to speed up evaluation. For instances larger than ml.t2.xlarge, you can set chunk_size = num_test_samples`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 1000\n",
    "num_test_samples = len(sent_embs)\n",
    "assert num_test_samples%chunk_size == 0, \"Chunk_size must be divisible by {}\".format(num_test_samples)\n",
    "num_chunks = int(num_test_samples / chunk_size)\n",
    "k_list = [1, 5, 10, 20, 50]\n",
    "largest_k = max(k_list)\n",
    "query_all_rankings = list()\n",
    "all_largest_k_list = list()\n",
    "\n",
    "for i in range(0, num_chunks*chunk_size, chunk_size):\n",
    "    print(\"Evaluating on the {}-th chunk\".format(i))\n",
    "    j = i+chunk_size\n",
    "    sent_emb_submat = sent_emb_mat[i:j, :]\n",
    "    gt_emb_submat = gt_emb_mat[:, i:j]\n",
    "    query_rankings, largest_k_list = get_chunk_query_rank(sent_emb_submat, basedocs_emb_mat, gt_emb_submat, largest_k)\n",
    "    query_all_rankings += query_rankings\n",
    "    all_largest_k_list.append(np.array(largest_k_list).astype(int))\n",
    "\n",
    "all_largest_k_mat = np.concatenate(all_largest_k_list, axis=0).astype(int)\n",
    "\n",
    "print(\"Summary:\")\n",
    "print(\"Mean query ranks is {}\".format(np.mean(query_all_rankings)))\n",
    "print(\"Percentiles of query ranks is 50%:{}, 80%:{}, 90%:{}, 99%:{}\".format(*np.percentile(query_all_rankings, [50, 80, 90, 99])))\n",
    "\n",
    "for k in k_list:\n",
    "    top_k_mat = all_largest_k_mat[:, -k:]\n",
    "    unique, counts = np.unique(top_k_mat, return_counts=True)\n",
    "    print(\"The hits at {} score is {}/{}\".format(k, counts[0], len(top_k_mat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with the StarSpace algorithm \n",
    "\n",
    "We compare the performance of Object2Vec with the StarSpace (https://github.com/facebookresearch/StarSpace) algorithm on the document retrieval evaluation task, using a set of 250 thousand Wikipedia documents. The experimental results displayed in the table below, show that Object2Vec significantly outperforms StarSpace on all metrics although both models use the same kind of encoders for sentences and documents.\n",
    "\n",
    "\n",
    "| Algorithm      | hits@1       | hits@10      | hits@20      |  mean rank  |\n",
    "| :------------- | :----------: | :-----------:| :----------: | ----------: |\n",
    "|  StarSpace     | 21.98%       | 42.77%       | 50.55%       |  303.34     |\n",
    "|  Object2Vec    | 26.40%       | 47.42%       | 53.83%       |  248.67     |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
