{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie genre prediction with Object2Vec Algorithm\n",
    "### MD: modified to work on a local container\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Install and import dependencies](#Install-and-import-dependencies)\n",
    "3. [Preprocessing](#Preprocessing)\n",
    "  1. [Build the vocabulary](#Build-the-vocabulary)\n",
    "  2. [Split data into train, validation and test](#Split-data-into-train,-validation-and-test)\n",
    "  3. [Negative sampling](#Negative-sampling)\n",
    "  4. [Tokenization](#Tokenization)\n",
    "  5. [Download pretrained word embeddings](#Download-pretrained-word-embeddings)\n",
    "4. [Sagemaker Training](#Sagemaker-Training)\n",
    "  1. [Upload data to S3](#Upload-data-to-S3)\n",
    "  1. [Training hyperparameters](#Training-hyperparameters)\n",
    "5. [Evaluation with Batch inference](#Evaluation-with-Batch-inference)\n",
    "6. [Online inference demo](#Online-inference-demo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, we will explore how ObjectToVec algorithm can be used in a multi label prediction setting \n",
    "to predict the genre of a movie from its plot description. We will be using a dataset provided from imdb.\n",
    "\n",
    "\n",
    "At a high level, the network architecture that we use for this task is illustrated in the diagram below.\n",
    "\n",
    "<img src=\"image.png\" width=\"500\">\n",
    "\n",
    "We cast the problem of multi-label prediction as a binary classification problem. A positive example is the tuple of movie plot description, and a movie genre that applies to the movie in the labeled data. If a movie has multiple genres, we create multiple positive examples for the movie, one for each genre. A negative example is a pair where the genre does not apply to the movie. The negative examples are generated by picking a random subset of genres which do not apply to the movie, as determined by the labeled dataset.\n",
    "\n",
    "Let us first start with downloading the data.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "Important: Before you begin downloading, please read the following README file using your browser and make sure you are okay with the license.\n",
    "ftp://ftp.fu-berlin.de/pub/misc/movies/database/frozendata/README\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-05-23 13:38:55--  ftp://ftp.fu-berlin.de/pub/misc/movies/database/frozendata/genres.list.gz\n",
      "           => ‘genres.list.gz’\n",
      "Resolving ftp.fu-berlin.de (ftp.fu-berlin.de)... 130.133.3.130\n",
      "Connecting to ftp.fu-berlin.de (ftp.fu-berlin.de)|130.133.3.130|:21... connected.\n",
      "Logging in as anonymous ... Logged in!\n",
      "==> SYST ... done.    ==> PWD ... done.\n",
      "==> TYPE I ... done.  ==> CWD (1) /pub/misc/movies/database/frozendata ... done.\n",
      "==> SIZE genres.list.gz ... 20525974\n",
      "==> PASV ... done.    ==> RETR genres.list.gz ... done.\n",
      "Length: 20525974 (20M) (unauthoritative)\n",
      "\n",
      "genres.list.gz      100%[===================>]  19.57M  1.51MB/s    in 19s     \n",
      "\n",
      "2020-05-23 13:39:18 (1.03 MB/s) - ‘genres.list.gz’ saved [20525974]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !wget ftp://ftp.fu-berlin.de/pub/misc/movies/database/frozendata/genres.list.gz\n",
    "# !gunzip genres.list.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-05-23 13:39:43--  ftp://ftp.fu-berlin.de/pub/misc/movies/database/frozendata/plot.list.gz\n",
      "           => ‘plot.list.gz’\n",
      "Resolving ftp.fu-berlin.de (ftp.fu-berlin.de)... 130.133.3.130\n",
      "Connecting to ftp.fu-berlin.de (ftp.fu-berlin.de)|130.133.3.130|:21... connected.\n",
      "Logging in as anonymous ... Logged in!\n",
      "==> SYST ... done.    ==> PWD ... done.\n",
      "==> TYPE I ... done.  ==> CWD (1) /pub/misc/movies/database/frozendata ... done.\n",
      "==> SIZE plot.list.gz ... 159742723\n",
      "==> PASV ... done.    ==> RETR plot.list.gz ... done.\n",
      "Length: 159742723 (152M) (unauthoritative)\n",
      "\n",
      "plot.list.gz        100%[===================>] 152.34M  2.00MB/s    in 1m 55s  \n",
      "\n",
      "2020-05-23 13:41:43 (1.33 MB/s) - ‘plot.list.gz’ saved [159742723]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !wget ftp://ftp.fu-berlin.de/pub/misc/movies/database/frozendata/plot.list.gz\n",
    "# !gunzip plot.list.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/ec2-user/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.8.tar.gz (981 kB)\n",
      "\u001b[K     |████████████████████████████████| 981 kB 335 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from langdetect) (1.11.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.8-py3-none-any.whl size=993191 sha256=2a9dc7ab8939bb14500daa8be0a522fe7d855c20abd66bb15ed5f0c4473ae627\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-fmehe9hd/wheels/53/88/5d/b239dc55d773b01fdd2059606b1a8f4b64548848b8f6e381c3\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.8\n"
     ]
    }
   ],
   "source": [
    "! {sys.prefix}/bin/pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/ec2-user/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.5.zip (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 507 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting click\n",
      "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "\u001b[K     |████████████████████████████████| 82 kB 399 kB/s eta 0:00:011\n",
      "\u001b[?25hCollecting joblib\n",
      "  Downloading joblib-0.15.1-py3-none-any.whl (298 kB)\n",
      "\u001b[K     |████████████████████████████████| 298 kB 488 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex\n",
      "  Downloading regex-2020.5.14-cp36-cp36m-manylinux2010_x86_64.whl (675 kB)\n",
      "\u001b[K     |████████████████████████████████| 675 kB 1.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm\n",
      "  Downloading tqdm-4.46.0-py2.py3-none-any.whl (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 1.8 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434676 sha256=690abbe3bbb2c9cf4946a5b7ac88ce2e5ecc1de3299f9264c541f939584863a2\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-n38s_135/wheels/de/5e/42/64abaeca668161c3e2cecc24f864a8fc421e3d07a104fc8a51\n",
      "Successfully built nltk\n",
      "Installing collected packages: click, joblib, regex, tqdm, nltk\n",
      "Successfully installed click-7.1.2 joblib-0.15.1 nltk-3.5 regex-2020.5.14 tqdm-4.46.0\n"
     ]
    }
   ],
   "source": [
    "! {sys.prefix}/bin/pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.5.12\n",
      "  latest version: 4.8.3\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/ec2-user/anaconda3\n",
      "\n",
      "  added / updated specs: \n",
      "    - sqlite\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    openssl-1.1.1g             |       h516909a_0         2.1 MB  conda-forge\n",
      "    certifi-2020.4.5.1         |   py37hc8dfbb8_0         151 KB  conda-forge\n",
      "    python_abi-3.7             |          1_cp37m           4 KB  conda-forge\n",
      "    ca-certificates-2020.4.5.1 |       hecc5488_0         146 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         2.4 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "    python_abi:      3.7-1_cp37m           conda-forge\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "    ca-certificates: 2019.11.28-hecc5488_0 conda-forge --> 2020.4.5.1-hecc5488_0     conda-forge\n",
      "    certifi:         2019.11.28-py37_0     conda-forge --> 2020.4.5.1-py37hc8dfbb8_0 conda-forge\n",
      "    openssl:         1.1.1d-h516909a_0     conda-forge --> 1.1.1g-h516909a_0         conda-forge\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "openssl-1.1.1g       | 2.1 MB    | ##################################### | 100% \n",
      "certifi-2020.4.5.1   | 151 KB    | ##################################### | 100% \n",
      "python_abi-3.7       | 4 KB      | ##################################### | 100% \n",
      "ca-certificates-2020 | 146 KB    | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "!conda upgrade -y sqlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/ec2-user/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Requirement already satisfied: jsonlines in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (1.2.0)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from jsonlines) (1.11.0)\n"
     ]
    }
   ],
   "source": [
    "! {sys.prefix}/bin/pip install jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from itertools import chain, islice\n",
    "\n",
    "import boto3\n",
    "import jsonlines\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "import seaborn as sns\n",
    "\n",
    "from langdetect import detect\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TreebankWordTokenizer, sent_tokenize\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "from sagemaker.session import s3_input\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "arn:aws:iam::558157414092:role/service-role/AmazonSageMaker-ExecutionRole-20200523T082014\n"
    }
   ],
   "source": [
    "# # execute this on aws sagemaker\n",
    "# role = get_execution_role()\n",
    "\n",
    "# use this if running sagemaker locally\n",
    "def resolve_sm_role():\n",
    "    client = boto3.client('iam', region_name='us-east-2')\n",
    "    response_roles = client.list_roles(\n",
    "        PathPrefix='/',\n",
    "        # Marker='string',\n",
    "        MaxItems=999\n",
    "    )\n",
    "    for role in response_roles['Roles']:\n",
    "        if role['RoleName'].startswith('AmazonSageMaker-ExecutionRole-'):\n",
    "#             print('Resolved SageMaker IAM Role to: ' + str(role))\n",
    "            return role['Arn']\n",
    "    raise Exception('Could not resolve what should be the SageMaker role to be used')\n",
    "\n",
    "# this is the role created by sagemaker notebook on aws\n",
    "role_arn = resolve_sm_role()\n",
    "print(role_arn)\n",
    "role=role_arn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'list'>\n[0, 0, 0, 0, 0]\n"
    }
   ],
   "source": [
    "row = [0] * 5\n",
    "print(type(row))\n",
    "print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['Action', 'Adult', 'Adventure', 'Animation', 'Biography', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Family', 'Fantasy', 'Game-Show', 'History', 'Horror', 'Lifestyle', 'Music', 'Musical', 'Mystery', 'News', 'Reality-TV', 'Reality-tv', 'Romance', 'Sci-Fi', 'Sci-fi', 'Short', 'Sport', 'Talk-Show', 'Thriller', 'War', 'Western']\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                         short_title  Action  Adult  \\\n0                                    \"!Next?\" (1994)       0      0   \n1                                 \"#1 Single\" (2006)       0      0   \n2                            \"#15SecondScare\" (2015)       0      0   \n3  \"#15SecondScare\" (2015) {Who Wants to Play wit...       0      0   \n4                         \"#1MinuteNightmare\" (2014)       0      0   \n\n   Adventure  Animation  Biography  Comedy  Crime  Documentary  Drama  ...  \\\n0          0          0          0       0      0            1      0  ...   \n1          0          0          0       0      0            0      0  ...   \n2          0          0          0       0      0            0      0  ...   \n3          0          0          0       0      0            0      1  ...   \n4          0          0          0       0      0            0      0  ...   \n\n   Reality-tv  Romance  Sci-Fi  Sci-fi  Short  Sport  Talk-Show  Thriller  \\\n0           0        0       0       0      0      0          0         0   \n1           0        0       0       0      0      0          0         0   \n2           0        0       0       0      1      0          0         1   \n3           0        0       0       0      1      0          0         1   \n4           0        0       0       0      0      0          0         0   \n\n   War  Western  \n0    0        0  \n1    0        0  \n2    0        0  \n3    0        0  \n4    0        0  \n\n[5 rows x 31 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>short_title</th>\n      <th>Action</th>\n      <th>Adult</th>\n      <th>Adventure</th>\n      <th>Animation</th>\n      <th>Biography</th>\n      <th>Comedy</th>\n      <th>Crime</th>\n      <th>Documentary</th>\n      <th>Drama</th>\n      <th>...</th>\n      <th>Reality-tv</th>\n      <th>Romance</th>\n      <th>Sci-Fi</th>\n      <th>Sci-fi</th>\n      <th>Short</th>\n      <th>Sport</th>\n      <th>Talk-Show</th>\n      <th>Thriller</th>\n      <th>War</th>\n      <th>Western</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>\"!Next?\" (1994)</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>\"#1 Single\" (2006)</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>\"#15SecondScare\" (2015)</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>\"#15SecondScare\" (2015) {Who Wants to Play wit...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>\"#1MinuteNightmare\" (2014)</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 31 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "def get_genres(filename):\n",
    "    \n",
    "    genres = defaultdict(list)\n",
    "    unique_genres = set()\n",
    "    \n",
    "    with open(filename, \"r\", errors='ignore') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('\"'):\n",
    "                \n",
    "                data = line.split('\\t')\n",
    "                \n",
    "                movie = data[0]\n",
    "                genre = data[-1].strip()\n",
    "                \n",
    "                genres[movie].append(genre)\n",
    "                unique_genres.add(genre)\n",
    "                \n",
    "    unique_genres = sorted(unique_genres)\n",
    "    print(unique_genres)\n",
    "    \n",
    "    # md: do a one hot encoding for movies and genres\n",
    "    data = []\n",
    "    for movie in genres:\n",
    "        \n",
    "        # md: create a list with dimension equal to number of genres, each element equal to 0\n",
    "        row = [0]*len(unique_genres)\n",
    "        \n",
    "        for g in genres[movie]:\n",
    "            row[unique_genres.index(g)] = 1\n",
    "            \n",
    "        row.insert(0, movie)\n",
    "        data.append(row)\n",
    "        \n",
    "    genres_df = pd.DataFrame(data)\n",
    "    genres_df.columns = ['short_title'] + unique_genres\n",
    "    return genres_df\n",
    "    \n",
    "genres_df = get_genres(\"genres.list\")\n",
    "genres_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "            short_title                                              title  \\\n0  \"#7DaysLater\" (2013)                               \"#7DaysLater\" (2013)   \n1   \"#BlackLove\" (2015)       \"#BlackLove\" (2015) {Crash the Party (#1.9)}   \n2   \"#BlackLove\" (2015)  \"#BlackLove\" (2015) {Making Lemonade Out of Le...   \n3   \"#BlackLove\" (2015)      \"#BlackLove\" (2015) {Miss Independent (#1.5)}   \n4   \"#BlackLove\" (2015)     \"#BlackLove\" (2015) {Sealing the Deal (#1.10)}   \n\n                                                plot  \n0   #7dayslater is an interactive comedy series f...  \n1   With just one week left in the workshops, the...  \n2   All of the women start making strides towards...  \n3   All five of these women are independent and s...  \n4   Despite having gone through a life changing p...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>short_title</th>\n      <th>title</th>\n      <th>plot</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>\"#7DaysLater\" (2013)</td>\n      <td>\"#7DaysLater\" (2013)</td>\n      <td>#7dayslater is an interactive comedy series f...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>\"#BlackLove\" (2015)</td>\n      <td>\"#BlackLove\" (2015) {Crash the Party (#1.9)}</td>\n      <td>With just one week left in the workshops, the...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>\"#BlackLove\" (2015)</td>\n      <td>\"#BlackLove\" (2015) {Making Lemonade Out of Le...</td>\n      <td>All of the women start making strides towards...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>\"#BlackLove\" (2015)</td>\n      <td>\"#BlackLove\" (2015) {Miss Independent (#1.5)}</td>\n      <td>All five of these women are independent and s...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>\"#BlackLove\" (2015)</td>\n      <td>\"#BlackLove\" (2015) {Sealing the Deal (#1.10)}</td>\n      <td>Despite having gone through a life changing p...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "def get_plots(filename):\n",
    "    \n",
    "    with open(filename, \"r\", errors='ignore') as f:\n",
    "        data = []\n",
    "        inside = False\n",
    "        plot = ''\n",
    "        full_title = ''\n",
    "        for line in f:\n",
    "            if line.startswith(\"MV:\") and not inside:\n",
    "                inside = True\n",
    "                full_title = line.split(\"MV:\")[1].strip()\n",
    "\n",
    "            elif line.startswith(\"PL:\") and inside:\n",
    "                plot += line.split(\"PL:\")[1].replace(\"\\n\", \"\")\n",
    "\n",
    "            elif line.startswith(\"MV:\") and inside:\n",
    "                short_title = full_title.split('{')[0].strip()\n",
    "                data.append((short_title, full_title, plot))\n",
    "                plot = ''\n",
    "                inside = False\n",
    "    plots_df = pd.DataFrame(data)\n",
    "    plots_df.columns = ['short_title', 'title', 'plot']\n",
    "    return plots_df\n",
    "\n",
    "plots_df = get_plots(\"plot.list\")\n",
    "plots_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now join the genre and the plot dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>plot</th>\n",
       "      <th>Action</th>\n",
       "      <th>Adult</th>\n",
       "      <th>Adventure</th>\n",
       "      <th>Animation</th>\n",
       "      <th>Biography</th>\n",
       "      <th>Comedy</th>\n",
       "      <th>Crime</th>\n",
       "      <th>Documentary</th>\n",
       "      <th>...</th>\n",
       "      <th>Reality-tv</th>\n",
       "      <th>Romance</th>\n",
       "      <th>Sci-Fi</th>\n",
       "      <th>Sci-fi</th>\n",
       "      <th>Short</th>\n",
       "      <th>Sport</th>\n",
       "      <th>Talk-Show</th>\n",
       "      <th>Thriller</th>\n",
       "      <th>War</th>\n",
       "      <th>Western</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"#7DaysLater\" (2013)</td>\n",
       "      <td>#7dayslater is an interactive comedy series f...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"#BlackLove\" (2015) {Crash the Party (#1.9)}</td>\n",
       "      <td>With just one week left in the workshops, the...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"#BlackLove\" (2015) {Making Lemonade Out of Le...</td>\n",
       "      <td>All of the women start making strides towards...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"#BlackLove\" (2015) {Miss Independent (#1.5)}</td>\n",
       "      <td>All five of these women are independent and s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"#BlackLove\" (2015) {Sealing the Deal (#1.10)}</td>\n",
       "      <td>Despite having gone through a life changing p...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                               \"#7DaysLater\" (2013)   \n",
       "1       \"#BlackLove\" (2015) {Crash the Party (#1.9)}   \n",
       "2  \"#BlackLove\" (2015) {Making Lemonade Out of Le...   \n",
       "3      \"#BlackLove\" (2015) {Miss Independent (#1.5)}   \n",
       "4     \"#BlackLove\" (2015) {Sealing the Deal (#1.10)}   \n",
       "\n",
       "                                                plot  Action  Adult  \\\n",
       "0   #7dayslater is an interactive comedy series f...       0      0   \n",
       "1   With just one week left in the workshops, the...       0      0   \n",
       "2   All of the women start making strides towards...       0      0   \n",
       "3   All five of these women are independent and s...       0      0   \n",
       "4   Despite having gone through a life changing p...       0      0   \n",
       "\n",
       "   Adventure  Animation  Biography  Comedy  Crime  Documentary  ...  \\\n",
       "0          0          0          0       1      0            0  ...   \n",
       "1          0          0          0       0      0            0  ...   \n",
       "2          0          0          0       0      0            0  ...   \n",
       "3          0          0          0       0      0            0  ...   \n",
       "4          0          0          0       0      0            0  ...   \n",
       "\n",
       "   Reality-tv  Romance  Sci-Fi  Sci-fi  Short  Sport  Talk-Show  Thriller  \\\n",
       "0           0        0       0       0      0      0          0         0   \n",
       "1           0        0       0       0      0      0          0         0   \n",
       "2           0        0       0       0      0      0          0         0   \n",
       "3           0        0       0       0      0      0          0         0   \n",
       "4           0        0       0       0      0      0          0         0   \n",
       "\n",
       "   War  Western  \n",
       "0    0        0  \n",
       "1    0        0  \n",
       "2    0        0  \n",
       "3    0        0  \n",
       "4    0        0  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = plots_df.merge(genres_df, how='inner', on='short_title')\n",
    "data_df.dropna(inplace=True)\n",
    "data_df.drop('short_title', axis=1, inplace=True)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Action',\n",
       " 'Adult',\n",
       " 'Adventure',\n",
       " 'Animation',\n",
       " 'Biography',\n",
       " 'Comedy',\n",
       " 'Crime',\n",
       " 'Documentary',\n",
       " 'Drama',\n",
       " 'Family',\n",
       " 'Fantasy',\n",
       " 'Game-Show',\n",
       " 'History',\n",
       " 'Horror',\n",
       " 'Lifestyle',\n",
       " 'Music',\n",
       " 'Musical',\n",
       " 'Mystery',\n",
       " 'News',\n",
       " 'Reality-TV',\n",
       " 'Reality-tv',\n",
       " 'Romance',\n",
       " 'Sci-Fi',\n",
       " 'Sci-fi',\n",
       " 'Short',\n",
       " 'Sport',\n",
       " 'Talk-Show',\n",
       " 'Thriller',\n",
       " 'War',\n",
       " 'Western']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genres = list(data_df.columns)[2:]\n",
    "genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Action', 13620),\n",
       " ('Adult', 129),\n",
       " ('Adventure', 11353),\n",
       " ('Animation', 12944),\n",
       " ('Biography', 1580),\n",
       " ('Comedy', 37354),\n",
       " ('Crime', 16777),\n",
       " ('Documentary', 13882),\n",
       " ('Drama', 51150),\n",
       " ('Family', 17127),\n",
       " ('Fantasy', 8488),\n",
       " ('Game-Show', 2316),\n",
       " ('History', 3165),\n",
       " ('Horror', 2826),\n",
       " ('Lifestyle', 0),\n",
       " ('Music', 3198),\n",
       " ('Musical', 779),\n",
       " ('Mystery', 12813),\n",
       " ('News', 4719),\n",
       " ('Reality-TV', 13748),\n",
       " ('Reality-tv', 1),\n",
       " ('Romance', 21557),\n",
       " ('Sci-Fi', 9504),\n",
       " ('Sci-fi', 0),\n",
       " ('Short', 858),\n",
       " ('Sport', 2406),\n",
       " ('Talk-Show', 6516),\n",
       " ('Thriller', 9511),\n",
       " ('War', 1534),\n",
       " ('Western', 2841)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = []\n",
    "for genre in genres:\n",
    "    counts.append((genre, data_df[genre].sum()))\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genre</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Action</td>\n",
       "      <td>13620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adult</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adventure</td>\n",
       "      <td>11353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Animation</td>\n",
       "      <td>12944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Biography</td>\n",
       "      <td>1580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Comedy</td>\n",
       "      <td>37354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Crime</td>\n",
       "      <td>16777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Documentary</td>\n",
       "      <td>13882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Drama</td>\n",
       "      <td>51150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Family</td>\n",
       "      <td>17127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Fantasy</td>\n",
       "      <td>8488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Game-Show</td>\n",
       "      <td>2316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>History</td>\n",
       "      <td>3165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Horror</td>\n",
       "      <td>2826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Lifestyle</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Music</td>\n",
       "      <td>3198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Musical</td>\n",
       "      <td>779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Mystery</td>\n",
       "      <td>12813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>News</td>\n",
       "      <td>4719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Reality-TV</td>\n",
       "      <td>13748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Reality-tv</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Romance</td>\n",
       "      <td>21557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Sci-Fi</td>\n",
       "      <td>9504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Sci-fi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Short</td>\n",
       "      <td>858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Sport</td>\n",
       "      <td>2406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Talk-Show</td>\n",
       "      <td>6516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Thriller</td>\n",
       "      <td>9511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>War</td>\n",
       "      <td>1534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Western</td>\n",
       "      <td>2841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          genre  count\n",
       "0        Action  13620\n",
       "1         Adult    129\n",
       "2     Adventure  11353\n",
       "3     Animation  12944\n",
       "4     Biography   1580\n",
       "5        Comedy  37354\n",
       "6         Crime  16777\n",
       "7   Documentary  13882\n",
       "8         Drama  51150\n",
       "9        Family  17127\n",
       "10      Fantasy   8488\n",
       "11    Game-Show   2316\n",
       "12      History   3165\n",
       "13       Horror   2826\n",
       "14    Lifestyle      0\n",
       "15        Music   3198\n",
       "16      Musical    779\n",
       "17      Mystery  12813\n",
       "18         News   4719\n",
       "19   Reality-TV  13748\n",
       "20   Reality-tv      1\n",
       "21      Romance  21557\n",
       "22       Sci-Fi   9504\n",
       "23       Sci-fi      0\n",
       "24        Short    858\n",
       "25        Sport   2406\n",
       "26    Talk-Show   6516\n",
       "27     Thriller   9511\n",
       "28          War   1534\n",
       "29      Western   2841"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distribution = pd.DataFrame(counts, columns=['genre', 'count'])\n",
    "distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the genres with 0 movies\n",
    "data_df.drop('Lifestyle', axis=1, inplace=True)\n",
    "data_df.drop('Sci-fi', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we select all the movies whose description are in English. Note that this will take about 12 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['plot_lang'] = data_df.apply(lambda row: detect(row['plot']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>plot</th>\n",
       "      <th>Action</th>\n",
       "      <th>Adult</th>\n",
       "      <th>Adventure</th>\n",
       "      <th>Animation</th>\n",
       "      <th>Biography</th>\n",
       "      <th>Comedy</th>\n",
       "      <th>Crime</th>\n",
       "      <th>Documentary</th>\n",
       "      <th>...</th>\n",
       "      <th>Reality-tv</th>\n",
       "      <th>Romance</th>\n",
       "      <th>Sci-Fi</th>\n",
       "      <th>Short</th>\n",
       "      <th>Sport</th>\n",
       "      <th>Talk-Show</th>\n",
       "      <th>Thriller</th>\n",
       "      <th>War</th>\n",
       "      <th>Western</th>\n",
       "      <th>plot_lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"#7DaysLater\" (2013)</td>\n",
       "      <td>#7dayslater is an interactive comedy series f...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"#BlackLove\" (2015) {Crash the Party (#1.9)}</td>\n",
       "      <td>With just one week left in the workshops, the...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"#BlackLove\" (2015) {Making Lemonade Out of Le...</td>\n",
       "      <td>All of the women start making strides towards...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"#BlackLove\" (2015) {Miss Independent (#1.5)}</td>\n",
       "      <td>All five of these women are independent and s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"#BlackLove\" (2015) {Sealing the Deal (#1.10)}</td>\n",
       "      <td>Despite having gone through a life changing p...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                               \"#7DaysLater\" (2013)   \n",
       "1       \"#BlackLove\" (2015) {Crash the Party (#1.9)}   \n",
       "2  \"#BlackLove\" (2015) {Making Lemonade Out of Le...   \n",
       "3      \"#BlackLove\" (2015) {Miss Independent (#1.5)}   \n",
       "4     \"#BlackLove\" (2015) {Sealing the Deal (#1.10)}   \n",
       "\n",
       "                                                plot  Action  Adult  \\\n",
       "0   #7dayslater is an interactive comedy series f...       0      0   \n",
       "1   With just one week left in the workshops, the...       0      0   \n",
       "2   All of the women start making strides towards...       0      0   \n",
       "3   All five of these women are independent and s...       0      0   \n",
       "4   Despite having gone through a life changing p...       0      0   \n",
       "\n",
       "   Adventure  Animation  Biography  Comedy  Crime  Documentary  ...  \\\n",
       "0          0          0          0       1      0            0  ...   \n",
       "1          0          0          0       0      0            0  ...   \n",
       "2          0          0          0       0      0            0  ...   \n",
       "3          0          0          0       0      0            0  ...   \n",
       "4          0          0          0       0      0            0  ...   \n",
       "\n",
       "   Reality-tv  Romance  Sci-Fi  Short  Sport  Talk-Show  Thriller  War  \\\n",
       "0           0        0       0      0      0          0         0    0   \n",
       "1           0        0       0      0      0          0         0    0   \n",
       "2           0        0       0      0      0          0         0    0   \n",
       "3           0        0       0      0      0          0         0    0   \n",
       "4           0        0       0      0      0          0         0    0   \n",
       "\n",
       "   Western  plot_lang  \n",
       "0        0         en  \n",
       "1        0         en  \n",
       "2        0         en  \n",
       "3        0         en  \n",
       "4        0         en  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en    131169\n",
       "nl       145\n",
       "fr       113\n",
       "de        36\n",
       "es        10\n",
       "it         7\n",
       "no         3\n",
       "da         3\n",
       "ca         2\n",
       "sv         2\n",
       "sl         1\n",
       "pt         1\n",
       "tl         1\n",
       "hu         1\n",
       "Name: plot_lang, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df['plot_lang'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### select only en types of records and save them to a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_df[data_df.plot_lang.isin(['en'])]\n",
    "df.to_csv(\"movies_genres_en.csv\", sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from now on we can read data from csv and save time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"movies_genres_en.csv\", delimiter='\\t', encoding='utf-8', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the vocabulary\n",
    "\n",
    "Lets define a few functions to tokenize our data and build the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter()"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = Counter()\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131169"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_plot_summary(summary):\n",
    "    for sent in sent_tokenize(summary):\n",
    "        for token in tokenizer.tokenize(sent):\n",
    "            yield token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello wonderful world']\n",
      "hello\n",
      "wonderful\n",
      "world\n"
     ]
    }
   ],
   "source": [
    "tmp_01 = sent_tokenize('hello wonderful world')\n",
    "print(tmp_01)\n",
    "\n",
    "for sent in tmp_01:\n",
    "    for token in tokenizer.tokenize(sent):\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNKNOWN = '<unk>'\n",
    "\n",
    "def build_vocab(data, max_vocab_size=None):\n",
    "    \n",
    "    vocab = Counter()\n",
    "    total = len(data)\n",
    "    \n",
    "    for i, row in enumerate(data.itertuples()):\n",
    "        \n",
    "        vocab.update(tokenize_plot_summary(row.plot))\n",
    "        \n",
    "        if (i+1)%1000 == 0:\n",
    "            sys.stdout.write(\".\")\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "    final_vocab = {word:i for i, (word, count) in enumerate(vocab.most_common(max_vocab_size))}\n",
    "    final_vocab[UNKNOWN]=len(final_vocab)+1\n",
    "    return final_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..................................................................................................................................."
     ]
    }
   ],
   "source": [
    "vocab = build_vocab(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  226171\n",
      "Saved vocabulary file to vocab.json\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocab size: \", len(vocab))\n",
    "with open(\"vocab.json\", \"w\") as f:\n",
    "    json.dump(vocab, f)\n",
    "    print(\"Saved vocabulary file to vocab.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start From Here If Data is Already Processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"movies_genres_en.csv\", delimiter='\\t', encoding='utf-8', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(131169, 31)\n"
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"vocab.json\", \"r\") as read_file:\n",
    "    vocab = json.load(read_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into train, validation and test\n",
    "\n",
    "Now we show how to prepare the data for training. First we define a function to convert a dataframe into a jsonlines format which can be used by the algorithm to train.\n",
    "\n",
    "First we split the dataframe into train, validation and test partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([['\"#7DaysLater\" (2013)',\n        \" #7dayslater is an interactive comedy series featuring an ensemble cast of YouTube celebrities. Each week the audience writes the brief via social media for an all-new episode featuring a well-known guest-star. Seven days later that week's episode premieres on TV and across multiple platforms.\",\n        0, ..., 0, 0, 'en'],\n       ['\"#BlackLove\" (2015) {Crash the Party (#1.9)}',\n        ' With just one week left in the workshops, the women consider the idea of \"The One.\" The ladies are stunned when Jahmil finally comes to a decision about Bentley and if he\\'s the one for her. Jack challenges Tennesha to express her feelings of love towards Errol, but can she put herself out there and face possible rejection?',\n        0, ..., 0, 0, 'en'],\n       ['\"#BlackLove\" (2015) {Making Lemonade Out of Lemons (#1.2)}',\n        \" All of the women start making strides towards finding their own version of a happy ending. Tennesha and Errol decide to become exclusive, but Laree just isn't ready to tell Karl she loves him, even though he has expressed that sentiment to her. Cynthia finds it hard to venture out on her own after her tumultuous separation, and Monet finally finds peace in her divorce when she decides to throw a 'freedom' party to celebrate!\",\n        0, ..., 0, 0, 'en'],\n       ...,\n       ['\"yevitne\" (2014) {(#1.4)}',\n        \" The new head of the crime squad travels to Mysen to learn about the latest progress of the investigation. According to Helen Zana's death has no connection with the investigation. Philip requests to change adoptive parents because Henning has completely rejected him since he confessed the truth.\",\n        0, ..., 0, 0, 'en'],\n       ['\"yevitne\" (2014) {(#1.6)}',\n        \" Henning is struggling between life and death. Philip decides to confess everything to Helen. If Henning's accident was orchestrated, it means there is a mole somewhere. Helen opens to Ron, and mentions the other witness. It is a mistake. Ron managed to find the identity of Philip and kidnapped him. But Philip had time to call Helen and tell him that he recognized the killer of the sandpit.\",\n        0, ..., 0, 0, 'en'],\n       ['\"mit milli\" (2006)',\n        \" Summer, Southern Turkey, dreams of losing their virginity... Four friends graduate from college and go on the holiday they've long been dreaming of. Inexperienced, they try their luck with tourists but to no avail...until they meet some local girls.\",\n        0, ..., 0, 0, 'en']], dtype=object)"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "131169"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "data_y = df.drop(['title', 'plot', 'plot_lang'], axis=1).values\n",
    "# print(data_y)\n",
    "tmp = np.argmax(data_y, axis=1)\n",
    "len(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(131169,)\n<class 'sklearn.model_selection._split.StratifiedShuffleSplit'>\n(87883,)\n<class 'sklearn.model_selection._split.StratifiedShuffleSplit'>\n"
    }
   ],
   "source": [
    "def split(df, test_size):\n",
    "    data = df.values\n",
    "    data_y = df.drop(['title', 'plot', 'plot_lang'], axis=1).values\n",
    "    \n",
    "    # StratifiedShuffleSplit does not work with one hot encoded / multiple labels. \n",
    "    # Doing the split on basis of arg max labels.\n",
    "    data_y = np.argmax(data_y, axis=1)\n",
    "    print(data_y.shape)\n",
    "    \n",
    "    stratified_split = StratifiedShuffleSplit(n_splits=2, test_size=test_size, random_state=42)\n",
    "    print(type(stratified_split))\n",
    "    \n",
    "    for train_index, test_index in stratified_split.split(data, data_y):\n",
    "        train, test = df.iloc[train_index], df.iloc[test_index]\n",
    "    return train, test\n",
    "\n",
    "train, test = split(df, 0.33)\n",
    "#Split the train further into train and validation\n",
    "train, validation = split(train, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(70306, 31)\n(43286, 31)\n"
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                                   title  \\\n42360                     \"Grange Hill\" (1978) {(#4.16)}   \n34969  \"Evil Lives Here\" (2016) {My Brother's Secret ...   \n71718                    \"Neighbours\" (1985) {(#1.7178)}   \n8291   \"Australian Story\" (1996) {A True Calling (#4....   \n85090  \"Rote Rosen\" (2006) {Eine neue Allianz (#1.2012)}   \n\n                                                    plot  Action  Adult  \\\n42360   Cathy's bunking off Cathy's group have been f...       0      0   \n34969   Danyall White always thought her brother, Ric...       0      0   \n71718   Both frustrated in love and school, Tyler tak...       0      0   \n8291    Millions of television viewers around the wor...       0      0   \n85090   Nora assumes Carla is mistaken. Nora wants to...       0      0   \n\n       Adventure  Animation  Biography  Comedy  Crime  Documentary  ...  \\\n42360          0          0          0       1      0            0  ...   \n34969          0          0          1       0      0            0  ...   \n71718          0          0          0       0      0            0  ...   \n8291           0          0          0       0      0            1  ...   \n85090          0          0          0       0      0            0  ...   \n\n       Reality-tv  Romance  Sci-Fi  Short  Sport  Talk-Show  Thriller  War  \\\n42360           0        0       0      0      0          0         0    0   \n34969           0        0       0      0      0          0         0    0   \n71718           0        1       0      0      0          0         0    0   \n8291            0        0       0      0      0          0         0    0   \n85090           0        0       0      0      0          0         0    0   \n\n       Western  plot_lang  \n42360        0         en  \n34969        0         en  \n71718        0         en  \n8291         0         en  \n85090        0         en  \n\n[5 rows x 31 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>plot</th>\n      <th>Action</th>\n      <th>Adult</th>\n      <th>Adventure</th>\n      <th>Animation</th>\n      <th>Biography</th>\n      <th>Comedy</th>\n      <th>Crime</th>\n      <th>Documentary</th>\n      <th>...</th>\n      <th>Reality-tv</th>\n      <th>Romance</th>\n      <th>Sci-Fi</th>\n      <th>Short</th>\n      <th>Sport</th>\n      <th>Talk-Show</th>\n      <th>Thriller</th>\n      <th>War</th>\n      <th>Western</th>\n      <th>plot_lang</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>42360</th>\n      <td>\"Grange Hill\" (1978) {(#4.16)}</td>\n      <td>Cathy's bunking off Cathy's group have been f...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>en</td>\n    </tr>\n    <tr>\n      <th>34969</th>\n      <td>\"Evil Lives Here\" (2016) {My Brother's Secret ...</td>\n      <td>Danyall White always thought her brother, Ric...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>en</td>\n    </tr>\n    <tr>\n      <th>71718</th>\n      <td>\"Neighbours\" (1985) {(#1.7178)}</td>\n      <td>Both frustrated in love and school, Tyler tak...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>en</td>\n    </tr>\n    <tr>\n      <th>8291</th>\n      <td>\"Australian Story\" (1996) {A True Calling (#4....</td>\n      <td>Millions of television viewers around the wor...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>en</td>\n    </tr>\n    <tr>\n      <th>85090</th>\n      <td>\"Rote Rosen\" (2006) {Eine neue Allianz (#1.2012)}</td>\n      <td>Nora assumes Carla is mistaken. Nora wants to...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>en</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 31 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                                   title  \\\n3759              \"Air Warriors\" (2014) {Harrier (#5.1)}   \n66259  \"Mission: Impossible\" (1966) {The Bargain (#3....   \n53300   \"Jjang!\" (2012) {U-KISS/ZE:A Interviews (#1.55)}   \n39564                                  \"Fun Farm\" (2014)   \n43992  \"Happy Tree Friends\" (1999/II) {Change of Hear...   \n\n                                                    plot  Action  Adult  \\\n3759    It's sleek, powerful, fast, and innovative, w...       0      0   \n66259   A former dictator, now in exile in Miami, pla...       1      0   \n53300   Happy Holidays from JJANG! This week on JJANG...       0      0   \n39564   52x7' Fun Farm is a very unique place. A farm...       0      0   \n43992   An emergency heart transplant for Disco Bear ...       0      0   \n\n       Adventure  Animation  Biography  Comedy  Crime  Documentary  ...  \\\n3759           0          0          0       0      0            1  ...   \n66259          1          0          0       0      1            0  ...   \n53300          0          0          0       0      0            0  ...   \n39564          0          1          0       0      0            0  ...   \n43992          0          1          0       1      0            0  ...   \n\n       Reality-tv  Romance  Sci-Fi  Short  Sport  Talk-Show  Thriller  War  \\\n3759            0        0       0      0      0          0         0    0   \n66259           0        0       0      0      0          0         1    0   \n53300           0        0       0      0      0          0         0    0   \n39564           0        0       0      0      0          0         0    0   \n43992           0        0       0      0      0          0         0    0   \n\n       Western  plot_lang  \n3759         0         en  \n66259        0         en  \n53300        0         en  \n39564        0         en  \n43992        0         en  \n\n[5 rows x 31 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>plot</th>\n      <th>Action</th>\n      <th>Adult</th>\n      <th>Adventure</th>\n      <th>Animation</th>\n      <th>Biography</th>\n      <th>Comedy</th>\n      <th>Crime</th>\n      <th>Documentary</th>\n      <th>...</th>\n      <th>Reality-tv</th>\n      <th>Romance</th>\n      <th>Sci-Fi</th>\n      <th>Short</th>\n      <th>Sport</th>\n      <th>Talk-Show</th>\n      <th>Thriller</th>\n      <th>War</th>\n      <th>Western</th>\n      <th>plot_lang</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3759</th>\n      <td>\"Air Warriors\" (2014) {Harrier (#5.1)}</td>\n      <td>It's sleek, powerful, fast, and innovative, w...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>en</td>\n    </tr>\n    <tr>\n      <th>66259</th>\n      <td>\"Mission: Impossible\" (1966) {The Bargain (#3....</td>\n      <td>A former dictator, now in exile in Miami, pla...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>en</td>\n    </tr>\n    <tr>\n      <th>53300</th>\n      <td>\"Jjang!\" (2012) {U-KISS/ZE:A Interviews (#1.55)}</td>\n      <td>Happy Holidays from JJANG! This week on JJANG...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>en</td>\n    </tr>\n    <tr>\n      <th>39564</th>\n      <td>\"Fun Farm\" (2014)</td>\n      <td>52x7' Fun Farm is a very unique place. A farm...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>en</td>\n    </tr>\n    <tr>\n      <th>43992</th>\n      <td>\"Happy Tree Friends\" (1999/II) {Change of Hear...</td>\n      <td>An emergency heart transplant for Disco Bear ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>en</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 31 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                                   title  \\\n42360                     \"Grange Hill\" (1978) {(#4.16)}   \n34969  \"Evil Lives Here\" (2016) {My Brother's Secret ...   \n71718                    \"Neighbours\" (1985) {(#1.7178)}   \n8291   \"Australian Story\" (1996) {A True Calling (#4....   \n85090  \"Rote Rosen\" (2006) {Eine neue Allianz (#1.2012)}   \n\n                                                    plot  Action  Adult  \\\n42360   Cathy's bunking off Cathy's group have been f...       0      0   \n34969   Danyall White always thought her brother, Ric...       0      0   \n71718   Both frustrated in love and school, Tyler tak...       0      0   \n8291    Millions of television viewers around the wor...       0      0   \n85090   Nora assumes Carla is mistaken. Nora wants to...       0      0   \n\n       Adventure  Animation  Biography  Comedy  Crime  Documentary  ...  \\\n42360          0          0          0       1      0            0  ...   \n34969          0          0          1       0      0            0  ...   \n71718          0          0          0       0      0            0  ...   \n8291           0          0          0       0      0            1  ...   \n85090          0          0          0       0      0            0  ...   \n\n       Reality-tv  Romance  Sci-Fi  Short  Sport  Talk-Show  Thriller  War  \\\n42360           0        0       0      0      0          0         0    0   \n34969           0        0       0      0      0          0         0    0   \n71718           0        1       0      0      0          0         0    0   \n8291            0        0       0      0      0          0         0    0   \n85090           0        0       0      0      0          0         0    0   \n\n       Western  plot_lang  \n42360        0         en  \n34969        0         en  \n71718        0         en  \n8291         0         en  \n85090        0         en  \n\n[5 rows x 31 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>plot</th>\n      <th>Action</th>\n      <th>Adult</th>\n      <th>Adventure</th>\n      <th>Animation</th>\n      <th>Biography</th>\n      <th>Comedy</th>\n      <th>Crime</th>\n      <th>Documentary</th>\n      <th>...</th>\n      <th>Reality-tv</th>\n      <th>Romance</th>\n      <th>Sci-Fi</th>\n      <th>Short</th>\n      <th>Sport</th>\n      <th>Talk-Show</th>\n      <th>Thriller</th>\n      <th>War</th>\n      <th>Western</th>\n      <th>plot_lang</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>42360</th>\n      <td>\"Grange Hill\" (1978) {(#4.16)}</td>\n      <td>Cathy's bunking off Cathy's group have been f...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>en</td>\n    </tr>\n    <tr>\n      <th>34969</th>\n      <td>\"Evil Lives Here\" (2016) {My Brother's Secret ...</td>\n      <td>Danyall White always thought her brother, Ric...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>en</td>\n    </tr>\n    <tr>\n      <th>71718</th>\n      <td>\"Neighbours\" (1985) {(#1.7178)}</td>\n      <td>Both frustrated in love and school, Tyler tak...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>en</td>\n    </tr>\n    <tr>\n      <th>8291</th>\n      <td>\"Australian Story\" (1996) {A True Calling (#4....</td>\n      <td>Millions of television viewers around the wor...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>en</td>\n    </tr>\n    <tr>\n      <th>85090</th>\n      <td>\"Rote Rosen\" (2006) {Eine neue Allianz (#1.2012)}</td>\n      <td>Nora assumes Carla is mistaken. Nora wants to...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>en</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 31 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative sampling\n",
    "\n",
    "The object2vec algorithm is setup as a binary classification problem. The true examples are the movie, genre pairs present in the dataset. In order to train the algorithm, we also need to provide negative examples. One option is to add all the genres to which the movie does not belong. However this strategy will create a highly skewed dataset with large percentage of negative example, as there are 27 classes present. Instead we choose to have 5 negative examples per positive example, as has been reported in related works like word2vec.\n",
    "\n",
    "Lets look at the class distribution and figure out the how much we should sample the negative examples to achieve a balanced distribution of positive and negative examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Index(['title', 'plot', 'Action', 'Adult', 'Adventure', 'Animation',\n       'Biography', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Family',\n       'Fantasy', 'Game-Show', 'History', 'Horror', 'Music', 'Musical',\n       'Mystery', 'News', 'Reality-TV', 'Reality-tv', 'Romance', 'Sci-Fi',\n       'Short', 'Sport', 'Talk-Show', 'Thriller', 'War', 'Western',\n       'plot_lang'],\n      dtype='object')"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['Action',\n 'Adult',\n 'Adventure',\n 'Animation',\n 'Biography',\n 'Comedy',\n 'Crime',\n 'Documentary',\n 'Drama',\n 'Family',\n 'Fantasy',\n 'Game-Show',\n 'History',\n 'Horror',\n 'Music',\n 'Musical',\n 'Mystery',\n 'News',\n 'Reality-TV',\n 'Reality-tv',\n 'Romance',\n 'Sci-Fi',\n 'Short',\n 'Sport',\n 'Talk-Show',\n 'Thriller',\n 'War',\n 'Western']"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "genres = list(train.columns)[2:-1]\n",
    "genres\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Number of genres:  28\n"
    }
   ],
   "source": [
    "print (\"Number of genres: \", len(genres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'Action': 'sum',\n 'Adult': 'sum',\n 'Adventure': 'sum',\n 'Animation': 'sum',\n 'Biography': 'sum',\n 'Comedy': 'sum',\n 'Crime': 'sum',\n 'Documentary': 'sum',\n 'Drama': 'sum',\n 'Family': 'sum',\n 'Fantasy': 'sum',\n 'Game-Show': 'sum',\n 'History': 'sum',\n 'Horror': 'sum',\n 'Music': 'sum',\n 'Musical': 'sum',\n 'Mystery': 'sum',\n 'News': 'sum',\n 'Reality-TV': 'sum',\n 'Reality-tv': 'sum',\n 'Romance': 'sum',\n 'Sci-Fi': 'sum',\n 'Short': 'sum',\n 'Sport': 'sum',\n 'Talk-Show': 'sum',\n 'Thriller': 'sum',\n 'War': 'sum',\n 'Western': 'sum'}"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "\n",
    "\n",
    "# create a dictionary for df aggregation, the values should be the funtion for aggregation\n",
    "agg = {genre:'sum' for genre in genres}\n",
    "type(agg)\n",
    "agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Action          7297\nAdult             70\nAdventure       6103\nAnimation       6924\nBiography        851\nComedy         20052\nCrime           8982\nDocumentary     7447\nDrama          27341\nFamily          9117\nFantasy         4466\nGame-Show       1218\nHistory         1737\nHorror          1516\nMusic           1681\nMusical          428\nMystery         6828\nNews            2544\nReality-TV      7360\nReality-tv         0\nRomance        11631\nSci-Fi          5105\nShort            472\nSport           1282\nTalk-Show       3510\nThriller        5129\nWar              839\nWestern         1503\ndtype: int64"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "agg_by_genre = train.agg(agg)\n",
    "agg_by_genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "151433"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "total_positive_samples = agg_by_genre.sum()\n",
    "total_positive_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1817135"
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "total_negative_samples = len(train)*len(genres) - total_positive_samples\n",
    "total_negative_samples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.4166806538864751"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "NEGATIVE_TO_POSITIVE_RATIO = 5\n",
    "sampling_percent = NEGATIVE_TO_POSITIVE_RATIO * total_positive_samples / total_negative_samples\n",
    "sampling_percent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "total positive examples:  151433\ntotal negative samples 1817135\nnegative sampling needed:  0.4166806538864751\n"
    }
   ],
   "source": [
    "\n",
    "print(\"total positive examples: \", total_positive_samples)\n",
    "print(\"total negative samples\", total_negative_samples)\n",
    "print(\"negative sampling needed: \", sampling_percent )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Now we can proceed to create the tokenized jsonlines dataset for training, validation and test partitions. We will use negative sampling of 0.4 for the training set, and add all the negatives for validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package stopwords to\n[nltk_data]     /home/ec2-user/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize(df, vocab, filename, negative_frac=1.0, use_stopwords=False):\n",
    "    # Rename the columns so that they are valid python identifiers\n",
    "    df = df.rename(lambda x:x.replace(\"-\", \"_\") ,axis='columns')\n",
    "\n",
    "    genres = list(df.columns)[2:-1]\n",
    "    max_seq_length = 0\n",
    "    total = len(df)\n",
    "    stop_words = set()\n",
    "\n",
    "    if use_stopwords:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    with jsonlines.open(filename, mode='w') as writer:\n",
    "        for j, row in enumerate(df.itertuples()):\n",
    "            tokens = [token for token in tokenize_plot_summary(row.plot) if token not in stop_words]\n",
    "\n",
    "            plot_token_ids = [vocab[token] if token in vocab else vocab[UNKNOWN] for token in tokens]\n",
    "            for i, genre in enumerate(genres):\n",
    "                label = getattr(row, genre)\n",
    "\n",
    "                # here we also consider generating of negative samples \n",
    "                if label == 1 or np.random.rand() < negative_frac:\n",
    "                    # All positive examples and fraction of negative examples are picked.\n",
    "                    writer.write({\"in0\": plot_token_ids, \"in1\": [i], \"label\": label})\n",
    "                    \n",
    "            max_seq_length = max(len(plot_token_ids), max_seq_length)\n",
    "            if (j+1)%1000==0:\n",
    "                sys.stdout.write(\".\")\n",
    "                sys.stdout.flush()\n",
    "        print(\"Finished tokenizing data. Max sequence length of the tokenized data: \", max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "......................................................................Finished tokenizing data. Max sequence length of the tokenized data:  1192\n"
    }
   ],
   "source": [
    "tokenize(df=train, vocab=vocab, filename=\"tokenized_movie_genres_train.jsonl\", negative_frac=0.4, use_stopwords=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": ".................Finished tokenizing data. Max sequence length of the tokenized data:  1465\n"
    }
   ],
   "source": [
    "tokenize(df=validation, vocab=vocab, filename=\"tokenized_movie_genres_validation.jsonl\", use_stopwords=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "...........................................Finished tokenizing data. Max sequence length of the tokenized data:  998\n"
    }
   ],
   "source": [
    "tokenize(df=test, vocab=vocab, filename=\"tokenized_movie_genres_test.jsonl\", use_stopwords=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For better performance, the training dataset needs to be shuffled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "!shuf tokenized_movie_genres_train.jsonl > tokenized_movie_genres_train_shuffled.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download pretrained word embeddings\n",
    "\n",
    "We will make use of pretrained word embeddings from https://nlp.stanford.edu/projects/glove/. \n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "Important: Before you begin downloading, please read the following  and make sure you are okay with the license.\n",
    "https://opendatacommons.org/licenses/pddl/1.0/\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "--2020-05-23 17:22:21--  http://nlp.stanford.edu/data/glove.840B.300d.zip\nResolving nlp.stanford.edu (nlp.stanford.edu)...171.64.67.140\nConnecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80...connected.\nHTTP request sent, awaiting response...302 Found\nLocation: https://nlp.stanford.edu/data/glove.840B.300d.zip [following]\n--2020-05-23 17:22:22--  https://nlp.stanford.edu/data/glove.840B.300d.zip\nConnecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443...connected.\nHTTP request sent, awaiting response...301 Moved Permanently\nLocation: http://downloads.cs.stanford.edu/nlp/data/glove.840B.300d.zip [following]\n--2020-05-23 17:22:25--  http://downloads.cs.stanford.edu/nlp/data/glove.840B.300d.zip\nResolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)...171.64.64.22\nConnecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80...connected.\nHTTP request sent, awaiting response...200 OK\nLength: 2176768927 (2.0G) [application/zip]\nSaving to: ‘/tmp/glove/glove.840B.300d.zip’\n\nglove.840B.300d.zip 100%[===================>]   2.03G  1.70MB/s    in 29m 50s \n\n2020-05-23 17:52:16 (1.16 MB/s) - ‘/tmp/glove/glove.840B.300d.zip’ saved [2176768927/2176768927]\n\nArchive:  /tmp/glove/glove.840B.300d.zip\n  inflating: /tmp/glove/glove.840B.300d.txt\n"
    }
   ],
   "source": [
    "# !mkdir /tmp/glove\n",
    "# !wget -P /tmp/glove/ http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
    "# !unzip -d /tmp/glove /tmp/glove/glove.840B.300d.zip\n",
    "# !rm /tmp/glove/glove.840B.300d.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sagemaker Training\n",
    "\n",
    "Let us start with defining some configurations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "bucket='md-backup-bucket-01' # customize to your bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'object2vec-movie-genre-prediction'\n",
    "\n",
    "container = get_image_uri(boto3.Session().region_name, 'object2vec')\n",
    "container\n",
    "\n",
    "train_s3_path = \"s3://{}/{}/data/train/\".format(bucket, prefix)\n",
    "validation_s3_path = \"s3://{}/{}/data/validation/\".format(bucket, prefix)\n",
    "test_s3_path = \"s3://{}/{}/data/test/\".format(bucket, prefix)\n",
    "auxiliary_s3_path = \"s3://{}/{}/data/auxiliary/\".format(bucket, prefix)\n",
    "prediction_s3_path = \"s3://{}/{}/predictions/\".format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'404615174143.dkr.ecr.us-east-2.amazonaws.com/object2vec:1'"
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "container\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Completed 342.5 MiB/346.0 MiB (564.2 KiB/s) with 1 file(s) remainingCompleted 342.8 MiB/346.0 MiB (563.9 KiB/s) with 1 file(s) remainingCompleted 343.0 MiB/346.0 MiB (561.3 KiB/s) with 1 file(s) remainingCompleted 343.3 MiB/346.0 MiB (560.6 KiB/s) with 1 file(s) remainingCompleted 343.5 MiB/346.0 MiB (559.5 KiB/s) with 1 file(s) remainingCompleted 343.8 MiB/346.0 MiB (559.6 KiB/s) with 1 file(s) remainingCompleted 344.0 MiB/346.0 MiB (559.6 KiB/s) with 1 file(s) remainingCompleted 344.3 MiB/346.0 MiB (556.9 KiB/s) with 1 file(s) remainingCompleted 344.5 MiB/346.0 MiB (554.6 KiB/s) with 1 file(s) remainingCompleted 344.8 MiB/346.0 MiB (548.0 KiB/s) with 1 file(s) remainingCompleted 345.0 MiB/346.0 MiB (545.1 KiB/s) with 1 file(s) remainingCompleted 345.3 MiB/346.0 MiB (541.5 KiB/s) with 1 file(s) remainingCompleted 345.5 MiB/346.0 MiB (529.2 KiB/s) with 1 file(s) remainingCompleted 345.8 MiB/346.0 MiB (500.6 KiB/s) with 1 file(s) remainingCompleted 346.0 MiB/346.0 MiB (490.6 KiB/s) with 1 file(s) remainingupload: ./tokenized_movie_genres_train_shuffled.jsonl to s3://md-backup-bucket-01/object2vec-movie-genre-prediction/data/train/tokenized_movie_genres_train_shuffled.jsonl\nCompleted 185.2 MiB/192.8 MiB (536.1 KiB/s) with 1 file(s) remainingCompleted 185.5 MiB/192.8 MiB (536.6 KiB/s) with 1 file(s) remainingCompleted 185.8 MiB/192.8 MiB (536.6 KiB/s) with 1 file(s) remainingCompleted 186.0 MiB/192.8 MiB (536.2 KiB/s) with 1 file(s) remainingCompleted 186.2 MiB/192.8 MiB (536.5 KiB/s) with 1 file(s) remainingCompleted 186.5 MiB/192.8 MiB (534.8 KiB/s) with 1 file(s) remainingCompleted 186.8 MiB/192.8 MiB (534.5 KiB/s) with 1 file(s) remainingCompleted 187.0 MiB/192.8 MiB (534.7 KiB/s) with 1 file(s) remainingCompleted 187.2 MiB/192.8 MiB (533.5 KiB/s) with 1 file(s) remainingCompleted 187.5 MiB/192.8 MiB (533.8 KiB/s) with 1 file(s) remainingCompleted 187.8 MiB/192.8 MiB (533.9 KiB/s) with 1 file(s) remainingCompleted 188.0 MiB/192.8 MiB (534.1 KiB/s) with 1 file(s) remainingCompleted 188.2 MiB/192.8 MiB (533.8 KiB/s) with 1 file(s) remainingCompleted 188.5 MiB/192.8 MiB (532.8 KiB/s) with 1 file(s) remainingCompleted 188.8 MiB/192.8 MiB (533.4 KiB/s) with 1 file(s) remainingCompleted 188.8 MiB/192.8 MiB (531.8 KiB/s) with 1 file(s) remainingCompleted 189.1 MiB/192.8 MiB (532.0 KiB/s) with 1 file(s) remainingCompleted 189.3 MiB/192.8 MiB (532.4 KiB/s) with 1 file(s) remainingCompleted 189.6 MiB/192.8 MiB (523.0 KiB/s) with 1 file(s) remainingCompleted 189.8 MiB/192.8 MiB (487.8 KiB/s) with 1 file(s) remainingCompleted 190.1 MiB/192.8 MiB (478.7 KiB/s) with 1 file(s) remainingCompleted 190.3 MiB/192.8 MiB (466.1 KiB/s) with 1 file(s) remainingCompleted 190.6 MiB/192.8 MiB (461.0 KiB/s) with 1 file(s) remainingCompleted 190.8 MiB/192.8 MiB (459.5 KiB/s) with 1 file(s) remainingCompleted 191.1 MiB/192.8 MiB (458.6 KiB/s) with 1 file(s) remainingCompleted 191.3 MiB/192.8 MiB (456.8 KiB/s) with 1 file(s) remainingCompleted 191.6 MiB/192.8 MiB (402.2 KiB/s) with 1 file(s) remainingCompleted 191.8 MiB/192.8 MiB (401.6 KiB/s) with 1 file(s) remainingCompleted 192.1 MiB/192.8 MiB (401.1 KiB/s) with 1 file(s) remainingCompleted 192.3 MiB/192.8 MiB (401.1 KiB/s) with 1 file(s) remainingCompleted 192.6 MiB/192.8 MiB (401.3 KiB/s) with 1 file(s) remainingCompleted 192.8 MiB/192.8 MiB (398.2 KiB/s) with 1 file(s) remainingupload: ./tokenized_movie_genres_validation.jsonl to s3://md-backup-bucket-01/object2vec-movie-genre-prediction/data/validation/tokenized_movie_genres_validation.jsonl\nCompleted 475.3 MiB/477.8 MiB (623.8 KiB/s) with 1 file(s) remainingCompleted 475.5 MiB/477.8 MiB (623.8 KiB/s) with 1 file(s) remainingCompleted 475.8 MiB/477.8 MiB (623.7 KiB/s) with 1 file(s) remainingCompleted 476.0 MiB/477.8 MiB (623.4 KiB/s) with 1 file(s) remainingCompleted 476.3 MiB/477.8 MiB (622.7 KiB/s) with 1 file(s) remainingCompleted 476.5 MiB/477.8 MiB (620.3 KiB/s) with 1 file(s) remainingCompleted 476.8 MiB/477.8 MiB (595.7 KiB/s) with 1 file(s) remainingCompleted 477.0 MiB/477.8 MiB (544.5 KiB/s) with 1 file(s) remainingCompleted 477.3 MiB/477.8 MiB (523.8 KiB/s) with 1 file(s) remainingCompleted 477.5 MiB/477.8 MiB (510.8 KiB/s) with 1 file(s) remainingCompleted 477.8 MiB/477.8 MiB (498.7 KiB/s) with 1 file(s) remainingupload: ./tokenized_movie_genres_test.jsonl to s3://md-backup-bucket-01/object2vec-movie-genre-prediction/data/test/tokenized_movie_genres_test.jsonl\n"
    }
   ],
   "source": [
    "!aws s3 cp tokenized_movie_genres_train_shuffled.jsonl {train_s3_path}\n",
    "!aws s3 cp tokenized_movie_genres_validation.jsonl {validation_s3_path}\n",
    "!aws s3 cp tokenized_movie_genres_test.jsonl {test_s3_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "upload: ./vocab.json to s3://md-backup-bucket-01/object2vec-movie-genre-prediction/data/auxiliary/vocab.json\nCompleted 5.2 GiB/5.3 GiB (627.9 KiB/s) with 1 file(s) remainingCompleted 5.2 GiB/5.3 GiB (627.8 KiB/s) with 1 file(s) remainingCompleted 5.2 GiB/5.3 GiB (627.8 KiB/s) with 1 file(s) remainingCompleted 5.2 GiB/5.3 GiB (627.8 KiB/s) with 1 file(s) remainingCompleted 5.2 GiB/5.3 GiB (627.8 KiB/s) with 1 file(s) remainingCompleted 5.2 GiB/5.3 GiB (627.8 KiB/s) with 1 file(s) remainingCompleted 5.2 GiB/5.3 GiB (627.8 KiB/s) with 1 file(s) remainingCompleted 5.2 GiB/5.3 GiB (627.9 KiB/s) with 1 file(s) remainingCompleted 5.2 GiB/5.3 GiB (627.8 KiB/s) with 1 file(s) remainingCompleted 5.2 GiB/5.3 GiB (627.8 KiB/s) with 1 file(s) remainingCompleted 5.2 GiB/5.3 GiB (627.8 KiB/s) with 1 file(s) remainingCompleted 5.2 GiB/5.3 GiB (627.8 KiB/s) with 1 file(s) remainingCompleted 5.2 GiB/5.3 GiB (627.7 KiB/s) with 1 file(s) remainingCompleted 5.2 GiB/5.3 GiB (627.4 KiB/s) with 1 file(s) remainingCompleted 5.2 GiB/5.3 GiB (627.3 KiB/s) with 1 file(s) remainingCompleted 5.2 GiB/5.3 GiB (627.2 KiB/s) with 1 file(s) remainingCompleted 5.2 GiB/5.3 GiB (627.2 KiB/s) with 1 file(s) remainingCompleted 5.3 GiB/5.3 GiB (627.2 KiB/s) with 1 file(s) remainingCompleted 5.3 GiB/5.3 GiB (627.2 KiB/s) with 1 file(s) remainingCompleted 5.3 GiB/5.3 GiB (627.2 KiB/s) with 1 file(s) remainingCompleted 5.3 GiB/5.3 GiB (627.2 KiB/s) with 1 file(s) remainingCompleted 5.3 GiB/5.3 GiB (627.2 KiB/s) with 1 file(s) remainingCompleted 5.3 GiB/5.3 GiB (627.3 KiB/s) with 1 file(s) remainingCompleted 5.3 GiB/5.3 GiB (627.2 KiB/s) with 1 file(s) remainingCompleted 5.3 GiB/5.3 GiB (627.0 KiB/s) with 1 file(s) remainingCompleted 5.3 GiB/5.3 GiB (626.6 KiB/s) with 1 file(s) remainingCompleted 5.3 GiB/5.3 GiB (626.5 KiB/s) with 1 file(s) remainingCompleted 5.3 GiB/5.3 GiB (626.4 KiB/s) with 1 file(s) remainingCompleted 5.3 GiB/5.3 GiB (626.4 KiB/s) with 1 file(s) remainingCompleted 5.3 GiB/5.3 GiB (626.4 KiB/s) with 1 file(s) remainingCompleted 5.3 GiB/5.3 GiB (626.3 KiB/s) with 1 file(s) remainingCompleted 5.3 GiB/5.3 GiB (626.3 KiB/s) with 1 file(s) remainingCompleted 5.3 GiB/5.3 GiB (626.2 KiB/s) with 1 file(s) remainingCompleted 5.3 GiB/5.3 GiB (626.2 KiB/s) with 1 file(s) remainingCompleted 5.3 GiB/5.3 GiB (626.1 KiB/s) with 1 file(s) remainingCompleted 5.3 GiB/5.3 GiB (625.7 KiB/s) with 1 file(s) remainingCompleted 5.3 GiB/5.3 GiB (625.5 KiB/s) with 1 file(s) remainingCompleted 5.3 GiB/5.3 GiB (625.2 KiB/s) with 1 file(s) remainingCompleted 5.3 GiB/5.3 GiB (625.2 KiB/s) with 1 file(s) remainingCompleted 5.3 GiB/5.3 GiB (625.1 KiB/s) with 1 file(s) remainingCompleted 5.3 GiB/5.3 GiB (624.9 KiB/s) with 1 file(s) remainingCompleted 5.3 GiB/5.3 GiB (624.9 KiB/s) with 1 file(s) remainingCompleted 5.3 GiB/5.3 GiB (624.8 KiB/s) with 1 file(s) remainingCompleted 5.3 GiB/5.3 GiB (624.5 KiB/s) with 1 file(s) remainingCompleted 5.3 GiB/5.3 GiB (624.3 KiB/s) with 1 file(s) remainingCompleted 5.3 GiB/5.3 GiB (623.6 KiB/s) with 1 file(s) remainingCompleted 5.3 GiB/5.3 GiB (623.4 KiB/s) with 1 file(s) remainingCompleted 5.3 GiB/5.3 GiB (623.4 KiB/s) with 1 file(s) remainingCompleted 5.3 GiB/5.3 GiB (622.8 KiB/s) with 1 file(s) remainingCompleted 5.3 GiB/5.3 GiB (622.7 KiB/s) with 1 file(s) remainingCompleted 5.3 GiB/5.3 GiB (622.6 KiB/s) with 1 file(s) remainingCompleted 5.3 GiB/5.3 GiB (622.5 KiB/s) with 1 file(s) remainingupload: ../../../../../../../tmp/glove/glove.840B.300d.txt to s3://md-backup-bucket-01/object2vec-movie-genre-prediction/data/auxiliary/glove.840B.300d.txt\n"
    }
   ],
   "source": [
    "!aws s3 cp vocab.json {auxiliary_s3_path}\n",
    "!aws s3 cp /tmp/glove/glove.840B.300d.txt {auxiliary_s3_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training hyperparameters\n",
    "\n",
    "The object2vec is a customizable algorithm and hence it has quite a few hyperparameters. Lets review some of the important ones:\n",
    "\n",
    "* **enc_dim**: The dimension of the encoder. Both the movie plot description and genre embeddings are mapped to this dimension. \n",
    "* **mlp_dim**: The dimension of the output from multilayer perceptron (MLP) layers.\n",
    "* **mlp_activation**: Type of activation function for the multilayer perceptron (MLP) layer.\n",
    "* **mlp_layers**: The number of multilayer perceptron (MLP) layers in the network.\n",
    "* **output_layer**: The type of output layer. We choose 'softmax' as it is a classification problem.\n",
    "* **bucket_width**: The allowed difference between data sequence length when bucketing is enabled. Bucketing is enabled when a non-zero value is specified for this parameter.\n",
    "* **num_classes**: The number of classes for classification training, which is 2 for our case.\n",
    "\n",
    "The **enc0** encodes the movie plot description which is a sequence, and **enc1** encodes the movie genre which is a single token. The encoder parameters:\n",
    "\n",
    "* **max_seq_len**: The maximum sequence length that will be considered. Any input tokens beyond max_seq_len will be truncated and ignored. We choose a value of 500 for enc\n",
    "* **network**: Network model. We choose hcnn for both enc0 and enc1.\n",
    "* **cnn_filter_width**: The filter width of the hcnn encoder.\n",
    "* **layers**: The number of layers. We choose 2 layers for enc0, as we want to capture richer structures in the movie plot description which is a sequence input. For enc1, we choose 1 layer.\n",
    "* **token_embedding_dim**: The output dimension of  token embedding layer. We choose a dimension of 300 for encoder 0, consistent with the dimension of the glove embdeddings. For enc1, we choose 10.\n",
    "* **pretrained_embedding_file**: The filename of pretrained token embedding file present in the auxiliary data channel. We use the glove embeddings for enc0. For enc1, the embeddings will be learned by the algorithm.\n",
    "* **freeze_pretrained_embedding**: Whether to freeze  pretrained embedding weights. We set this to True for enc0.\n",
    "* **vocab_file**: The vocabulary file for mapping pretrained token embeddings to vocabulary IDs. This is specified only for enc0, as we use pretrained embeddings only for enc0.\n",
    "* **vocab_size**: The vocabulary size of the tokens. For enc0, it is the number of words appearing the dataset. For enc1, it is the number of genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    " 'enc_dim': 4096, \n",
    " 'mlp_dim': 512, \n",
    " 'mlp_activation': 'relu', \n",
    " 'mlp_layers': 2, \n",
    " 'output_layer': 'softmax',\n",
    " 'bucket_width': 10, \n",
    " 'num_classes': 2,\n",
    " \n",
    " 'mini_batch_size': 256,\n",
    " \n",
    " 'enc0_max_seq_len': 500,\n",
    " 'enc1_max_seq_len': 2,\n",
    " \n",
    " 'enc0_network': 'hcnn',\n",
    " 'enc1_network': 'hcnn',\n",
    "    \n",
    " 'enc0_layers': '2',\n",
    " 'enc1_layers': '1',\n",
    "    \n",
    " 'enc0_cnn_filter_width': 2,\n",
    " 'enc1_cnn_filter_width': 1,\n",
    " \n",
    " 'enc0_token_embedding_dim': 300,\n",
    " 'enc1_token_embedding_dim': 10,\n",
    " \n",
    " 'enc0_pretrained_embedding_file' : \"glove.840B.300d.txt\",\n",
    " \n",
    " 'enc0_freeze_pretrained_embedding': 'true',\n",
    " \n",
    " 'enc0_vocab_file': 'vocab.json',\n",
    " 'enc1_vocab_file': '',\n",
    " \n",
    " 'enc0_vocab_size': len(vocab),\n",
    " 'enc1_vocab_size': len(genres),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "Note that the training will take approximately 1.5 hours to complete on the ml.p2.8xlarge instance type\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "o2v = sagemaker.estimator.Estimator(container,\n",
    "                                    # get_execution_role(),\n",
    "                                    role, \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.p3.8xlarge',\n",
    "                                    output_path=\"s3://{}/{}/output\".format(bucket, prefix),\n",
    "                                   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "20 21:45:44 INFO 140260686526272] Epoch: 5, batches: 2600, num_examples: 665600, 6346.9 samples/sec, epoch time so far: 0:01:44.870096\u001b[0m\n\u001b[34m[05/23/2020 21:45:44 INFO 140260686526272] #011Training metrics: perplexity: 1.052 cross_entropy: 0.051 accuracy: 0.981 \u001b[0m\n\u001b[34m[05/23/2020 21:45:48 INFO 140260686526272] Epoch: 5, batches: 2700, num_examples: 691200, 6330.2 samples/sec, epoch time so far: 0:01:49.191039\u001b[0m\n\u001b[34m[05/23/2020 21:45:48 INFO 140260686526272] #011Training metrics: perplexity: 1.052 cross_entropy: 0.051 accuracy: 0.980 \u001b[0m\n\u001b[34m[05/23/2020 21:45:52 INFO 140260686526272] Epoch: 5, batches: 2800, num_examples: 716800, 6334.0 samples/sec, epoch time so far: 0:01:53.167647\u001b[0m\n\u001b[34m[05/23/2020 21:45:52 INFO 140260686526272] #011Training metrics: perplexity: 1.053 cross_entropy: 0.051 accuracy: 0.980 \u001b[0m\n\u001b[34m[05/23/2020 21:45:56 INFO 140260686526272] Epoch: 5, batches: 2900, num_examples: 742400, 6325.2 samples/sec, epoch time so far: 0:01:57.372203\u001b[0m\n\u001b[34m[05/23/2020 21:45:56 INFO 140260686526272] #011Training metrics: perplexity: 1.053 cross_entropy: 0.051 accuracy: 0.980 \u001b[0m\n\u001b[34m[05/23/2020 21:46:00 INFO 140260686526272] Epoch: 5, batches: 3000, num_examples: 768000, 6325.9 samples/sec, epoch time so far: 0:02:01.405805\u001b[0m\n\u001b[34m[05/23/2020 21:46:00 INFO 140260686526272] #011Training metrics: perplexity: 1.053 cross_entropy: 0.052 accuracy: 0.980 \u001b[0m\n\u001b[34m[05/23/2020 21:46:04 INFO 140260686526272] Epoch: 5, batches: 3100, num_examples: 793600, 6314.3 samples/sec, epoch time so far: 0:02:05.682593\u001b[0m\n\u001b[34m[05/23/2020 21:46:04 INFO 140260686526272] #011Training metrics: perplexity: 1.053 cross_entropy: 0.052 accuracy: 0.980 \u001b[0m\n\u001b[34m[05/23/2020 21:46:08 INFO 140260686526272] Epoch: 5, batches: 3200, num_examples: 819200, 6323.7 samples/sec, epoch time so far: 0:02:09.544327\u001b[0m\n\u001b[34m[05/23/2020 21:46:08 INFO 140260686526272] #011Training metrics: perplexity: 1.053 cross_entropy: 0.052 accuracy: 0.980 \u001b[0m\n\u001b[34m[05/23/2020 21:46:12 INFO 140260686526272] Epoch: 5, batches: 3300, num_examples: 844800, 6321.2 samples/sec, epoch time so far: 0:02:13.644935\u001b[0m\n\u001b[34m[05/23/2020 21:46:12 INFO 140260686526272] #011Training metrics: perplexity: 1.054 cross_entropy: 0.052 accuracy: 0.980 \u001b[0m\n\u001b[34m[05/23/2020 21:46:16 INFO 140260686526272] Epoch: 5, batches: 3400, num_examples: 870400, 6333.0 samples/sec, epoch time so far: 0:02:17.438164\u001b[0m\n\u001b[34m[05/23/2020 21:46:16 INFO 140260686526272] #011Training metrics: perplexity: 1.054 cross_entropy: 0.052 accuracy: 0.980 \u001b[0m\n\u001b[34m[05/23/2020 21:46:19 INFO 140260686526272] **************\u001b[0m\n\u001b[34m[05/23/2020 21:46:19 INFO 140260686526272] Completed Epoch: 5, time taken: 0:02:19.755670\u001b[0m\n\u001b[34m[05/23/2020 21:46:19 INFO 140260686526272] Epoch 5 Training metrics:   perplexity: 1.054 cross_entropy: 0.052 accuracy: 0.980 \u001b[0m\n\u001b[34m[05/23/2020 21:46:19 INFO 140260686526272] #quality_metric: host=algo-1, epoch=5, train cross_entropy <loss>=0.0524896389439\u001b[0m\n\u001b[34m[05/23/2020 21:46:19 INFO 140260686526272] #quality_metric: host=algo-1, epoch=5, train accuracy <score>=0.979847455176\u001b[0m\n\u001b[34m[05/23/2020 21:46:44 INFO 140260686526272] Epoch 5 Validation metrics: perplexity: 1.168 cross_entropy: 0.155 accuracy: 0.953 \u001b[0m\n\u001b[34m[05/23/2020 21:46:44 INFO 140260686526272] #quality_metric: host=algo-1, epoch=5, validation cross_entropy <loss>=0.155124376891\u001b[0m\n\u001b[34m[05/23/2020 21:46:44 INFO 140260686526272] #quality_metric: host=algo-1, epoch=5, validation accuracy <score>=0.952771347737\u001b[0m\n\u001b[34m[05/23/2020 21:46:44 INFO 140260686526272] **************\u001b[0m\n\u001b[34m[05/23/2020 21:46:44 INFO 140260686526272] patience losses: [0.13914461252006508, 0.13001383112594625, 0.13538903106657818]\u001b[0m\n\u001b[34m[05/23/2020 21:46:44 INFO 140260686526272] min patience losses: 0.130013831126\u001b[0m\n\u001b[34m[05/23/2020 21:46:44 INFO 140260686526272] current loss: 0.155124376891\u001b[0m\n\u001b[34m[05/23/2020 21:46:44 INFO 140260686526272] absolute loss difference: 0.0251105457647\u001b[0m\n\u001b[34m[05/23/2020 21:46:44 INFO 140260686526272] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n\u001b[34m#metrics {\"Metrics\": {\"early_stop.time\": {\"count\": 1, \"max\": 0.43082237243652344, \"sum\": 0.43082237243652344, \"min\": 0.43082237243652344}, \"update.time\": {\"count\": 1, \"max\": 165271.30913734436, \"sum\": 165271.30913734436, \"min\": 165271.30913734436}}, \"EndTime\": 1590270404.578701, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"ObjectToVec\"}, \"StartTime\": 1590270239.057169}\n\u001b[0m\n\u001b[34m[05/23/2020 21:46:44 INFO 140260686526272] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 3458, \"sum\": 3458.0, \"min\": 3458}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 3458, \"sum\": 3458.0, \"min\": 3458}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 885248, \"sum\": 885248.0, \"min\": 885248}, \"Total Batches Seen\": {\"count\": 1, \"max\": 20748, \"sum\": 20748.0, \"min\": 20748}, \"Total Records Seen\": {\"count\": 1, \"max\": 5311488, \"sum\": 5311488.0, \"min\": 5311488}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 885248, \"sum\": 885248.0, \"min\": 885248}, \"Reset Count\": {\"count\": 1, \"max\": 6, \"sum\": 6.0, \"min\": 6}}, \"EndTime\": 1590270404.580283, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"ObjectToVec\", \"epoch\": 5}, \"StartTime\": 1590270239.307366}\n\u001b[0m\n\u001b[34m[05/23/2020 21:46:44 INFO 140260686526272] #throughput_metric: host=algo-1, train throughput=5356.27569852 records/second\u001b[0m\n\u001b[34m[05/23/2020 21:46:48 INFO 140260686526272] Epoch: 6, batches: 100, num_examples: 25600, 6340.3 samples/sec, epoch time so far: 0:00:04.037693\u001b[0m\n\u001b[34m[05/23/2020 21:46:48 INFO 140260686526272] #011Training metrics: perplexity: 1.026 cross_entropy: 0.025 accuracy: 0.991 \u001b[0m\n\u001b[34m[05/23/2020 21:46:52 INFO 140260686526272] Epoch: 6, batches: 200, num_examples: 51200, 6371.4 samples/sec, epoch time so far: 0:00:08.035899\u001b[0m\n\u001b[34m[05/23/2020 21:46:52 INFO 140260686526272] #011Training metrics: perplexity: 1.028 cross_entropy: 0.027 accuracy: 0.991 \u001b[0m\n\u001b[34m[05/23/2020 21:46:57 INFO 140260686526272] Epoch: 6, batches: 300, num_examples: 76800, 6264.7 samples/sec, epoch time so far: 0:00:12.259108\u001b[0m\n\u001b[34m[05/23/2020 21:46:57 INFO 140260686526272] #011Training metrics: perplexity: 1.029 cross_entropy: 0.029 accuracy: 0.990 \u001b[0m\n\u001b[34m[05/23/2020 21:47:00 INFO 140260686526272] Epoch: 6, batches: 400, num_examples: 102400, 6429.5 samples/sec, epoch time so far: 0:00:15.926656\u001b[0m\n\u001b[34m[05/23/2020 21:47:00 INFO 140260686526272] #011Training metrics: perplexity: 1.029 cross_entropy: 0.029 accuracy: 0.990 \u001b[0m\n\u001b[34m[05/23/2020 21:47:05 INFO 140260686526272] Epoch: 6, batches: 500, num_examples: 128000, 6320.6 samples/sec, epoch time so far: 0:00:20.251110\u001b[0m\n\u001b[34m[05/23/2020 21:47:05 INFO 140260686526272] #011Training metrics: perplexity: 1.031 cross_entropy: 0.030 accuracy: 0.989 \u001b[0m\n\u001b[34m[05/23/2020 21:47:09 INFO 140260686526272] Epoch: 6, batches: 600, num_examples: 153600, 6304.8 samples/sec, epoch time so far: 0:00:24.362538\u001b[0m\n\u001b[34m[05/23/2020 21:47:09 INFO 140260686526272] #011Training metrics: perplexity: 1.031 cross_entropy: 0.031 accuracy: 0.989 \u001b[0m\n\u001b[34m[05/23/2020 21:47:13 INFO 140260686526272] Epoch: 6, batches: 700, num_examples: 179200, 6296.0 samples/sec, epoch time so far: 0:00:28.462298\u001b[0m\n\u001b[34m[05/23/2020 21:47:13 INFO 140260686526272] #011Training metrics: perplexity: 1.032 cross_entropy: 0.031 accuracy: 0.989 \u001b[0m\n\u001b[34m[05/23/2020 21:47:17 INFO 140260686526272] Epoch: 6, batches: 800, num_examples: 204800, 6306.7 samples/sec, epoch time so far: 0:00:32.473379\u001b[0m\n\u001b[34m[05/23/2020 21:47:17 INFO 140260686526272] #011Training metrics: perplexity: 1.033 cross_entropy: 0.033 accuracy: 0.988 \u001b[0m\n\u001b[34m[05/23/2020 21:47:21 INFO 140260686526272] Epoch: 6, batches: 900, num_examples: 230400, 6338.4 samples/sec, epoch time so far: 0:00:36.350040\u001b[0m\n\u001b[34m[05/23/2020 21:47:21 INFO 140260686526272] #011Training metrics: perplexity: 1.033 cross_entropy: 0.033 accuracy: 0.988 \u001b[0m\n\u001b[34m[05/23/2020 21:47:25 INFO 140260686526272] Epoch: 6, batches: 1000, num_examples: 256000, 6316.5 samples/sec, epoch time so far: 0:00:40.529032\u001b[0m\n\u001b[34m[05/23/2020 21:47:25 INFO 140260686526272] #011Training metrics: perplexity: 1.034 cross_entropy: 0.033 accuracy: 0.988 \u001b[0m\n\u001b[34m[05/23/2020 21:47:29 INFO 140260686526272] Epoch: 6, batches: 1100, num_examples: 281600, 6309.5 samples/sec, epoch time so far: 0:00:44.631106\u001b[0m\n\u001b[34m[05/23/2020 21:47:29 INFO 140260686526272] #011Training metrics: perplexity: 1.034 cross_entropy: 0.034 accuracy: 0.988 \u001b[0m\n\u001b[34m[05/23/2020 21:47:33 INFO 140260686526272] Epoch: 6, batches: 1200, num_examples: 307200, 6323.7 samples/sec, epoch time so far: 0:00:48.579098\u001b[0m\n\u001b[34m[05/23/2020 21:47:33 INFO 140260686526272] #011Training metrics: perplexity: 1.034 cross_entropy: 0.034 accuracy: 0.988 \u001b[0m\n\u001b[34m[05/23/2020 21:47:37 INFO 140260686526272] Epoch: 6, batches: 1300, num_examples: 332800, 6318.2 samples/sec, epoch time so far: 0:00:52.673168\u001b[0m\n\u001b[34m[05/23/2020 21:47:37 INFO 140260686526272] #011Training metrics: perplexity: 1.035 cross_entropy: 0.034 accuracy: 0.988 \u001b[0m\n\u001b[34m[05/23/2020 21:47:41 INFO 140260686526272] Epoch: 6, batches: 1400, num_examples: 358400, 6319.7 samples/sec, epoch time so far: 0:00:56.711960\u001b[0m\n\u001b[34m[05/23/2020 21:47:41 INFO 140260686526272] #011Training metrics: perplexity: 1.035 cross_entropy: 0.035 accuracy: 0.987 \u001b[0m\n\u001b[34m[05/23/2020 21:47:45 INFO 140260686526272] Epoch: 6, batches: 1500, num_examples: 384000, 6304.1 samples/sec, epoch time so far: 0:01:00.912520\u001b[0m\n\u001b[34m[05/23/2020 21:47:45 INFO 140260686526272] #011Training metrics: perplexity: 1.036 cross_entropy: 0.035 accuracy: 0.987 \u001b[0m\n\u001b[34m[05/23/2020 21:47:49 INFO 140260686526272] Epoch: 6, batches: 1600, num_examples: 409600, 6306.1 samples/sec, epoch time so far: 0:01:04.953040\u001b[0m\n\u001b[34m[05/23/2020 21:47:49 INFO 140260686526272] #011Training metrics: perplexity: 1.036 cross_entropy: 0.036 accuracy: 0.987 \u001b[0m\n\u001b[34m[05/23/2020 21:47:53 INFO 140260686526272] Epoch: 6, batches: 1700, num_examples: 435200, 6320.3 samples/sec, epoch time so far: 0:01:08.857898\u001b[0m\n\u001b[34m[05/23/2020 21:47:53 INFO 140260686526272] #011Training metrics: perplexity: 1.037 cross_entropy: 0.036 accuracy: 0.987 \u001b[0m\n\u001b[34m[05/23/2020 21:47:58 INFO 140260686526272] Epoch: 6, batches: 1800, num_examples: 460800, 6273.3 samples/sec, epoch time so far: 0:01:13.454662\u001b[0m\n\u001b[34m[05/23/2020 21:47:58 INFO 140260686526272] #011Training metrics: perplexity: 1.037 cross_entropy: 0.036 accuracy: 0.987 \u001b[0m\n\u001b[34m[05/23/2020 21:48:02 INFO 140260686526272] Epoch: 6, batches: 1900, num_examples: 486400, 6263.7 samples/sec, epoch time so far: 0:01:17.654380\u001b[0m\n\u001b[34m[05/23/2020 21:48:02 INFO 140260686526272] #011Training metrics: perplexity: 1.037 cross_entropy: 0.037 accuracy: 0.986 \u001b[0m\n\u001b[34m[05/23/2020 21:48:06 INFO 140260686526272] Epoch: 6, batches: 2000, num_examples: 512000, 6301.1 samples/sec, epoch time so far: 0:01:21.256097\u001b[0m\n\u001b[34m[05/23/2020 21:48:06 INFO 140260686526272] #011Training metrics: perplexity: 1.037 cross_entropy: 0.037 accuracy: 0.986 \u001b[0m\n\u001b[34m[05/23/2020 21:48:10 INFO 140260686526272] Epoch: 6, batches: 2100, num_examples: 537600, 6290.9 samples/sec, epoch time so far: 0:01:25.456997\u001b[0m\n\u001b[34m[05/23/2020 21:48:10 INFO 140260686526272] #011Training metrics: perplexity: 1.038 cross_entropy: 0.037 accuracy: 0.986 \u001b[0m\n\u001b[34m[05/23/2020 21:48:14 INFO 140260686526272] Epoch: 6, batches: 2200, num_examples: 563200, 6311.2 samples/sec, epoch time so far: 0:01:29.238678\u001b[0m\n\u001b[34m[05/23/2020 21:48:14 INFO 140260686526272] #011Training metrics: perplexity: 1.038 cross_entropy: 0.037 accuracy: 0.986 \u001b[0m\n\u001b[34m[05/23/2020 21:48:17 INFO 140260686526272] Epoch: 6, batches: 2300, num_examples: 588800, 6322.5 samples/sec, epoch time so far: 0:01:33.127311\u001b[0m\n\u001b[34m[05/23/2020 21:48:17 INFO 140260686526272] #011Training metrics: perplexity: 1.038 cross_entropy: 0.038 accuracy: 0.986 \u001b[0m\n\u001b[34m[05/23/2020 21:48:22 INFO 140260686526272] Epoch: 6, batches: 2400, num_examples: 614400, 6321.7 samples/sec, epoch time so far: 0:01:37.188286\u001b[0m\n\u001b[34m[05/23/2020 21:48:22 INFO 140260686526272] #011Training metrics: perplexity: 1.039 cross_entropy: 0.038 accuracy: 0.986 \u001b[0m\n\u001b[34m[05/23/2020 21:48:26 INFO 140260686526272] Epoch: 6, batches: 2500, num_examples: 640000, 6313.2 samples/sec, epoch time so far: 0:01:41.374828\u001b[0m\n\u001b[34m[05/23/2020 21:48:26 INFO 140260686526272] #011Training metrics: perplexity: 1.039 cross_entropy: 0.038 accuracy: 0.986 \u001b[0m\n\u001b[34m[05/23/2020 21:48:30 INFO 140260686526272] Epoch: 6, batches: 2600, num_examples: 665600, 6308.4 samples/sec, epoch time so far: 0:01:45.510892\u001b[0m\n\u001b[34m[05/23/2020 21:48:30 INFO 140260686526272] #011Training metrics: perplexity: 1.039 cross_entropy: 0.039 accuracy: 0.986 \u001b[0m\n\u001b[34m[05/23/2020 21:48:34 INFO 140260686526272] Epoch: 6, batches: 2700, num_examples: 691200, 6305.3 samples/sec, epoch time so far: 0:01:49.621954\u001b[0m\n\u001b[34m[05/23/2020 21:48:34 INFO 140260686526272] #011Training metrics: perplexity: 1.040 cross_entropy: 0.039 accuracy: 0.985 \u001b[0m\n\u001b[34m[05/23/2020 21:48:38 INFO 140260686526272] Epoch: 6, batches: 2800, num_examples: 716800, 6309.8 samples/sec, epoch time so far: 0:01:53.600308\u001b[0m\n\u001b[34m[05/23/2020 21:48:38 INFO 140260686526272] #011Training metrics: perplexity: 1.040 cross_entropy: 0.039 accuracy: 0.985 \u001b[0m\n\u001b[34m[05/23/2020 21:48:42 INFO 140260686526272] Epoch: 6, batches: 2900, num_examples: 742400, 6293.8 samples/sec, epoch time so far: 0:01:57.957973\u001b[0m\n\u001b[34m[05/23/2020 21:48:42 INFO 140260686526272] #011Training metrics: perplexity: 1.040 cross_entropy: 0.039 accuracy: 0.985 \u001b[0m\n\u001b[34m[05/23/2020 21:48:46 INFO 140260686526272] Epoch: 6, batches: 3000, num_examples: 768000, 6312.0 samples/sec, epoch time so far: 0:02:01.673407\u001b[0m\n\u001b[34m[05/23/2020 21:48:46 INFO 140260686526272] #011Training metrics: perplexity: 1.041 cross_entropy: 0.040 accuracy: 0.985 \u001b[0m\n\u001b[34m[05/23/2020 21:48:50 INFO 140260686526272] Epoch: 6, batches: 3100, num_examples: 793600, 6315.7 samples/sec, epoch time so far: 0:02:05.655887\u001b[0m\n\u001b[34m[05/23/2020 21:48:50 INFO 140260686526272] #011Training metrics: perplexity: 1.041 cross_entropy: 0.040 accuracy: 0.985 \u001b[0m\n\u001b[34m[05/23/2020 21:48:54 INFO 140260686526272] Epoch: 6, batches: 3200, num_examples: 819200, 6313.1 samples/sec, epoch time so far: 0:02:09.762180\u001b[0m\n\u001b[34m[05/23/2020 21:48:54 INFO 140260686526272] #011Training metrics: perplexity: 1.041 cross_entropy: 0.040 accuracy: 0.985 \u001b[0m\n\u001b[34m[05/23/2020 21:48:58 INFO 140260686526272] Epoch: 6, batches: 3300, num_examples: 844800, 6318.2 samples/sec, epoch time so far: 0:02:13.709086\u001b[0m\n\u001b[34m[05/23/2020 21:48:58 INFO 140260686526272] #011Training metrics: perplexity: 1.041 cross_entropy: 0.041 accuracy: 0.985 \u001b[0m\n\u001b[34m[05/23/2020 21:49:02 INFO 140260686526272] Epoch: 6, batches: 3400, num_examples: 870400, 6320.8 samples/sec, epoch time so far: 0:02:17.704681\u001b[0m\n\u001b[34m[05/23/2020 21:49:02 INFO 140260686526272] #011Training metrics: perplexity: 1.042 cross_entropy: 0.041 accuracy: 0.985 \u001b[0m\n\u001b[34m[05/23/2020 21:49:04 INFO 140260686526272] **************\u001b[0m\n\u001b[34m[05/23/2020 21:49:04 INFO 140260686526272] Completed Epoch: 6, time taken: 0:02:19.838812\u001b[0m\n\u001b[34m[05/23/2020 21:49:04 INFO 140260686526272] Epoch 6 Training metrics:   perplexity: 1.042 cross_entropy: 0.041 accuracy: 0.985 \u001b[0m\n\u001b[34m[05/23/2020 21:49:04 INFO 140260686526272] #quality_metric: host=algo-1, epoch=6, train cross_entropy <loss>=0.0410588992471\u001b[0m\n\u001b[34m[05/23/2020 21:49:04 INFO 140260686526272] #quality_metric: host=algo-1, epoch=6, train accuracy <score>=0.984552351431\u001b[0m\n\u001b[34m[05/23/2020 21:49:30 INFO 140260686526272] Epoch 6 Validation metrics: perplexity: 1.195 cross_entropy: 0.178 accuracy: 0.948 \u001b[0m\n\u001b[34m[05/23/2020 21:49:30 INFO 140260686526272] #quality_metric: host=algo-1, epoch=6, validation cross_entropy <loss>=0.178127800685\u001b[0m\n\u001b[34m[05/23/2020 21:49:30 INFO 140260686526272] #quality_metric: host=algo-1, epoch=6, validation accuracy <score>=0.947991014017\u001b[0m\n\u001b[34m[05/23/2020 21:49:30 INFO 140260686526272] **************\u001b[0m\n\u001b[34m[05/23/2020 21:49:30 INFO 140260686526272] patience losses: [0.13001383112594625, 0.13538903106657818, 0.15512437689067754]\u001b[0m\n\u001b[34m[05/23/2020 21:49:30 INFO 140260686526272] min patience losses: 0.130013831126\u001b[0m\n\u001b[34m[05/23/2020 21:49:30 INFO 140260686526272] current loss: 0.178127800685\u001b[0m\n\u001b[34m[05/23/2020 21:49:30 INFO 140260686526272] absolute loss difference: 0.048113969559\u001b[0m\n\u001b[34m[05/23/2020 21:49:30 INFO 140260686526272] Bad epoch: loss has not improved (enough). Bad count:4\u001b[0m\n\u001b[34m[05/23/2020 21:49:30 INFO 140260686526272] Bad epochs exceeded patience. Stopping training early!\u001b[0m\n\u001b[34m[05/23/2020 21:49:30 INFO 140260686526272] Early stopping criterion met! Stopping training at epoch: 6\u001b[0m\n\u001b[34m#metrics {\"Metrics\": {\"early_stop.time\": {\"count\": 1, \"max\": 0.35500526428222656, \"sum\": 0.35500526428222656, \"min\": 0.35500526428222656}, \"update.time\": {\"count\": 1, \"max\": 165298.23112487793, \"sum\": 165298.23112487793, \"min\": 165298.23112487793}}, \"EndTime\": 1590270570.126821, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"ObjectToVec\"}, \"StartTime\": 1590270404.578788}\n\u001b[0m\n\u001b[34m[05/23/2020 21:49:30 INFO 140260686526272] Early stop condition met. Stopping training.\u001b[0m\n\u001b[34m[05/23/2020 21:49:30 INFO 140260686526272] #progress_metric: host=algo-1, completed 100 % epochs\u001b[0m\n\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 3458, \"sum\": 3458.0, \"min\": 3458}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 3458, \"sum\": 3458.0, \"min\": 3458}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 885248, \"sum\": 885248.0, \"min\": 885248}, \"Total Batches Seen\": {\"count\": 1, \"max\": 24206, \"sum\": 24206.0, \"min\": 24206}, \"Total Records Seen\": {\"count\": 1, \"max\": 6196736, \"sum\": 6196736.0, \"min\": 6196736}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 885248, \"sum\": 885248.0, \"min\": 885248}, \"Reset Count\": {\"count\": 1, \"max\": 7, \"sum\": 7.0, \"min\": 7}}, \"EndTime\": 1590270570.127085, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"ObjectToVec\", \"epoch\": 6}, \"StartTime\": 1590270404.82857}\n\u001b[0m\n\u001b[34m[05/23/2020 21:49:30 INFO 140260686526272] #throughput_metric: host=algo-1, train throughput=5355.44671551 records/second\u001b[0m\n\u001b[34m[05/23/2020 21:49:30 WARNING 140260686526272] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n\u001b[34m[05/23/2020 21:49:30 INFO 140260686526272] Best model based on epoch 3. Best loss: 0.130\u001b[0m\n\u001b[34m#metrics {\"Metrics\": {\"finalize.time\": {\"count\": 1, \"max\": 11.667013168334961, \"sum\": 11.667013168334961, \"min\": 11.667013168334961}}, \"EndTime\": 1590270570.138993, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"ObjectToVec\"}, \"StartTime\": 1590270570.126889}\n\u001b[0m\n\u001b[34m[05/23/2020 21:49:30 INFO 140260686526272] Serializing model to /opt/ml/model/model_algo-1\u001b[0m\n\u001b[34m[05/23/2020 21:49:31 INFO 140260686526272] Saved checkpoint to \"/tmp/tmpR3S4CS/state-0001.params\"\u001b[0m\n\u001b[34m[05/23/2020 21:49:32 INFO 140260686526272] Test data is not provided.\u001b[0m\n\u001b[34m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 1686743.7620162964, \"sum\": 1686743.7620162964, \"min\": 1686743.7620162964}, \"model.serialize.time\": {\"count\": 1, \"max\": 2204.664945602417, \"sum\": 2204.664945602417, \"min\": 2204.664945602417}, \"setuptime\": {\"count\": 1, \"max\": 346.62318229675293, \"sum\": 346.62318229675293, \"min\": 346.62318229675293}}, \"EndTime\": 1590270572.682134, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"ObjectToVec\"}, \"StartTime\": 1590270570.139087}\n\u001b[0m\n\n2020-05-23 21:49:37 Uploading - Uploading generated training model\n2020-05-23 21:50:20 Completed - Training job completed\nTraining seconds: 1859\nBillable seconds: 1859\n"
    }
   ],
   "source": [
    "                                   \n",
    "o2v.set_hyperparameters(**hyperparameters)\n",
    "input_data = {\n",
    "    \"train\": s3_input(train_s3_path, content_type=\"application/jsonlines\"),\n",
    "    \"validation\": s3_input(validation_s3_path, content_type=\"application/jsonlines\"),\n",
    "    \"auxiliary\": s3_input(auxiliary_s3_path)\n",
    "}\n",
    "o2v.fit(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with Batch inference\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "Note that the batch inference will take approximately 30 minutes to complete on the ml.p2.8xlarge instance type\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "140050967226176] module data shapes:[DataDesc[source,(16L, 460L),<type 'numpy.float32'>,NTC], DataDesc[target,(16L, 10L),<type 'numpy.float32'>,NTC]]\u001b[0m\n\u001b[34m[05/23/2020 22:04:24 INFO 140050967226176] module label shapes:None\u001b[0m\n\u001b[34m[05/23/2020 22:04:24 INFO 140050967226176] data iter data shapes:[DataDesc[source,(16, 310L),<type 'numpy.float32'>,NTC], DataDesc[target,(16, 10L),<type 'numpy.float32'>,NTC]]\u001b[0m\n\u001b[34m[05/23/2020 22:04:24 INFO 140050967226176] data iter label shapes:None\u001b[0m\n\u001b[35m#metrics {\"Metrics\": {\"model.evaluate.time\": {\"count\": 1, \"max\": 3163.2018089294434, \"sum\": 3163.2018089294434, \"min\": 3163.2018089294434}, \"json.encoder.time\": {\"count\": 1, \"max\": 842.4201011657715, \"sum\": 842.4201011657715, \"min\": 842.4201011657715}, \"jsonlines_bucket_iterator.time\": {\"count\": 1, \"max\": 327.84485816955566, \"sum\": 327.84485816955566, \"min\": 327.84485816955566}, \"invocations.count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1590271463.695261, \"Dimensions\": {\"Host\": \"UNKNOWN\", \"Operation\": \"scoring\", \"Algorithm\": \"AlgorithmModel\"}, \"StartTime\": 1590271459.128459}\n\u001b[0m\n\u001b[35m[05/23/2020 22:04:24 INFO 140050967226176] Using modality NetworkType.FULL\u001b[0m\n\u001b[35m[05/23/2020 22:04:24 INFO 140050967226176] module data shapes:[DataDesc[source,(16L, 460L),<type 'numpy.float32'>,NTC], DataDesc[target,(16L, 10L),<type 'numpy.float32'>,NTC]]\u001b[0m\n\u001b[35m[05/23/2020 22:04:24 INFO 140050967226176] module label shapes:None\u001b[0m\n\u001b[35m[05/23/2020 22:04:24 INFO 140050967226176] data iter data shapes:[DataDesc[source,(16, 310L),<type 'numpy.float32'>,NTC], DataDesc[target,(16, 10L),<type 'numpy.float32'>,NTC]]\u001b[0m\n\u001b[35m[05/23/2020 22:04:24 INFO 140050967226176] data iter label shapes:None\u001b[0m\n\u001b[34m[05/23/2020 22:04:27 INFO 140050967226176] model_output shape:(15772L, 2L)\u001b[0m\n\u001b[35m[05/23/2020 22:04:27 INFO 140050967226176] model_output shape:(15772L, 2L)\u001b[0m\n\u001b[34m#metrics {\"Metrics\": {\"model.evaluate.time\": {\"count\": 1, \"max\": 3432.659864425659, \"sum\": 3432.659864425659, \"min\": 3432.659864425659}, \"json.encoder.time\": {\"count\": 1, \"max\": 22.04418182373047, \"sum\": 22.04418182373047, \"min\": 22.04418182373047}, \"jsonlines_bucket_iterator.time\": {\"count\": 1, \"max\": 328.2580375671387, \"sum\": 328.2580375671387, \"min\": 328.2580375671387}, \"invocations.count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1590271467.712286, \"Dimensions\": {\"Host\": \"UNKNOWN\", \"Operation\": \"scoring\", \"Algorithm\": \"AlgorithmModel\"}, \"StartTime\": 1590271463.69538}\n\u001b[0m\n\u001b[35m#metrics {\"Metrics\": {\"model.evaluate.time\": {\"count\": 1, \"max\": 3432.659864425659, \"sum\": 3432.659864425659, \"min\": 3432.659864425659}, \"json.encoder.time\": {\"count\": 1, \"max\": 22.04418182373047, \"sum\": 22.04418182373047, \"min\": 22.04418182373047}, \"jsonlines_bucket_iterator.time\": {\"count\": 1, \"max\": 328.2580375671387, \"sum\": 328.2580375671387, \"min\": 328.2580375671387}, \"invocations.count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1590271467.712286, \"Dimensions\": {\"Host\": \"UNKNOWN\", \"Operation\": \"scoring\", \"Algorithm\": \"AlgorithmModel\"}, \"StartTime\": 1590271463.69538}\n\u001b[0m\n\u001b[34m[05/23/2020 22:04:28 INFO 140050967226176] Using modality NetworkType.FULL\u001b[0m\n\u001b[34m[05/23/2020 22:04:28 INFO 140050967226176] module data shapes:[DataDesc[source,(16L, 310L),<type 'numpy.float32'>,NTC], DataDesc[target,(16L, 10L),<type 'numpy.float32'>,NTC]]\u001b[0m\n\u001b[34m[05/23/2020 22:04:28 INFO 140050967226176] module label shapes:None\u001b[0m\n\u001b[34m[05/23/2020 22:04:28 INFO 140050967226176] data iter data shapes:[DataDesc[source,(16, 940L),<type 'numpy.float32'>,NTC], DataDesc[target,(16, 10L),<type 'numpy.float32'>,NTC]]\u001b[0m\n\u001b[34m[05/23/2020 22:04:28 INFO 140050967226176] data iter label shapes:None\u001b[0m\n\u001b[35m[05/23/2020 22:04:28 INFO 140050967226176] Using modality NetworkType.FULL\u001b[0m\n\u001b[35m[05/23/2020 22:04:28 INFO 140050967226176] module data shapes:[DataDesc[source,(16L, 310L),<type 'numpy.float32'>,NTC], DataDesc[target,(16L, 10L),<type 'numpy.float32'>,NTC]]\u001b[0m\n\u001b[35m[05/23/2020 22:04:28 INFO 140050967226176] module label shapes:None\u001b[0m\n\u001b[35m[05/23/2020 22:04:28 INFO 140050967226176] data iter data shapes:[DataDesc[source,(16, 940L),<type 'numpy.float32'>,NTC], DataDesc[target,(16, 10L),<type 'numpy.float32'>,NTC]]\u001b[0m\n\u001b[35m[05/23/2020 22:04:28 INFO 140050967226176] data iter label shapes:None\u001b[0m\n\u001b[34m[05/23/2020 22:04:31 INFO 140050967226176] model_output shape:(15677L, 2L)\u001b[0m\n\u001b[35m[05/23/2020 22:04:31 INFO 140050967226176] model_output shape:(15677L, 2L)\u001b[0m\n\u001b[34m[05/23/2020 22:04:35 INFO 140050967226176] Using modality NetworkType.FULL\u001b[0m\n\u001b[34m[05/23/2020 22:04:35 INFO 140050967226176] module data shapes:[DataDesc[source,(16L, 940L),<type 'numpy.float32'>,NTC], DataDesc[target,(16L, 10L),<type 'numpy.float32'>,NTC]]\u001b[0m\n\u001b[34m[05/23/2020 22:04:35 INFO 140050967226176] module label shapes:None\u001b[0m\n\u001b[34m[05/23/2020 22:04:35 INFO 140050967226176] data iter data shapes:[DataDesc[source,(16, 390L),<type 'numpy.float32'>,NTC], DataDesc[target,(16, 10L),<type 'numpy.float32'>,NTC]]\u001b[0m\n\u001b[34m[05/23/2020 22:04:35 INFO 140050967226176] data iter label shapes:None\u001b[0m\n\u001b[35m[05/23/2020 22:04:35 INFO 140050967226176] Using modality NetworkType.FULL\u001b[0m\n\u001b[35m[05/23/2020 22:04:35 INFO 140050967226176] module data shapes:[DataDesc[source,(16L, 940L),<type 'numpy.float32'>,NTC], DataDesc[target,(16L, 10L),<type 'numpy.float32'>,NTC]]\u001b[0m\n\u001b[35m[05/23/2020 22:04:35 INFO 140050967226176] module label shapes:None\u001b[0m\n\u001b[35m[05/23/2020 22:04:35 INFO 140050967226176] data iter data shapes:[DataDesc[source,(16, 390L),<type 'numpy.float32'>,NTC], DataDesc[target,(16, 10L),<type 'numpy.float32'>,NTC]]\u001b[0m\n\u001b[35m[05/23/2020 22:04:35 INFO 140050967226176] data iter label shapes:None\u001b[0m\n\u001b[34m[05/23/2020 22:04:38 INFO 140050967226176] model_output shape:(15392L, 2L)\u001b[0m\n\u001b[35m[05/23/2020 22:04:38 INFO 140050967226176] model_output shape:(15392L, 2L)\u001b[0m\n\u001b[34m#metrics {\"Metrics\": {\"model.evaluate.time\": {\"count\": 1, \"max\": 3162.440061569214, \"sum\": 3162.440061569214, \"min\": 3162.440061569214}, \"json.encoder.time\": {\"count\": 1, \"max\": 382.659912109375, \"sum\": 382.659912109375, \"min\": 382.659912109375}, \"jsonlines_bucket_iterator.time\": {\"count\": 1, \"max\": 325.28209686279297, \"sum\": 325.28209686279297, \"min\": 325.28209686279297}, \"invocations.count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1590271479.109882, \"Dimensions\": {\"Host\": \"UNKNOWN\", \"Operation\": \"scoring\", \"Algorithm\": \"AlgorithmModel\"}, \"StartTime\": 1590271475.003314}\n\u001b[0m\n\u001b[35m#metrics {\"Metrics\": {\"model.evaluate.time\": {\"count\": 1, \"max\": 3162.440061569214, \"sum\": 3162.440061569214, \"min\": 3162.440061569214}, \"json.encoder.time\": {\"count\": 1, \"max\": 382.659912109375, \"sum\": 382.659912109375, \"min\": 382.659912109375}, \"jsonlines_bucket_iterator.time\": {\"count\": 1, \"max\": 325.28209686279297, \"sum\": 325.28209686279297, \"min\": 325.28209686279297}, \"invocations.count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1590271479.109882, \"Dimensions\": {\"Host\": \"UNKNOWN\", \"Operation\": \"scoring\", \"Algorithm\": \"AlgorithmModel\"}, \"StartTime\": 1590271475.003314}\n\u001b[0m\n\u001b[34m[05/23/2020 22:04:39 INFO 140050967226176] Using modality NetworkType.FULL\u001b[0m\n\u001b[34m[05/23/2020 22:04:39 INFO 140050967226176] module data shapes:[DataDesc[source,(16L, 390L),<type 'numpy.float32'>,NTC], DataDesc[target,(16L, 10L),<type 'numpy.float32'>,NTC]]\u001b[0m\n\u001b[34m[05/23/2020 22:04:39 INFO 140050967226176] module label shapes:None\u001b[0m\n\u001b[34m[05/23/2020 22:04:39 INFO 140050967226176] data iter data shapes:[DataDesc[source,(16, 620L),<type 'numpy.float32'>,NTC], DataDesc[target,(16, 10L),<type 'numpy.float32'>,NTC]]\u001b[0m\n\u001b[34m[05/23/2020 22:04:39 INFO 140050967226176] data iter label shapes:None\u001b[0m\n\u001b[35m[05/23/2020 22:04:39 INFO 140050967226176] Using modality NetworkType.FULL\u001b[0m\n\u001b[35m[05/23/2020 22:04:39 INFO 140050967226176] module data shapes:[DataDesc[source,(16L, 390L),<type 'numpy.float32'>,NTC], DataDesc[target,(16L, 10L),<type 'numpy.float32'>,NTC]]\u001b[0m\n\u001b[35m[05/23/2020 22:04:39 INFO 140050967226176] module label shapes:None\u001b[0m\n\u001b[35m[05/23/2020 22:04:39 INFO 140050967226176] data iter data shapes:[DataDesc[source,(16, 620L),<type 'numpy.float32'>,NTC], DataDesc[target,(16, 10L),<type 'numpy.float32'>,NTC]]\u001b[0m\n\u001b[35m[05/23/2020 22:04:39 INFO 140050967226176] data iter label shapes:None\u001b[0m\n\u001b[34m[05/23/2020 22:04:42 INFO 140050967226176] model_output shape:(14368L, 2L)\u001b[0m\n\u001b[35m[05/23/2020 22:04:42 INFO 140050967226176] model_output shape:(14368L, 2L)\u001b[0m\n\u001b[34m#metrics {\"Metrics\": {\"model.evaluate.time\": {\"count\": 1, \"max\": 2990.1859760284424, \"sum\": 2990.1859760284424, \"min\": 2990.1859760284424}, \"json.encoder.time\": {\"count\": 1, \"max\": 1329.822063446045, \"sum\": 1329.822063446045, \"min\": 1329.822063446045}, \"jsonlines_bucket_iterator.time\": {\"count\": 1, \"max\": 334.84506607055664, \"sum\": 334.84506607055664, \"min\": 334.84506607055664}, \"invocations.count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1590271483.994261, \"Dimensions\": {\"Host\": \"UNKNOWN\", \"Operation\": \"scoring\", \"Algorithm\": \"AlgorithmModel\"}, \"StartTime\": 1590271479.110126}\n\u001b[0m\n\u001b[35m#metrics {\"Metrics\": {\"model.evaluate.time\": {\"count\": 1, \"max\": 2990.1859760284424, \"sum\": 2990.1859760284424, \"min\": 2990.1859760284424}, \"json.encoder.time\": {\"count\": 1, \"max\": 1329.822063446045, \"sum\": 1329.822063446045, \"min\": 1329.822063446045}, \"jsonlines_bucket_iterator.time\": {\"count\": 1, \"max\": 334.84506607055664, \"sum\": 334.84506607055664, \"min\": 334.84506607055664}, \"invocations.count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1590271483.994261, \"Dimensions\": {\"Host\": \"UNKNOWN\", \"Operation\": \"scoring\", \"Algorithm\": \"AlgorithmModel\"}, \"StartTime\": 1590271479.110126}\n\u001b[0m\n\u001b[34m[05/23/2020 22:04:44 INFO 140050967226176] module data shapes:[DataDesc[source,(16L, 620L),<type 'numpy.float32'>,NTC], DataDesc[target,(16L, 10L),<type 'numpy.float32'>,NTC]]\u001b[0m\n\u001b[34m[05/23/2020 22:04:44 INFO 140050967226176] module label shapes:None\u001b[0m\n\u001b[34m[05/23/2020 22:04:44 INFO 140050967226176] data iter data shapes:[DataDesc[source,(16, 410L),<type 'numpy.float32'>,NTC], DataDesc[target,(16, 10L),<type 'numpy.float32'>,NTC]]\u001b[0m\n\u001b[34m[05/23/2020 22:04:44 INFO 140050967226176] data iter label shapes:None\u001b[0m\n\u001b[35m[05/23/2020 22:04:44 INFO 140050967226176] module data shapes:[DataDesc[source,(16L, 620L),<type 'numpy.float32'>,NTC], DataDesc[target,(16L, 10L),<type 'numpy.float32'>,NTC]]\u001b[0m\n\u001b[35m[05/23/2020 22:04:44 INFO 140050967226176] module label shapes:None\u001b[0m\n\u001b[35m[05/23/2020 22:04:44 INFO 140050967226176] data iter data shapes:[DataDesc[source,(16, 410L),<type 'numpy.float32'>,NTC], DataDesc[target,(16, 10L),<type 'numpy.float32'>,NTC]]\u001b[0m\n\u001b[35m[05/23/2020 22:04:44 INFO 140050967226176] data iter label shapes:None\u001b[0m\n\u001b[34m[05/23/2020 22:04:47 INFO 140050967226176] model_output shape:(14774L, 2L)\u001b[0m\n\u001b[35m[05/23/2020 22:04:47 INFO 140050967226176] model_output shape:(14774L, 2L)\u001b[0m\n\u001b[34m#metrics {\"Metrics\": {\"model.evaluate.time\": {\"count\": 1, \"max\": 3096.7328548431396, \"sum\": 3096.7328548431396, \"min\": 3096.7328548431396}, \"json.encoder.time\": {\"count\": 1, \"max\": 261.2290382385254, \"sum\": 261.2290382385254, \"min\": 261.2290382385254}, \"jsonlines_bucket_iterator.time\": {\"count\": 1, \"max\": 328.46689224243164, \"sum\": 328.46689224243164, \"min\": 328.46689224243164}, \"invocations.count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1590271487.843424, \"Dimensions\": {\"Host\": \"UNKNOWN\", \"Operation\": \"scoring\", \"Algorithm\": \"AlgorithmModel\"}, \"StartTime\": 1590271483.995076}\n\u001b[0m\n\u001b[35m#metrics {\"Metrics\": {\"model.evaluate.time\": {\"count\": 1, \"max\": 3096.7328548431396, \"sum\": 3096.7328548431396, \"min\": 3096.7328548431396}, \"json.encoder.time\": {\"count\": 1, \"max\": 261.2290382385254, \"sum\": 261.2290382385254, \"min\": 261.2290382385254}, \"jsonlines_bucket_iterator.time\": {\"count\": 1, \"max\": 328.46689224243164, \"sum\": 328.46689224243164, \"min\": 328.46689224243164}, \"invocations.count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1590271487.843424, \"Dimensions\": {\"Host\": \"UNKNOWN\", \"Operation\": \"scoring\", \"Algorithm\": \"AlgorithmModel\"}, \"StartTime\": 1590271483.995076}\n\u001b[0m\n\u001b[34m[05/23/2020 22:04:48 INFO 140050967226176] Using modality NetworkType.FULL\u001b[0m\n\u001b[34m[05/23/2020 22:04:48 INFO 140050967226176] module data shapes:[DataDesc[source,(16L, 410L),<type 'numpy.float32'>,NTC], DataDesc[target,(16L, 10L),<type 'numpy.float32'>,NTC]]\u001b[0m\n\u001b[34m[05/23/2020 22:04:48 INFO 140050967226176] module label shapes:None\u001b[0m\n\u001b[34m[05/23/2020 22:04:48 INFO 140050967226176] data iter data shapes:[DataDesc[source,(16, 470L),<type 'numpy.float32'>,NTC], DataDesc[target,(16, 10L),<type 'numpy.float32'>,NTC]]\u001b[0m\n\u001b[34m[05/23/2020 22:04:48 INFO 140050967226176] data iter label shapes:None\u001b[0m\n\u001b[35m[05/23/2020 22:04:48 INFO 140050967226176] Using modality NetworkType.FULL\u001b[0m\n\u001b[35m[05/23/2020 22:04:48 INFO 140050967226176] module data shapes:[DataDesc[source,(16L, 410L),<type 'numpy.float32'>,NTC], DataDesc[target,(16L, 10L),<type 'numpy.float32'>,NTC]]\u001b[0m\n\u001b[35m[05/23/2020 22:04:48 INFO 140050967226176] module label shapes:None\u001b[0m\n\u001b[35m[05/23/2020 22:04:48 INFO 140050967226176] data iter data shapes:[DataDesc[source,(16, 470L),<type 'numpy.float32'>,NTC], DataDesc[target,(16, 10L),<type 'numpy.float32'>,NTC]]\u001b[0m\n\u001b[35m[05/23/2020 22:04:48 INFO 140050967226176] data iter label shapes:None\u001b[0m\n\u001b[34m[05/23/2020 22:04:51 INFO 140050967226176] model_output shape:(15015L, 2L)\u001b[0m\n\u001b[35m[05/23/2020 22:04:51 INFO 140050967226176] model_output shape:(15015L, 2L)\u001b[0m\n\u001b[34m#metrics {\"Metrics\": {\"model.evaluate.time\": {\"count\": 1, \"max\": 3089.629888534546, \"sum\": 3089.629888534546, \"min\": 3089.629888534546}, \"json.encoder.time\": {\"count\": 1, \"max\": 707.0279121398926, \"sum\": 707.0279121398926, \"min\": 707.0279121398926}, \"jsonlines_bucket_iterator.time\": {\"count\": 1, \"max\": 375.1859664916992, \"sum\": 375.1859664916992, \"min\": 375.1859664916992}, \"invocations.count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1590271492.178113, \"Dimensions\": {\"Host\": \"UNKNOWN\", \"Operation\": \"scoring\", \"Algorithm\": \"AlgorithmModel\"}, \"StartTime\": 1590271487.843947}\n\u001b[0m\n\u001b[35m#metrics {\"Metrics\": {\"model.evaluate.time\": {\"count\": 1, \"max\": 3089.629888534546, \"sum\": 3089.629888534546, \"min\": 3089.629888534546}, \"json.encoder.time\": {\"count\": 1, \"max\": 707.0279121398926, \"sum\": 707.0279121398926, \"min\": 707.0279121398926}, \"jsonlines_bucket_iterator.time\": {\"count\": 1, \"max\": 375.1859664916992, \"sum\": 375.1859664916992, \"min\": 375.1859664916992}, \"invocations.count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1590271492.178113, \"Dimensions\": {\"Host\": \"UNKNOWN\", \"Operation\": \"scoring\", \"Algorithm\": \"AlgorithmModel\"}, \"StartTime\": 1590271487.843947}\n\u001b[0m\n\u001b[34m[05/23/2020 22:04:53 INFO 140050967226176] Using modality NetworkType.FULL\u001b[0m\n\u001b[34m[05/23/2020 22:04:53 INFO 140050967226176] module data shapes:[DataDesc[source,(16L, 470L),<type 'numpy.float32'>,NTC], DataDesc[target,(16L, 10L),<type 'numpy.float32'>,NTC]]\u001b[0m\n\u001b[34m[05/23/2020 22:04:53 INFO 140050967226176] module label shapes:None\u001b[0m\n\u001b[34m[05/23/2020 22:04:53 INFO 140050967226176] data iter data shapes:[DataDesc[source,(16, 390L),<type 'numpy.float32'>,NTC], DataDesc[target,(16, 10L),<type 'numpy.float32'>,NTC]]\u001b[0m\n\u001b[34m[05/23/2020 22:04:53 INFO 140050967226176] data iter label shapes:None\u001b[0m\n\u001b[35m[05/23/2020 22:04:53 INFO 140050967226176] Using modality NetworkType.FULL\u001b[0m\n\u001b[35m[05/23/2020 22:04:53 INFO 140050967226176] module data shapes:[DataDesc[source,(16L, 470L),<type 'numpy.float32'>,NTC], DataDesc[target,(16L, 10L),<type 'numpy.float32'>,NTC]]\u001b[0m\n\u001b[35m[05/23/2020 22:04:53 INFO 140050967226176] module label shapes:None\u001b[0m\n\u001b[35m[05/23/2020 22:04:53 INFO 140050967226176] data iter data shapes:[DataDesc[source,(16, 390L),<type 'numpy.float32'>,NTC], DataDesc[target,(16, 10L),<type 'numpy.float32'>,NTC]]\u001b[0m\n\u001b[35m[05/23/2020 22:04:53 INFO 140050967226176] data iter label shapes:None\u001b[0m\n\u001b[34m[05/23/2020 22:04:55 INFO 140050967226176] model_output shape:(15447L, 2L)\u001b[0m\n\u001b[34m#metrics {\"Metrics\": {\"model.evaluate.time\": {\"count\": 1, \"max\": 3149.0249633789062, \"sum\": 3149.0249633789062, \"min\": 3149.0249633789062}, \"json.encoder.time\": {\"count\": 1, \"max\": 328.83405685424805, \"sum\": 328.83405685424805, \"min\": 328.83405685424805}, \"jsonlines_bucket_iterator.time\": {\"count\": 1, \"max\": 380.24187088012695, \"sum\": 380.24187088012695, \"min\": 380.24187088012695}, \"invocations.count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1590271496.203156, \"Dimensions\": {\"Host\": \"UNKNOWN\", \"Operation\": \"scoring\", \"Algorithm\": \"AlgorithmModel\"}, \"StartTime\": 1590271492.178277}\n\u001b[0m\n\u001b[35m[05/23/2020 22:04:55 INFO 140050967226176] model_output shape:(15447L, 2L)\u001b[0m\n\u001b[35m#metrics {\"Metrics\": {\"model.evaluate.time\": {\"count\": 1, \"max\": 3149.0249633789062, \"sum\": 3149.0249633789062, \"min\": 3149.0249633789062}, \"json.encoder.time\": {\"count\": 1, \"max\": 328.83405685424805, \"sum\": 328.83405685424805, \"min\": 328.83405685424805}, \"jsonlines_bucket_iterator.time\": {\"count\": 1, \"max\": 380.24187088012695, \"sum\": 380.24187088012695, \"min\": 380.24187088012695}, \"invocations.count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1590271496.203156, \"Dimensions\": {\"Host\": \"UNKNOWN\", \"Operation\": \"scoring\", \"Algorithm\": \"AlgorithmModel\"}, \"StartTime\": 1590271492.178277}\n\u001b[0m\n\u001b[34m[05/23/2020 22:04:56 INFO 140050967226176] Using modality NetworkType.FULL\u001b[0m\n\u001b[34m[05/23/2020 22:04:56 INFO 140050967226176] module data shapes:[DataDesc[source,(16L, 390L),<type 'numpy.float32'>,NTC], DataDesc[target,(16L, 10L),<type 'numpy.float32'>,NTC]]\u001b[0m\n\u001b[34m[05/23/2020 22:04:56 INFO 140050967226176] module label shapes:None\u001b[0m\n\u001b[34m[05/23/2020 22:04:56 INFO 140050967226176] data iter data shapes:[DataDesc[source,(16, 410L),<type 'numpy.float32'>,NTC], DataDesc[target,(16, 10L),<type 'numpy.float32'>,NTC]]\u001b[0m\n\u001b[34m[05/23/2020 22:04:56 INFO 140050967226176] data iter label shapes:None\u001b[0m\n\u001b[35m[05/23/2020 22:04:56 INFO 140050967226176] Using modality NetworkType.FULL\u001b[0m\n\u001b[35m[05/23/2020 22:04:56 INFO 140050967226176] module data shapes:[DataDesc[source,(16L, 390L),<type 'numpy.float32'>,NTC], DataDesc[target,(16L, 10L),<type 'numpy.float32'>,NTC]]\u001b[0m\n\u001b[35m[05/23/2020 22:04:56 INFO 140050967226176] module label shapes:None\u001b[0m\n\u001b[35m[05/23/2020 22:04:56 INFO 140050967226176] data iter data shapes:[DataDesc[source,(16, 410L),<type 'numpy.float32'>,NTC], DataDesc[target,(16, 10L),<type 'numpy.float32'>,NTC]]\u001b[0m\n\u001b[35m[05/23/2020 22:04:56 INFO 140050967226176] data iter label shapes:None\u001b[0m\n\u001b[34m[05/23/2020 22:04:58 INFO 140050967226176] model_output shape:(9965L, 2L)\u001b[0m\n\u001b[35m[05/23/2020 22:04:58 INFO 140050967226176] model_output shape:(9965L, 2L)\u001b[0m\n\u001b[34m#metrics {\"Metrics\": {\"model.evaluate.time\": {\"count\": 1, \"max\": 2021.1410522460938, \"sum\": 2021.1410522460938, \"min\": 2021.1410522460938}, \"json.encoder.time\": {\"count\": 1, \"max\": 216.57800674438477, \"sum\": 216.57800674438477, \"min\": 216.57800674438477}, \"jsonlines_bucket_iterator.time\": {\"count\": 1, \"max\": 259.3350410461426, \"sum\": 259.3350410461426, \"min\": 259.3350410461426}, \"invocations.count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1590271498.809101, \"Dimensions\": {\"Host\": \"UNKNOWN\", \"Operation\": \"scoring\", \"Algorithm\": \"AlgorithmModel\"}, \"StartTime\": 1590271496.203375}\n\u001b[0m\n\u001b[35m#metrics {\"Metrics\": {\"model.evaluate.time\": {\"count\": 1, \"max\": 2021.1410522460938, \"sum\": 2021.1410522460938, \"min\": 2021.1410522460938}, \"json.encoder.time\": {\"count\": 1, \"max\": 216.57800674438477, \"sum\": 216.57800674438477, \"min\": 216.57800674438477}, \"jsonlines_bucket_iterator.time\": {\"count\": 1, \"max\": 259.3350410461426, \"sum\": 259.3350410461426, \"min\": 259.3350410461426}, \"invocations.count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1590271498.809101, \"Dimensions\": {\"Host\": \"UNKNOWN\", \"Operation\": \"scoring\", \"Algorithm\": \"AlgorithmModel\"}, \"StartTime\": 1590271496.203375}\n\u001b[0m\n\n"
    }
   ],
   "source": [
    "transformer = o2v.transformer(instance_count=1, \n",
    "                              instance_type=\"ml.p3.8xlarge\", \n",
    "                              output_path=prediction_s3_path)\n",
    "transformer.transform(data=test_s3_path, content_type=\"application/jsonlines\", split_type=\"Line\")\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the predictions from s3 to perform the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "download: s3://md-backup-bucket-01/object2vec-movie-genre-prediction/predictions/tokenized_movie_genres_test.jsonl.out to ../../pyspark_mnist/tokenized_movie_genres_test.jsonl.out\n"
    }
   ],
   "source": [
    "!aws s3 cp --recursive {prediction_s3_path} /home/ec2-user/SageMaker/AWS-ML-Certification/__my_study/pyspark_mnist/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(filename, predictions, genre_dict, threshold=0.5):\n",
    "    metrics = {g:{\"genre\": g, \"tp\":0, \"tn\":0, \"fp\":0, \"fn\":0} for g in genre_dict.values()}\n",
    "    with jsonlines.open(filename, \"r\") as reader, jsonlines.open(predictions, \"r\") as preds:\n",
    "        for row, preds in zip(reader, preds):\n",
    "            prediction = preds[\"scores\"][1] > threshold\n",
    "            label = row[\"label\"]\n",
    "            g = genre_dict[row[\"in1\"][0]]\n",
    "            if prediction == 1:\n",
    "                if label == prediction:\n",
    "                    metrics[g][\"tp\"] +=1\n",
    "                else:\n",
    "                    metrics[g][\"fp\"]+=1\n",
    "            elif prediction == 0:\n",
    "                if label == prediction:\n",
    "                    metrics[g][\"tn\"]+=1\n",
    "                else:\n",
    "                    metrics[g][\"fn\"]+=1\n",
    "    summary = pd.DataFrame(list(metrics.values())).set_index('genre')\n",
    "    summary['accuracy'] = summary.apply (lambda row: (row.tp + row.tn) / (row.tp + row.tn + row.fp + row.fn),axis=1)\n",
    "    summary['precision'] = summary.apply (lambda row: row.tp / (row.tp + row.fp),axis=1)\n",
    "    summary['recall'] = summary.apply (lambda row: row.tp / (row.tp + row.fn),axis=1)\n",
    "    summary['f1'] = summary.apply (lambda row: 2*(row.precision * row.recall) /(row.precision + row.recall),axis=1)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "               fn    fp     tn     tp  accuracy  precision    recall        f1\ngenre                                                                         \nAction       1698  1006  37787   2795  0.937532   0.735333  0.622079  0.673981\nAdult          26     2  43242     16  0.999353   0.888889  0.380952  0.533333\nAdventure    1734   996  38543   2013  0.936931   0.668993  0.537230  0.595915\nAnimation     823  1116  37906   3441  0.955205   0.755102  0.806989  0.780184\nBiography     498    29  42734     25  0.987825   0.462963  0.047801  0.086655\nComedy       1635  5801  25192  10658  0.828212   0.647548  0.866997  0.741375\nCrime        1249  1697  36085   4255  0.931941   0.714886  0.773074  0.742842\nDocumentary  1249  1934  36787   3316  0.926466   0.631619  0.726396  0.675700\nDrama        1944  4287  22200  14855  0.856050   0.776042  0.884279  0.826633\nFamily       1620  2028  35631   4007  0.915723   0.663960  0.712102  0.687189\nFantasy      1100  1050  39374   1762  0.950330   0.626600  0.615653  0.621079\nGame-Show     272   175  42334    505  0.989673   0.742647  0.649936  0.693205\nHistory       684   198  42065    339  0.979624   0.631285  0.331378  0.434615\nHorror        526   565  41786    409  0.974796   0.419918  0.437433  0.428497\nMusic         461   296  41915    614  0.982512   0.674725  0.571163  0.618640\nMusical       234     3  43027     22  0.994525   0.880000  0.085938  0.156584\nMystery      1389  2033  36999   2865  0.920944   0.584933  0.673484  0.626093\nNews          329   663  41081   1213  0.977083   0.646588  0.786641  0.709772\nReality-TV   1210  1289  37462   3325  0.942268   0.720633  0.733186  0.726855\nReality-tv      1     0  43285      0  0.999977        NaN  0.000000       NaN\nRomance      1452  1683  34590   5561  0.927575   0.767670  0.792956  0.780108\nSci-Fi        955  1067  39112   2152  0.953287   0.668531  0.692630  0.680367\nShort         260     1  43014     11  0.993970   0.916667  0.040590  0.077739\nSport         397   159  42323    407  0.987155   0.719081  0.506219  0.594161\nTalk-Show     467   800  40360   1659  0.970730   0.674664  0.780339  0.723664\nThriller     1466  1408  38819   1593  0.933604   0.530823  0.520758  0.525743\nWar           335    52  42728    171  0.991059   0.766816  0.337945  0.469136\nWestern       240    87  42252    707  0.992446   0.890428  0.746568  0.812177",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fn</th>\n      <th>fp</th>\n      <th>tn</th>\n      <th>tp</th>\n      <th>accuracy</th>\n      <th>precision</th>\n      <th>recall</th>\n      <th>f1</th>\n    </tr>\n    <tr>\n      <th>genre</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Action</th>\n      <td>1698</td>\n      <td>1006</td>\n      <td>37787</td>\n      <td>2795</td>\n      <td>0.937532</td>\n      <td>0.735333</td>\n      <td>0.622079</td>\n      <td>0.673981</td>\n    </tr>\n    <tr>\n      <th>Adult</th>\n      <td>26</td>\n      <td>2</td>\n      <td>43242</td>\n      <td>16</td>\n      <td>0.999353</td>\n      <td>0.888889</td>\n      <td>0.380952</td>\n      <td>0.533333</td>\n    </tr>\n    <tr>\n      <th>Adventure</th>\n      <td>1734</td>\n      <td>996</td>\n      <td>38543</td>\n      <td>2013</td>\n      <td>0.936931</td>\n      <td>0.668993</td>\n      <td>0.537230</td>\n      <td>0.595915</td>\n    </tr>\n    <tr>\n      <th>Animation</th>\n      <td>823</td>\n      <td>1116</td>\n      <td>37906</td>\n      <td>3441</td>\n      <td>0.955205</td>\n      <td>0.755102</td>\n      <td>0.806989</td>\n      <td>0.780184</td>\n    </tr>\n    <tr>\n      <th>Biography</th>\n      <td>498</td>\n      <td>29</td>\n      <td>42734</td>\n      <td>25</td>\n      <td>0.987825</td>\n      <td>0.462963</td>\n      <td>0.047801</td>\n      <td>0.086655</td>\n    </tr>\n    <tr>\n      <th>Comedy</th>\n      <td>1635</td>\n      <td>5801</td>\n      <td>25192</td>\n      <td>10658</td>\n      <td>0.828212</td>\n      <td>0.647548</td>\n      <td>0.866997</td>\n      <td>0.741375</td>\n    </tr>\n    <tr>\n      <th>Crime</th>\n      <td>1249</td>\n      <td>1697</td>\n      <td>36085</td>\n      <td>4255</td>\n      <td>0.931941</td>\n      <td>0.714886</td>\n      <td>0.773074</td>\n      <td>0.742842</td>\n    </tr>\n    <tr>\n      <th>Documentary</th>\n      <td>1249</td>\n      <td>1934</td>\n      <td>36787</td>\n      <td>3316</td>\n      <td>0.926466</td>\n      <td>0.631619</td>\n      <td>0.726396</td>\n      <td>0.675700</td>\n    </tr>\n    <tr>\n      <th>Drama</th>\n      <td>1944</td>\n      <td>4287</td>\n      <td>22200</td>\n      <td>14855</td>\n      <td>0.856050</td>\n      <td>0.776042</td>\n      <td>0.884279</td>\n      <td>0.826633</td>\n    </tr>\n    <tr>\n      <th>Family</th>\n      <td>1620</td>\n      <td>2028</td>\n      <td>35631</td>\n      <td>4007</td>\n      <td>0.915723</td>\n      <td>0.663960</td>\n      <td>0.712102</td>\n      <td>0.687189</td>\n    </tr>\n    <tr>\n      <th>Fantasy</th>\n      <td>1100</td>\n      <td>1050</td>\n      <td>39374</td>\n      <td>1762</td>\n      <td>0.950330</td>\n      <td>0.626600</td>\n      <td>0.615653</td>\n      <td>0.621079</td>\n    </tr>\n    <tr>\n      <th>Game-Show</th>\n      <td>272</td>\n      <td>175</td>\n      <td>42334</td>\n      <td>505</td>\n      <td>0.989673</td>\n      <td>0.742647</td>\n      <td>0.649936</td>\n      <td>0.693205</td>\n    </tr>\n    <tr>\n      <th>History</th>\n      <td>684</td>\n      <td>198</td>\n      <td>42065</td>\n      <td>339</td>\n      <td>0.979624</td>\n      <td>0.631285</td>\n      <td>0.331378</td>\n      <td>0.434615</td>\n    </tr>\n    <tr>\n      <th>Horror</th>\n      <td>526</td>\n      <td>565</td>\n      <td>41786</td>\n      <td>409</td>\n      <td>0.974796</td>\n      <td>0.419918</td>\n      <td>0.437433</td>\n      <td>0.428497</td>\n    </tr>\n    <tr>\n      <th>Music</th>\n      <td>461</td>\n      <td>296</td>\n      <td>41915</td>\n      <td>614</td>\n      <td>0.982512</td>\n      <td>0.674725</td>\n      <td>0.571163</td>\n      <td>0.618640</td>\n    </tr>\n    <tr>\n      <th>Musical</th>\n      <td>234</td>\n      <td>3</td>\n      <td>43027</td>\n      <td>22</td>\n      <td>0.994525</td>\n      <td>0.880000</td>\n      <td>0.085938</td>\n      <td>0.156584</td>\n    </tr>\n    <tr>\n      <th>Mystery</th>\n      <td>1389</td>\n      <td>2033</td>\n      <td>36999</td>\n      <td>2865</td>\n      <td>0.920944</td>\n      <td>0.584933</td>\n      <td>0.673484</td>\n      <td>0.626093</td>\n    </tr>\n    <tr>\n      <th>News</th>\n      <td>329</td>\n      <td>663</td>\n      <td>41081</td>\n      <td>1213</td>\n      <td>0.977083</td>\n      <td>0.646588</td>\n      <td>0.786641</td>\n      <td>0.709772</td>\n    </tr>\n    <tr>\n      <th>Reality-TV</th>\n      <td>1210</td>\n      <td>1289</td>\n      <td>37462</td>\n      <td>3325</td>\n      <td>0.942268</td>\n      <td>0.720633</td>\n      <td>0.733186</td>\n      <td>0.726855</td>\n    </tr>\n    <tr>\n      <th>Reality-tv</th>\n      <td>1</td>\n      <td>0</td>\n      <td>43285</td>\n      <td>0</td>\n      <td>0.999977</td>\n      <td>NaN</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>Romance</th>\n      <td>1452</td>\n      <td>1683</td>\n      <td>34590</td>\n      <td>5561</td>\n      <td>0.927575</td>\n      <td>0.767670</td>\n      <td>0.792956</td>\n      <td>0.780108</td>\n    </tr>\n    <tr>\n      <th>Sci-Fi</th>\n      <td>955</td>\n      <td>1067</td>\n      <td>39112</td>\n      <td>2152</td>\n      <td>0.953287</td>\n      <td>0.668531</td>\n      <td>0.692630</td>\n      <td>0.680367</td>\n    </tr>\n    <tr>\n      <th>Short</th>\n      <td>260</td>\n      <td>1</td>\n      <td>43014</td>\n      <td>11</td>\n      <td>0.993970</td>\n      <td>0.916667</td>\n      <td>0.040590</td>\n      <td>0.077739</td>\n    </tr>\n    <tr>\n      <th>Sport</th>\n      <td>397</td>\n      <td>159</td>\n      <td>42323</td>\n      <td>407</td>\n      <td>0.987155</td>\n      <td>0.719081</td>\n      <td>0.506219</td>\n      <td>0.594161</td>\n    </tr>\n    <tr>\n      <th>Talk-Show</th>\n      <td>467</td>\n      <td>800</td>\n      <td>40360</td>\n      <td>1659</td>\n      <td>0.970730</td>\n      <td>0.674664</td>\n      <td>0.780339</td>\n      <td>0.723664</td>\n    </tr>\n    <tr>\n      <th>Thriller</th>\n      <td>1466</td>\n      <td>1408</td>\n      <td>38819</td>\n      <td>1593</td>\n      <td>0.933604</td>\n      <td>0.530823</td>\n      <td>0.520758</td>\n      <td>0.525743</td>\n    </tr>\n    <tr>\n      <th>War</th>\n      <td>335</td>\n      <td>52</td>\n      <td>42728</td>\n      <td>171</td>\n      <td>0.991059</td>\n      <td>0.766816</td>\n      <td>0.337945</td>\n      <td>0.469136</td>\n    </tr>\n    <tr>\n      <th>Western</th>\n      <td>240</td>\n      <td>87</td>\n      <td>42252</td>\n      <td>707</td>\n      <td>0.992446</td>\n      <td>0.890428</td>\n      <td>0.746568</td>\n      <td>0.812177</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": [
    "genre_dict = {i:genre for i, genre in enumerate(genres)}\n",
    "summary =evaluate(\"tokenized_movie_genres_test.jsonl\", \"tokenized_movie_genres_test.jsonl.out\", genre_dict, threshold=0.6)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy:  0.954885611316097\nMicro Precision:  0.6930519264333491\nMicro Recall:  0.7390640129101668\nMicro F1:  0.7153188143967596\n"
    }
   ],
   "source": [
    "tp_sum = summary[\"tp\"].sum()\n",
    "fp_sum = summary[\"fp\"].sum()\n",
    "tn_sum = summary[\"tn\"].sum()\n",
    "fn_sum = summary[\"fn\"].sum()\n",
    "precision = (tp_sum) / (tp_sum + fp_sum)\n",
    "recall = (tp_sum) / (tp_sum + fn_sum)\n",
    "\n",
    "print(\"Accuracy: \", (tp_sum + tn_sum) / (tp_sum + fp_sum + tn_sum + fn_sum))\n",
    "print(\"Micro Precision: \", precision)\n",
    "print(\"Micro Recall: \", recall)\n",
    "print(\"Micro F1: \", 2*precision*recall/(precision + recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compared the performance with [fastText](https://fasttext.cc/). Fasttext does not perform multi-label predictions, so to do a fair comparison we trained 28 binary classification models with fastText for each of the movie genres and combined the results of each predictor. While training the fastText models we set **wordNgrams** to 2, **dim** to 300 and  **pretrainedVectors** to the glove embeddings.\n",
    "\n",
    "<img src=\"comparison.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online inference demo\n",
    "\n",
    "In this section we setup a online inference endpoint and perform inference for a few recently released movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using already existing model: object2vec-2020-05-23-21-16-44-612\n--------------!"
    }
   ],
   "source": [
    "predictor = o2v.deploy(initial_instance_count=1, instance_type=\"ml.m4.xlarge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_genre_predictions(movie_summary, genre_dict, vocab, predictor, threshold=0.5):\n",
    "\n",
    "    plot_token_ids = [vocab[token] if token in vocab else vocab[UNKNOWN] for token in tokenize_plot_summary(movie_summary)]\n",
    "\n",
    "    batch = [{\"in0\": plot_token_ids, \"in1\": [genre_id]} for genre_id in range(len(genre_dict))]\n",
    "\n",
    "    request = {\"instances\": batch}\n",
    "    response = predictor.predict(data=json.dumps(request))\n",
    "\n",
    "    scores = [score[\"scores\"] for score in json.loads(response)[\"predictions\"]]\n",
    "\n",
    "    predictions = [genre_dict[i] for i, score in enumerate(scores) if score[1] > threshold]\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['Action', 'Adventure', 'Sci-Fi']"
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "source": [
    "star_trek = \"Ten years before Kirk, Spock and the Enterprise, theUSS Discovery discovers new worlds and lifeforms \\\n",
    "as one Starfleet officer learns to understand all things alien.\"\n",
    "\n",
    "get_movie_genre_predictions(star_trek, genre_dict, vocab, predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['Drama', 'Fantasy', 'Horror', 'Mystery', 'Thriller']"
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "source": [
    "nun = \"A priest with a haunted past and a novice on the threshold of her final vows are sent by the Vatican \\\n",
    "to investigate the death of a young nun in Romania and confront a malevolent force in the form of a demonic nun.\"\n",
    "\n",
    "get_movie_genre_predictions(nun, genre_dict, vocab, predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['Animation']"
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "source": [
    "fantastic_beasts = \"The second installment of the 'Fantastic Beasts' series set in J.K. Rowling's Wizarding World \\\n",
    "featuring the adventures of magizoologist Newt Scamander.\"\n",
    "\n",
    "get_movie_genre_predictions(fantastic_beasts, genre_dict, vocab, predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}