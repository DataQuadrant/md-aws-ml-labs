{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Horovod Distributed Training with SageMaker TensorFlow script mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Horovod is a distributed training framework based on Message Passing Interfae (MPI). For information about Horovod, see [Horovod README](https://github.com/uber/horovod).\n",
    "\n",
    "You can perform distributed training with Horovod on SageMaker by using the SageMaker Tensorflow container. If MPI is enabled when you create the training job, SageMaker creates the MPI environment and executes the `mpirun` command to execute the training script. Details on how to configure mpi settings in training job are described later in this example.\n",
    "\n",
    "In this example notebook, we create a Horovod training job that uses the MNIST data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the environment\n",
    "\n",
    "We get the `IAM` role that this notebook is running as and pass that role to the TensorFlow estimator that SageMaker uses to get data and perform training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Collecting keras\n  Downloading Keras-2.3.1-py2.py3-none-any.whl (377 kB)\n\u001b[K     |████████████████████████████████| 377 kB 1.2 MB/s \n\u001b[?25hRequirement already satisfied: scipy>=0.14 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from keras) (1.3.1)\nRequirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from keras) (3.12)\nRequirement already satisfied: keras-preprocessing>=1.0.5 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from keras) (1.1.0)\nRequirement already satisfied: keras-applications>=1.0.6 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from keras) (1.0.8)\nRequirement already satisfied: numpy>=1.9.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from keras) (1.16.4)\nRequirement already satisfied: six>=1.9.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from keras) (1.11.0)\nRequirement already satisfied: h5py in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from keras) (2.10.0)\nInstalling collected packages: keras\nSuccessfully installed keras-2.3.1\n"
    }
   ],
   "source": [
    "import sys\n",
    "! {sys.prefix}/bin/pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import os\n",
    "from sagemaker.utils import sagemaker_timestamp\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# default_s3_bucket = sagemaker_session.default_bucket()\n",
    "default_s3_bucket = 'md-labs-bucket'\n",
    "\n",
    "# sagemaker_iam_role = get_execution_role()\n",
    "sagemaker_iam_role = 'arn:aws:iam::868024899531:role/service-role/AmazonSageMaker-ExecutionRole-20200530T112594'\n",
    "\n",
    "train_script = \"mnist_hvd.py\"\n",
    "instance_count = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data for training\n",
    "\n",
    "Now we download the MNIST dataset to the local `/tmp/data/` directory and then upload it to an S3 bucket. After uploading the dataset to S3, we delete the data from `/tmp/data/`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\nDownloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n11493376/11490434 [==============================] - 8s 1us/step\nCompleted 34.2 MiB/44.9 MiB (696.0 KiB/s) with 1 file(s) remainingCompleted 34.5 MiB/44.9 MiB (696.3 KiB/s) with 1 file(s) remainingCompleted 34.8 MiB/44.9 MiB (696.5 KiB/s) with 1 file(s) remainingCompleted 35.0 MiB/44.9 MiB (696.8 KiB/s) with 1 file(s) remainingCompleted 35.2 MiB/44.9 MiB (699.0 KiB/s) with 1 file(s) remainingCompleted 35.5 MiB/44.9 MiB (701.8 KiB/s) with 1 file(s) remainingCompleted 35.8 MiB/44.9 MiB (699.8 KiB/s) with 1 file(s) remainingCompleted 36.0 MiB/44.9 MiB (694.4 KiB/s) with 1 file(s) remainingCompleted 36.2 MiB/44.9 MiB (697.3 KiB/s) with 1 file(s) remainingCompleted 36.5 MiB/44.9 MiB (701.3 KiB/s) with 1 file(s) remainingCompleted 36.8 MiB/44.9 MiB (702.4 KiB/s) with 1 file(s) remainingCompleted 37.0 MiB/44.9 MiB (691.4 KiB/s) with 1 file(s) remainingCompleted 37.2 MiB/44.9 MiB (694.0 KiB/s) with 1 file(s) remainingCompleted 37.5 MiB/44.9 MiB (696.6 KiB/s) with 1 file(s) remainingCompleted 37.8 MiB/44.9 MiB (698.2 KiB/s) with 1 file(s) remainingCompleted 38.0 MiB/44.9 MiB (699.2 KiB/s) with 1 file(s) remainingCompleted 38.2 MiB/44.9 MiB (697.7 KiB/s) with 1 file(s) remainingCompleted 38.5 MiB/44.9 MiB (696.2 KiB/s) with 1 file(s) remainingCompleted 38.8 MiB/44.9 MiB (690.8 KiB/s) with 1 file(s) remainingCompleted 39.0 MiB/44.9 MiB (691.8 KiB/s) with 1 file(s) remainingCompleted 39.2 MiB/44.9 MiB (687.7 KiB/s) with 1 file(s) remainingCompleted 39.5 MiB/44.9 MiB (691.1 KiB/s) with 1 file(s) remainingCompleted 39.8 MiB/44.9 MiB (688.7 KiB/s) with 1 file(s) remainingCompleted 40.0 MiB/44.9 MiB (680.2 KiB/s) with 1 file(s) remainingCompleted 40.2 MiB/44.9 MiB (678.3 KiB/s) with 1 file(s) remainingCompleted 40.5 MiB/44.9 MiB (679.9 KiB/s) with 1 file(s) remainingCompleted 40.8 MiB/44.9 MiB (682.4 KiB/s) with 1 file(s) remainingCompleted 41.0 MiB/44.9 MiB (677.1 KiB/s) with 1 file(s) remainingCompleted 41.2 MiB/44.9 MiB (678.1 KiB/s) with 1 file(s) remainingCompleted 41.5 MiB/44.9 MiB (676.5 KiB/s) with 1 file(s) remainingCompleted 41.7 MiB/44.9 MiB (677.7 KiB/s) with 1 file(s) remainingCompleted 41.9 MiB/44.9 MiB (680.8 KiB/s) with 1 file(s) remainingCompleted 42.2 MiB/44.9 MiB (677.4 KiB/s) with 1 file(s) remainingCompleted 42.4 MiB/44.9 MiB (680.2 KiB/s) with 1 file(s) remainingCompleted 42.7 MiB/44.9 MiB (683.4 KiB/s) with 1 file(s) remainingCompleted 42.9 MiB/44.9 MiB (676.2 KiB/s) with 1 file(s) remainingCompleted 43.2 MiB/44.9 MiB (679.0 KiB/s) with 1 file(s) remainingCompleted 43.4 MiB/44.9 MiB (671.2 KiB/s) with 1 file(s) remainingCompleted 43.7 MiB/44.9 MiB (673.9 KiB/s) with 1 file(s) remainingCompleted 43.9 MiB/44.9 MiB (675.0 KiB/s) with 1 file(s) remainingCompleted 44.2 MiB/44.9 MiB (670.2 KiB/s) with 1 file(s) remainingCompleted 44.4 MiB/44.9 MiB (665.7 KiB/s) with 1 file(s) remainingCompleted 44.7 MiB/44.9 MiB (661.8 KiB/s) with 1 file(s) remainingCompleted 44.9 MiB/44.9 MiB (659.6 KiB/s) with 1 file(s) remainingupload: ../../../../../../../tmp/data/mnist_train/train.npz to s3://md-labs-bucket/mnist/train.npz\nupload: ../../../../../../../tmp/data/mnist_test/test.npz to s3://md-labs-bucket/mnist/test.npz\ntraining data at  s3://md-labs-bucket/mnist/train.npz\ntest data at  s3://md-labs-bucket/mnist/test.npz\n"
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "s3_train_path = \"s3://{}/mnist/train.npz\".format(default_s3_bucket)\n",
    "s3_test_path = \"s3://{}/mnist/test.npz\".format(default_s3_bucket)\n",
    "\n",
    "# Create local directory\n",
    "! mkdir -p /tmp/data/mnist_train\n",
    "! mkdir -p /tmp/data/mnist_test\n",
    "\n",
    "# Save data locally\n",
    "np.savez('/tmp/data/mnist_train/train.npz', data=x_train, labels=y_train)\n",
    "np.savez('/tmp/data/mnist_test/test.npz', data=x_test, labels=y_test)\n",
    "\n",
    "# Upload the dataset to s3\n",
    "! aws s3 cp /tmp/data/mnist_train/train.npz $s3_train_path\n",
    "! aws s3 cp /tmp/data/mnist_test/test.npz $s3_test_path\n",
    "\n",
    "print('training data at ', s3_train_path)\n",
    "print('test data at ', s3_test_path)\n",
    "! rm -rf /tmp/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a script for horovod distributed training\n",
    "\n",
    "This example is based on the [Keras MNIST horovod example](https://github.com/uber/horovod/blob/master/examples/keras_mnist.py) example in the horovod github repository.\n",
    "\n",
    "To run this script we have to make following modifications:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Accept `--model_dir` as a command-line argument\n",
    "Modify the script to accept `model_dir` as a command-line argument that defines the directory path (i.e. `/opt/ml/model/`) where the output model is saved. Because Sagemaker deletes the training cluster when training completes, saving the model to `/opt/ml/model/` directory prevents the trained model from getting lost, because when the training job completes, SageMaker writes the data stored in `/opt/ml/model/` to an S3 bucket. \n",
    "\n",
    "This also allows the SageMaker training job to integrate with other SageMaker services, such as hosted inference endpoints or batch transform jobs. It also allows you to host the trained model outside of SageMaker.\n",
    "\n",
    "The following code adds `model_dir` as a command-line argument to the script:\n",
    "\n",
    "```\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model_dir', type=str)\n",
    "```\n",
    "\n",
    "More details can be found [here](https://github.com/aws/sagemaker-containers/blob/master/README.rst)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load train and test data\n",
    "\n",
    "You can get local directory path where the `train` and `test` data is downloaded by reading the environment variable `SM_CHANNEL_TRAIN` and `SM_CHANNEL_TEST` respectively.\n",
    "After you get the directory path, load the data into memory.\n",
    "\n",
    "Here is the code:\n",
    "\n",
    "```\n",
    "x_train = np.load(os.path.join(os.environ['SM_CHANNEL_TRAIN'], 'train.npz'))['data']\n",
    "y_train = np.load(os.path.join(os.environ['SM_CHANNEL_TRAIN'], 'train.npz'))['labels']\n",
    "\n",
    "x_test = np.load(os.path.join(os.environ['SM_CHANNEL_TEST'], 'test.npz'))['data']\n",
    "y_test = np.load(os.path.join(os.environ['SM_CHANNEL_TEST'], 'test.npz'))['labels']\n",
    "```\n",
    "\n",
    "For a list of all environment variables set by SageMaker that are accessible inside a training script, see [SageMaker Containers](https://github.com/aws/sagemaker-containers/blob/master/README.rst)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Save the model only at the master node\n",
    "\n",
    "Because in Horovod the training is distributed to multiple nodes, the model should only be saved by the master node. The following code in the script does this:\n",
    "\n",
    "```\n",
    "# Horovod: Save model only on worker 0 (i.e. master)\n",
    "if hvd.rank() == 0:\n",
    "    saved_model_path = tf.contrib.saved_model.save_keras_model(model, args.model_dir)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training script\n",
    "\n",
    "Here is the final training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\u001b[37m#     Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.\u001b[39;49;00m\n\u001b[37m#\u001b[39;49;00m\n\u001b[37m#     Licensed under the Apache License, Version 2.0 (the \"License\").\u001b[39;49;00m\n\u001b[37m#     You may not use this file except in compliance with the License.\u001b[39;49;00m\n\u001b[37m#     A copy of the License is located at\u001b[39;49;00m\n\u001b[37m#\u001b[39;49;00m\n\u001b[37m#         https://aws.amazon.com/apache-2-0/\u001b[39;49;00m\n\u001b[37m#\u001b[39;49;00m\n\u001b[37m#     or in the \"license\" file accompanying this file. This file is distributed\u001b[39;49;00m\n\u001b[37m#     on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\u001b[39;49;00m\n\u001b[37m#     express or implied. See the License for the specific language governing\u001b[39;49;00m\n\u001b[37m#     permissions and limitations under the License.\u001b[39;49;00m\n\n\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36m__future__\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m print_function\n\n\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n\n\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n\n\u001b[37m# import tensorflow.keras as keras\u001b[39;49;00m\n\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mkeras\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdatasets\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m mnist\n\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mkeras\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmodels\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Sequential\n\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mkeras\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mlayers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Dense, Dropout, Flatten\n\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mkeras\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mlayers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Conv2D, MaxPooling2D\n\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mkeras\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m backend \u001b[34mas\u001b[39;49;00m K\n\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mmath\u001b[39;49;00m\n\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtf\u001b[39;49;00m\n\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mhorovod\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mtensorflow\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mkeras\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mhvd\u001b[39;49;00m\n\n\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n    \n    num_gpus = \u001b[36mint\u001b[39;49;00m(os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_NUM_GPUS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n\n    parser = argparse.ArgumentParser()\n\n    \u001b[37m# Data, model, and output directories. These are required.\u001b[39;49;00m\n    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--output-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_OUTPUT_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m)\n    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAIN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--test\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TEST\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n\n    args, _ = parser.parse_known_args()\n\n    \u001b[37m# Horovod: initialize Horovod.\u001b[39;49;00m\n    hvd.init()\n\n    \u001b[37m# Horovod: pin GPU to be used to process local rank (one GPU per process)\u001b[39;49;00m\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = \u001b[34mTrue\u001b[39;49;00m\n    config.gpu_options.visible_device_list = \u001b[36mstr\u001b[39;49;00m(hvd.local_rank())\n    K.set_session(tf.Session(config=config))\n\n    batch_size = \u001b[34m128\u001b[39;49;00m\n    num_classes = \u001b[34m10\u001b[39;49;00m\n\n    \u001b[37m# Horovod: adjust number of epochs based on number of GPUs.\u001b[39;49;00m\n    epochs = \u001b[36mint\u001b[39;49;00m(math.ceil(\u001b[34m12.0\u001b[39;49;00m / hvd.size()))\n\n    \u001b[37m# Input image dimensions\u001b[39;49;00m\n    img_rows, img_cols = \u001b[34m28\u001b[39;49;00m, \u001b[34m28\u001b[39;49;00m\n\n    \u001b[37m# The data, shuffled and split between train and test sets\u001b[39;49;00m\n\n    x_train = np.load(os.path.join(args.train, \u001b[33m'\u001b[39;49;00m\u001b[33mtrain.npz\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))[\u001b[33m'\u001b[39;49;00m\u001b[33mdata\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n    y_train = np.load(os.path.join(args.train, \u001b[33m'\u001b[39;49;00m\u001b[33mtrain.npz\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))[\u001b[33m'\u001b[39;49;00m\u001b[33mlabels\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mTrain dataset loaded from: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(os.path.join(args.train, \u001b[33m'\u001b[39;49;00m\u001b[33mtrain.npz\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)))\n\n    x_test = np.load(os.path.join(args.test, \u001b[33m'\u001b[39;49;00m\u001b[33mtest.npz\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))[\u001b[33m'\u001b[39;49;00m\u001b[33mdata\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n    y_test = np.load(os.path.join(args.test, \u001b[33m'\u001b[39;49;00m\u001b[33mtest.npz\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))[\u001b[33m'\u001b[39;49;00m\u001b[33mlabels\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mTest dataset loaded from: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(os.path.join(args.test, \u001b[33m'\u001b[39;49;00m\u001b[33mtest.npz\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)))\n\n\n    \u001b[34mif\u001b[39;49;00m K.image_data_format() == \u001b[33m'\u001b[39;49;00m\u001b[33mchannels_first\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n        x_train = x_train.reshape(x_train.shape[\u001b[34m0\u001b[39;49;00m], \u001b[34m1\u001b[39;49;00m, img_rows, img_cols)\n        x_test = x_test.reshape(x_test.shape[\u001b[34m0\u001b[39;49;00m], \u001b[34m1\u001b[39;49;00m, img_rows, img_cols)\n        input_shape = (\u001b[34m1\u001b[39;49;00m, img_rows, img_cols)\n    \u001b[34melse\u001b[39;49;00m:\n        x_train = x_train.reshape(x_train.shape[\u001b[34m0\u001b[39;49;00m], img_rows, img_cols, \u001b[34m1\u001b[39;49;00m)\n        x_test = x_test.reshape(x_test.shape[\u001b[34m0\u001b[39;49;00m], img_rows, img_cols, \u001b[34m1\u001b[39;49;00m)\n        input_shape = (img_rows, img_cols, \u001b[34m1\u001b[39;49;00m)\n\n    x_train = x_train.astype(\u001b[33m'\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n    x_test = x_test.astype(\u001b[33m'\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n    x_train /= \u001b[34m255\u001b[39;49;00m\n    x_test /= \u001b[34m255\u001b[39;49;00m\n    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mx_train shape:\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, x_train.shape)\n    \u001b[36mprint\u001b[39;49;00m(x_train.shape[\u001b[34m0\u001b[39;49;00m], \u001b[33m'\u001b[39;49;00m\u001b[33mtrain samples\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n    \u001b[36mprint\u001b[39;49;00m(x_test.shape[\u001b[34m0\u001b[39;49;00m], \u001b[33m'\u001b[39;49;00m\u001b[33mtest samples\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n\n    \u001b[37m# Convert class vectors to binary class matrices\u001b[39;49;00m\n    y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n    y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n\n    model = Sequential()\n    model.add(Conv2D(\u001b[34m32\u001b[39;49;00m, kernel_size=(\u001b[34m3\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m),\n                     activation=\u001b[33m'\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n                     input_shape=input_shape))\n    model.add(Conv2D(\u001b[34m64\u001b[39;49;00m, (\u001b[34m3\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m), activation=\u001b[33m'\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n    model.add(MaxPooling2D(pool_size=(\u001b[34m2\u001b[39;49;00m, \u001b[34m2\u001b[39;49;00m)))\n    model.add(Dropout(\u001b[34m0.25\u001b[39;49;00m))\n    model.add(Flatten())\n    model.add(Dense(\u001b[34m128\u001b[39;49;00m, activation=\u001b[33m'\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n    model.add(Dropout(\u001b[34m0.5\u001b[39;49;00m))\n    model.add(Dense(num_classes, activation=\u001b[33m'\u001b[39;49;00m\u001b[33msoftmax\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n\n    \u001b[37m# Horovod: adjust learning rate based on number of GPUs.\u001b[39;49;00m\n    opt = tf.keras.optimizers.Adadelta(\u001b[34m1.0\u001b[39;49;00m * hvd.size())\n\n    \u001b[37m# Horovod: add Horovod Distributed Optimizer.\u001b[39;49;00m\n    opt = hvd.DistributedOptimizer(opt)\n\n    model.compile(loss=tf.keras.losses.categorical_crossentropy,\n                  optimizer=opt,\n                  metrics=[\u001b[33m'\u001b[39;49;00m\u001b[33maccuracy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n\n    callbacks = [\n        \u001b[37m# Horovod: broadcast initial variable states from rank 0 to all other processes.\u001b[39;49;00m\n        \u001b[37m# This is necessary to ensure consistent initialization of all workers when\u001b[39;49;00m\n        \u001b[37m# training is started with random weights or restored from a checkpoint.\u001b[39;49;00m\n        hvd.callbacks.BroadcastGlobalVariablesCallback(\u001b[34m0\u001b[39;49;00m),\n    ]\n\n    \u001b[37m# Horovod: save checkpoints only on worker 0 to prevent other workers from corrupting them.\u001b[39;49;00m\n    \u001b[34mif\u001b[39;49;00m hvd.rank() == \u001b[34m0\u001b[39;49;00m:\n        callbacks.append(tf.keras.callbacks.ModelCheckpoint(\u001b[33m'\u001b[39;49;00m\u001b[33m./checkpoint-\u001b[39;49;00m\u001b[33m{epoch}\u001b[39;49;00m\u001b[33m.h5\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n\n    model.fit(x_train, y_train,\n              batch_size=batch_size,\n              callbacks=callbacks,\n              epochs=epochs,\n              verbose=\u001b[34m1\u001b[39;49;00m,\n              validation_data=(x_test, y_test))\n    score = model.evaluate(x_test, y_test, verbose=\u001b[34m0\u001b[39;49;00m)\n\n    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mTest loss:\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, score[\u001b[34m0\u001b[39;49;00m])\n    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mTest accuracy:\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, score[\u001b[34m1\u001b[39;49;00m])\n\n    \u001b[37m# Horovod: Save model only on worker 0 (i.e. master)\u001b[39;49;00m\n    \u001b[34mif\u001b[39;49;00m hvd.rank() == \u001b[34m0\u001b[39;49;00m:\n        model.save(os.path.join(args.model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.h5\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n"
    }
   ],
   "source": [
    "# !cat 'mnist_hvd.py'\n",
    "!pygmentize 'mnist_hvd.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test locally using SageMaker Python SDK TensorFlow Estimator\n",
    "\n",
    "You can use the SageMaker Python SDK TensorFlow estimator to easily train locally and in SageMaker.\n",
    "\n",
    "This notebook shows how to use the SageMaker Python SDK to run your code in a local container before deploying to SageMaker's managed training or hosting environments. Just change your estimator's `train_instance_type` to `local` or `local_gpu`. For more information, see: https://github.com/aws/sagemaker-python-sdk#local-mode.\n",
    "\n",
    "To use this feature, you need to install docker-compose (and nvidia-docker if you are training with a GPU). Run the following script to install docker-compose or nvidia-docker-compose, and configure the notebook environment for you.\n",
    "\n",
    "**Note**: You can only run a single local notebook at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "/bin/bash: ./setup.sh: No such file or directory\n"
    }
   ],
   "source": [
    "!/bin/bash ./setup.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train locally, set `train_instance_type` to `local`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_instance_type='local'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MPI environment for Horovod can be configured by setting the following flags in the `mpi` field of the `distribution` dictionary that you pass to the TensorFlow estimator :\n",
    "\n",
    "* ``enabled (bool)``: If set to ``True``, the MPI setup is performed and ``mpirun`` command is executed.\n",
    "* ``processes_per_host (int) [Optional]``: Number of processes MPI should launch on each host. Note, this should not be greater than the available slots on the selected instance type. This flag should be set for the multi-cpu/gpu training.\n",
    "* ``custom_mpi_options (str) [Optional]``: Any mpirun flag(s) can be passed in this field that will be added to the mpirun command executed by SageMaker to launch distributed horovod training.\n",
    "\n",
    "For more information about the `distribution` dictionary, see the SageMaker Python SDK [README](https://github.com/aws/sagemaker-python-sdk/blob/v1.17.3/src/sagemaker/tensorflow/README.rst).\n",
    "\n",
    "First, enable MPI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distributions = {'mpi': {'enabled': True}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create the Tensorflow estimator passing the `train_instance_type` and `distribution`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_local = TensorFlow(entry_point=train_script,\n",
    "                       role=sagemaker_iam_role,\n",
    "                       train_instance_count=instance_count,\n",
    "                       train_instance_type=train_instance_type,\n",
    "                       script_mode=True,\n",
    "                       framework_version='1.15.2',\n",
    "                       py_version='py3',\n",
    "                       distributions=distributions,\n",
    "                       base_job_name='hvd-mnist-local')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call `fit()` to start the local training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_local.fit({\"train\":s3_train_path, \"test\":s3_test_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train in SageMaker\n",
    "\n",
    "After you test the training job locally, run it on SageMaker:\n",
    "\n",
    "First, change the instance type from `local` to the valid EC2 instance type. For example, `ml.c4.xlarge`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_instance_type='ml.c4.xlarge'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also provide your custom MPI options by passing in the `custom_mpi_options` field of `distribution` dictionary that will be added to the `mpirun` command executed by SageMaker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distributions = {'mpi': {'enabled': True, \"custom_mpi_options\": \"-verbose --NCCL_DEBUG=INFO\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create the Tensorflow estimator passing the `train_instance_type` and `distribution` to launch the training job in sagemaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = TensorFlow(entry_point=train_script,\n",
    "                       role=sagemaker_iam_role,\n",
    "                       train_instance_count=instance_count,\n",
    "                       train_instance_type=train_instance_type,\n",
    "                       script_mode=True,\n",
    "                       framework_version='1.15.2',\n",
    "                       py_version='py3',\n",
    "                       distributions=distributions,\n",
    "                       base_job_name='hvd-mnist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call `fit()` to start the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit({\"train\":s3_train_path, \"test\":s3_test_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Horovod training in SageMaker using multiple CPU/GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enable mulitiple CPUs or GPUs for horovod training, set the `processes_per_host` field in the `mpi` section of the `distribution` dictionary to the desired value of processes that will be executed per instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distributions = {'mpi': {'enabled': True, \"processes_per_host\": 2}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create the Tensorflow estimator passing the `train_instance_type` and `distribution`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = TensorFlow(entry_point=train_script,\n",
    "                       role=sagemaker_iam_role,\n",
    "                       train_instance_count=instance_count,\n",
    "                       train_instance_type=train_instance_type,\n",
    "                       script_mode=True,\n",
    "                       framework_version='1.15.2',\n",
    "                       py_version='py3',\n",
    "                       distributions=distributions,\n",
    "                       base_job_name='hvd-mnist-multi-cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call `fit()` to start the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit({\"train\":s3_train_path, \"test\":s3_test_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving horovod training performance on SageMaker\n",
    "\n",
    "Performing Horovod training inside a VPC improves the network latency between nodes, leading to higher performance and stability of Horovod training jobs.\n",
    "\n",
    "For a detailed explanation of how to configure a VPC for SageMaker training, see [Secure Training and Inference with VPC](https://github.com/aws/sagemaker-python-sdk#secure-training-and-inference-with-vpc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup VPC infrastructure\n",
    "We will setup following resources as part of VPC stack:\n",
    "* `VPC`: AWS Virtual private cloud with CIDR block.\n",
    "* `Subnets`: Two subnets with the CIDR blocks `10.0.0.0/24` and `10.0.1.0/24`\n",
    "* `Security Group`: Defining the open ingress and egress ports, such as TCP.\n",
    "* `VpcEndpoint`: S3 Vpc endpoint allowing sagemaker's vpc cluster to dosenload data from S3.\n",
    "* `Route Table`: Defining routes and is tied to subnets and VPC.\n",
    "\n",
    "Complete cloud formation template for setting up the VPC stack can be seen [here](./vpc_infra_cfn.json)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "from time import sleep\n",
    "\n",
    "def create_vpn_infra(stack_name=\"hvdvpcstack\"):\n",
    "    cfn = boto3.client(\"cloudformation\")\n",
    "\n",
    "    cfn_template = open(\"vpc_infra_cfn.json\", \"r\").read()\n",
    "    \n",
    "    try:\n",
    "        vpn_stack = cfn.create_stack(StackName=(stack_name),\n",
    "                                     TemplateBody=cfn_template)\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'AlreadyExistsException':\n",
    "            print(\"Stack: {} already exists, so skipping stack creation.\".format(stack_name))\n",
    "        else:\n",
    "            print(\"Unexpected error: %s\" % e)\n",
    "            raise e\n",
    "\n",
    "    describe_stack = cfn.describe_stacks(StackName=stack_name)[\"Stacks\"][0]\n",
    "\n",
    "    while describe_stack[\"StackStatus\"] == \"CREATE_IN_PROGRESS\":\n",
    "        describe_stack = cfn.describe_stacks(StackName=stack_name)[\"Stacks\"][0]\n",
    "        sleep(0.5)\n",
    "\n",
    "    if describe_stack[\"StackStatus\"] != \"CREATE_COMPLETE\":\n",
    "        raise ValueError(\"Stack creation failed in state: {}\".format(describe_stack[\"StackStatus\"]))\n",
    "\n",
    "    print(\"Stack: {} created successfully with status: {}\".format(stack_name, describe_stack[\"StackStatus\"]))\n",
    "\n",
    "    subnets = []\n",
    "    security_groups = []\n",
    "\n",
    "    for output_field in describe_stack[\"Outputs\"]:\n",
    "\n",
    "        if output_field[\"OutputKey\"] == \"SecurityGroupId\":\n",
    "            security_groups.append(output_field[\"OutputValue\"])\n",
    "        if output_field[\"OutputKey\"] == \"Subnet1Id\" or output_field[\"OutputKey\"] == \"Subnet2Id\":\n",
    "            subnets.append(output_field[\"OutputValue\"])\n",
    "\n",
    "    return subnets, security_groups\n",
    "\n",
    "\n",
    "subnets, security_groups = create_vpn_infra()\n",
    "print(\"Subnets: {}\".format(subnets))\n",
    "print(\"Security Groups: {}\".format(security_groups))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VPC training in SageMaker\n",
    "Now, we create the Tensorflow estimator, passing the `train_instance_type` and `distribution`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = TensorFlow(entry_point=train_script,\n",
    "                       role=sagemaker_iam_role,\n",
    "                       train_instance_count=instance_count,\n",
    "                       train_instance_type=train_instance_type,\n",
    "                       script_mode=True,\n",
    "                       framework_version='1.15.2',\n",
    "                       py_version='py3',\n",
    "                       distributions=distributions,\n",
    "                       security_group_ids=security_groups,\n",
    "                       subnets=subnets,\n",
    "                       base_job_name='hvd-mnist-vpc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call `fit()` to start the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit({\"train\":s3_train_path, \"test\":s3_test_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training is completed, you can host the saved model by using TensorFlow Serving on SageMaker. For an example that uses TensorFlow Serving, see [(https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/tensorflow_serving_container/tensorflow_serving_container.ipynb](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/tensorflow_serving_container/tensorflow_serving_container.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference Links:\n",
    "* [SageMaker Container MPI Support.](https://github.com/aws/sagemaker-containers/blob/master/src/sagemaker_containers/_mpi.py)\n",
    "* [Horovod Official Documentation](https://github.com/uber/horovod)\n",
    "* [SageMaker Tensorflow script mode example.](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/tensorflow_script_mode_quickstart/tensorflow_script_mode_quickstart.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}