{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "resources:  \n",
    "[AWS docs - Build a multi model container](https://docs.aws.amazon.com/sagemaker/latest/dg/build-multi-model-build-container.html)  \n",
    "[Multi Mode Server on github](https://github.com/awslabs/multi-model-server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker Multi-Model Endpoints using your own algorithm container\n",
    "With [Amazon SageMaker multi-model endpoints](https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html), customers can create an endpoint that seamlessly hosts up to thousands of models. These endpoints are well suited to use cases where any one of a large number of models, which can be served from a common inference container, needs to be invokable on-demand and where it is acceptable for infrequently invoked models to incur some additional latency. For applications which require consistently low inference latency, a traditional endpoint is still the best choice.\n",
    "\n",
    "At a high level, Amazon SageMaker manages the loading and unloading of models for a multi-model endpoint, as they are needed. \n",
    "  \n",
    "When an invocation request is made for a particular model, Amazon SageMaker  \n",
    "- routes the request to an instance assigned to that model, \n",
    "- downloads the model artifacts from S3 onto that instance, and  \n",
    "- initiates loading of the model into the memory of the container. \n",
    "  \n",
    "As soon as the loading is complete, Amazon SageMaker performs the requested invocation and returns the result. \n",
    "  \n",
    "If the model is already loaded in memory on the selected instance, the downloading and loading steps are skipped and the invocation is performed immediately.\n",
    "\n",
    "For the inference container to serve multiple models in a multi-model endpoint, it must implement [additional APIs](https://docs.aws.amazon.com/sagemaker/latest/dg/build-multi-model-build-container.html) in order to load, list, get, unload and invoke specific models. \n",
    "  \n",
    "This notebook demonstrates how to build your own inference container that implements these APIs.\n",
    "\n",
    "---\n",
    "\n",
    "### Contents\n",
    "\n",
    "1. [Introduction to Multi Model Server (MMS)](#Introduction-to-Multi-Model-Server-(MMS))\n",
    "  1. [Handling Out Of Memory conditions](#Handling-Out-Of-Memory-conditions)\n",
    "  1. [SageMaker Inference Toolkit](#SageMaker-Inference-Toolkit)\n",
    "1. [Building and registering a container using MMS](#Building-and-registering-a-container-using-MMS)\n",
    "1. [Set up the environment](#Set-up-the-environment)\n",
    "1. [Upload model artifacts to S3](#Upload-model-artifacts-to-S3)\n",
    "1. [Create a multi-model endpoint](#Create-a-multi-model-endpoint)\n",
    "  1. [Import models into hosting](#Import-models-into-hosting)\n",
    "  1. [Create endpoint configuration](#Create-endpoint-configuration)\n",
    "  1. [Create endpoint](#Create-endpoint)\n",
    "1. [Invoke models](#Invoke-models)\n",
    "  1. [Add models to the endpoint](#Add-models-to-the-endpoint)\n",
    "  1. [Updating a model](#Updating-a-model)\n",
    "1. [(Optional) Delete the hosting resources](#(Optional)-Delete-the-hosting-resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Multi Model Server (MMS)\n",
    "\n",
    "[Multi Model Server](https://github.com/awslabs/multi-model-server) is an open source framework for serving machine learning models. It provides the HTTP frontend and model management capabilities required by multi-model endpoints to host multiple models within a single container, load models into and unload models out of the container dynamically, and performing inference on a specified loaded model.\n",
    "\n",
    "MMS supports a pluggable custom backend handler where you can implement your own algorithm. This example uses a handler that supports loading and inference for MXNet models, which we will inspect below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\"\"\n",
      "ModelHandler defines an example model handler for load and inference requests for MXNet CPU models\n",
      "\"\"\"\n",
      "from collections import namedtuple\n",
      "import glob\n",
      "import json\n",
      "import logging\n",
      "import os\n",
      "import re\n",
      "\n",
      "import mxnet as mx\n",
      "import numpy as np\n",
      "\n",
      "class ModelHandler(object):\n",
      "    \"\"\"\n",
      "    A sample Model handler implementation.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self):\n",
      "        self.initialized = False\n",
      "        self.mx_model = None\n",
      "        self.shapes = None\n",
      "\n",
      "    def get_model_files_prefix(self, model_dir):\n",
      "        \"\"\"\n",
      "        Get the model prefix name for the model artifacts (symbol and parameter file).\n",
      "        This assume model artifact directory contains a symbol file, parameter file, \n",
      "        model shapes file and a synset file defining the labels\n",
      "\n",
      "        :param model_dir: Path to the directory with model artifacts\n",
      "        :return: prefix string for model artifact files\n",
      "        \"\"\"\n",
      "        sym_file_suffix = \"-symbol.json\"\n",
      "        checkpoint_prefix_regex = \"{}/*{}\".format(model_dir, sym_file_suffix) # Ex output: /opt/ml/models/resnet-18/model/*-symbol.json\n",
      "        checkpoint_prefix_filename = glob.glob(checkpoint_prefix_regex)[0] # Ex output: /opt/ml/models/resnet-18/model/resnet18-symbol.json\n",
      "        checkpoint_prefix = os.path.basename(checkpoint_prefix_filename).split(sym_file_suffix)[0] # Ex output: resnet18\n",
      "        logging.info(\"Prefix for the model artifacts: {}\".format(checkpoint_prefix))\n",
      "        return checkpoint_prefix\n",
      "\n",
      "    def get_input_data_shapes(self, model_dir, checkpoint_prefix):\n",
      "        \"\"\"\n",
      "        Get the model input data shapes and return the list\n",
      "\n",
      "        :param model_dir: Path to the directory with model artifacts\n",
      "        :param checkpoint_prefix: Model files prefix name\n",
      "        :return: prefix string for model artifact files\n",
      "        \"\"\"\n",
      "        shapes_file_path = os.path.join(model_dir, \"{}-{}\".format(checkpoint_prefix, \"shapes.json\"))\n",
      "        if not os.path.isfile(shapes_file_path):\n",
      "            raise RuntimeError(\"Missing {} file.\".format(shapes_file_path))\n",
      "\n",
      "        with open(shapes_file_path) as f:\n",
      "            self.shapes = json.load(f)\n",
      "\n",
      "        data_shapes = []\n",
      "\n",
      "        for input_data in self.shapes:\n",
      "            data_name = input_data[\"name\"]\n",
      "            data_shape = input_data[\"shape\"]\n",
      "            data_shapes.append((data_name, tuple(data_shape)))\n",
      "\n",
      "        return data_shapes\n",
      "\n",
      "    def initialize(self, context):\n",
      "        \"\"\"\n",
      "        Initialize model. This will be called during model loading time\n",
      "        :param context: Initial context contains model server system properties.\n",
      "        :return:\n",
      "        \"\"\"\n",
      "        self.initialized = True\n",
      "        properties = context.system_properties\n",
      "        # Contains the url parameter passed to the load request\n",
      "        model_dir = properties.get(\"model_dir\") \n",
      "        gpu_id = properties.get(\"gpu_id\")\n",
      "\n",
      "        checkpoint_prefix = self.get_model_files_prefix(model_dir)\n",
      "\n",
      "        # Read the model input data shapes\n",
      "        data_shapes = self.get_input_data_shapes(model_dir, checkpoint_prefix)\n",
      "         \n",
      "        # Load MXNet model\n",
      "        try:\n",
      "            ctx = mx.cpu() # Set the context on CPU\n",
      "            sym, arg_params, aux_params = mx.model.load_checkpoint(checkpoint_prefix, 0)  # epoch set to 0\n",
      "            self.mx_model = mx.mod.Module(symbol=sym, context=ctx, label_names=None)\n",
      "            self.mx_model.bind(for_training=False, data_shapes=data_shapes, \n",
      "                   label_shapes=self.mx_model._label_shapes)\n",
      "            self.mx_model.set_params(arg_params, aux_params, allow_missing=True)\n",
      "            with open(\"synset.txt\", 'r') as f:\n",
      "                self.labels = [l.rstrip() for l in f]\n",
      "        except (mx.base.MXNetError, RuntimeError) as memerr:\n",
      "            if re.search('Failed to allocate (.*) Memory', str(memerr), re.IGNORECASE):\n",
      "                logging.error(\"Memory allocation exception: {}\".format(memerr))\n",
      "                raise MemoryError\n",
      "            raise           \n",
      "\n",
      "    def preprocess(self, request):\n",
      "        \"\"\"\n",
      "        Transform raw input into model input data.\n",
      "        :param request: list of raw requests\n",
      "        :return: list of preprocessed model input data\n",
      "        \"\"\"\n",
      "        # Take the input data and pre-process it make it inference ready\n",
      "\n",
      "        img_list = []\n",
      "        for idx, data in enumerate(request):\n",
      "            # Read the bytearray of the image from the input\n",
      "            img_arr = data.get('body')  \n",
      "\n",
      "            # Input image is in bytearray, convert it to MXNet NDArray\n",
      "            img = mx.img.imdecode(img_arr) \n",
      "            if img is None:\n",
      "                return None\n",
      "\n",
      "            # convert into format (batch, RGB, width, height)\n",
      "            img = mx.image.imresize(img, 224, 224) # resize\n",
      "            img = img.transpose((2, 0, 1)) # Channel first\n",
      "            img = img.expand_dims(axis=0) # batchify\n",
      "            img_list.append(img)\n",
      "\n",
      "        return img_list\n",
      "\n",
      "    def inference(self, model_input):\n",
      "        \"\"\"\n",
      "        Internal inference methods\n",
      "        :param model_input: transformed model input data list\n",
      "        :return: list of inference output in NDArray\n",
      "        \"\"\"\n",
      "        # Do some inference call to engine here and return output\n",
      "        Batch = namedtuple('Batch', ['data'])\n",
      "        self.mx_model.forward(Batch(model_input))\n",
      "        prob = self.mx_model.get_outputs()[0].asnumpy()\n",
      "        return prob\n",
      "\n",
      "    def postprocess(self, inference_output):\n",
      "        \"\"\"\n",
      "        Return predict result in as list.\n",
      "        :param inference_output: list of inference output\n",
      "        :return: list of predict results\n",
      "        \"\"\"\n",
      "        # Take output from network and post-process to desired format\n",
      "        prob = np.squeeze(inference_output)\n",
      "        a = np.argsort(prob)[::-1]\n",
      "        return [['probability=%f, class=%s' %(prob[i], self.labels[i]) for i in a[0:5]]]\n",
      "        \n",
      "    def handle(self, data, context):\n",
      "        \"\"\"\n",
      "        Call preprocess, inference and post-process functions\n",
      "        :param data: input data\n",
      "        :param context: mms context\n",
      "        \"\"\"\n",
      "        \n",
      "        model_input = self.preprocess(data)\n",
      "        model_out = self.inference(model_input)\n",
      "        return self.postprocess(model_out)\n",
      "\n",
      "_service = ModelHandler()\n",
      "\n",
      "\n",
      "def handle(data, context):\n",
      "    if not _service.initialized:\n",
      "        _service.initialize(context)\n",
      "\n",
      "    if data is None:\n",
      "        return None\n",
      "\n",
      "    return _service.handle(data, context)\n"
     ]
    }
   ],
   "source": [
    "! cat container/model_handler.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of note are the `handle(data, context)` and `initialize(self, context)` methods.\n",
    "\n",
    "The `initialize` method will be called when a model is loaded into memory. In this example, it loads the model artifacts at `model_dir` into MXNet.\n",
    "\n",
    "The `handle` method will be called when invoking the model. In this example, it validates the input payload and then forwards the input to MXNet, returning the output.\n",
    "\n",
    "This handler class is instantiated for every model loaded into the container, so state in the handler is not shared across models.\n",
    "\n",
    "### Handling Out Of Memory conditions\n",
    "If MXNet fails to load the model due to lack of memory, a `MemoryError` is raised. Any time a model cannot be loaded due to lack of memory or any other resource constraint, a `MemoryError` must be raised. MMS will interpret the `MemoryError`, and return a 507 HTTP status code to SageMaker, where SageMaker will initiate unloading unused models to reclaim resources so the requested model can be loaded.\n",
    "\n",
    "### SageMaker Inference Toolkit\n",
    "MMS supports [various settings](https://github.com/awslabs/multi-model-server/blob/master/docker/advanced_settings.md#description-of-config-file-settings) for the frontend server it starts.\n",
    "\n",
    "[SageMaker Inference Toolkit](https://github.com/aws/sagemaker-inference-toolkit) is a library that bootstraps MMS in a way that is compatible with SageMaker multi-model endpoints, while still allowing you to tweak important performance parameters, such as the number of workers per model. The inference container in this example uses the Inference Toolkit to start MMS which can be seen in the __`container/dockerd-entrypoint.py`__ file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and registering a container using MMS\n",
    "\n",
    "The shell script below will build a Docker image which uses MMS as the front end (configured through SageMaker Inference Toolkit), and `container/model_handler.py` that we inspected above as the backend handler. It will then upload the image to an ECR repository in your account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "sha256:e4646af98fa279d3ff01acfc2c44ec7a49bdc01e58d03500b25f0f4ace7f6b85\n",
      "The push refers to repository [868024899531.dkr.ecr.us-east-2.amazonaws.com/demo-sagemaker-multimodel]\n",
      "2bdff9425f8f: Preparing\n",
      "9b8e4227eb2e: Preparing\n",
      "c9dd188bbf3d: Preparing\n",
      "ff19f4f3267a: Preparing\n",
      "ced76b2cd145: Preparing\n",
      "068b2e491227: Preparing\n",
      "ef5afc19235f: Preparing\n",
      "92f15a626238: Preparing\n",
      "fa1693d66d0b: Preparing\n",
      "293b479c17a5: Preparing\n",
      "bd95983a8d99: Preparing\n",
      "96eda0f553ba: Preparing\n",
      "293b479c17a5: Waiting\n",
      "bd95983a8d99: Waiting\n",
      "96eda0f553ba: Waiting\n",
      "fa1693d66d0b: Waiting\n",
      "ced76b2cd145: Waiting\n",
      "068b2e491227: Waiting\n",
      "ef5afc19235f: Waiting\n",
      "92f15a626238: Waiting\n",
      "2bdff9425f8f: Layer already exists\n",
      "ff19f4f3267a: Layer already exists\n",
      "9b8e4227eb2e: Layer already exists\n",
      "ced76b2cd145: Layer already exists\n",
      "068b2e491227: Layer already exists\n",
      "92f15a626238: Layer already exists\n",
      "fa1693d66d0b: Layer already exists\n",
      "c9dd188bbf3d: Layer already exists\n",
      "bd95983a8d99: Layer already exists\n",
      "96eda0f553ba: Layer already exists\n",
      "ef5afc19235f: Layer already exists\n",
      "293b479c17a5: Layer already exists\n",
      "latest: digest: sha256:293097841174fa1424144bff7b50878a6775c46b48a105803a23b39d1e543f6f size: 2820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "# The name of our algorithm\n",
    "algorithm_name=demo-sagemaker-multimodel\n",
    "\n",
    "cd container\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "# region=${region:-us-west-2}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "$(aws ecr get-login --region ${region} --no-include-email)\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "docker build -q -t ${algorithm_name} .\n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the environment\n",
    "Define the S3 bucket and prefix where the model artifacts that will be invokable by your multi-model endpoint will be located.\n",
    "\n",
    "Also define the IAM role that will give SageMaker access to the model artifacts and ECR image that was created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -qU awscli boto3 sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sm_client = boto3.client(service_name='sagemaker')\n",
    "runtime_sm_client = boto3.client(service_name='sagemaker-runtime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'868024899531'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "account_id = boto3.client('sts').get_caller_identity()['Account']\n",
    "account_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'us-east-2'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "region = boto3.Session().region_name\n",
    "region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sagemaker-us-east-2-868024899531'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket = 'sagemaker-{}-{}'.format(region, account_id)\n",
    "bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'md-ml-labs-bucket'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'DEMO-multimodel-endpoint'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolved SageMaker IAM Role to: {'Path': '/service-role/', 'RoleName': 'AmazonSageMaker-ExecutionRole-20200208T092301', 'RoleId': 'AROA4UGSQ27FVTPNSTGPW', 'Arn': 'arn:aws:iam::868024899531:role/service-role/AmazonSageMaker-ExecutionRole-20200208T092301', 'CreateDate': datetime.datetime(2020, 2, 8, 15, 23, 34, tzinfo=tzlocal()), 'AssumeRolePolicyDocument': {'Version': '2012-10-17', 'Statement': [{'Effect': 'Allow', 'Principal': {'Service': 'sagemaker.amazonaws.com'}, 'Action': 'sts:AssumeRole'}]}, 'Description': 'SageMaker execution role created from the SageMaker AWS Management Console.', 'MaxSessionDuration': 3600}\n",
      "arn:aws:iam::868024899531:role/service-role/AmazonSageMaker-ExecutionRole-20200208T092301\n"
     ]
    }
   ],
   "source": [
    "# # execute this on aws sagemaker\n",
    "# role = get_execution_role()\n",
    "\n",
    "# use this if running sagemaker locally\n",
    "def resolve_sm_role():\n",
    "    client = boto3.client('iam', region_name='us-east-2')\n",
    "    response_roles = client.list_roles(\n",
    "        PathPrefix='/',\n",
    "        # Marker='string',\n",
    "        MaxItems=999\n",
    "    )\n",
    "    for role in response_roles['Roles']:\n",
    "        if role['RoleName'].startswith('AmazonSageMaker-ExecutionRole-'):\n",
    "            print('Resolved SageMaker IAM Role to: ' + str(role))\n",
    "            return role['Arn']\n",
    "    raise Exception('Could not resolve what should be the SageMaker role to be used')\n",
    "\n",
    "# this is the role created by sagemaker notebook on aws\n",
    "role_arn = resolve_sm_role()\n",
    "print(role_arn)\n",
    "role=role_arn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload model artifacts to S3\n",
    "In this example we will use pre-trained ResNet 18 and ResNet 152 models, both trained on the ImageNet datset. First we will download the models from MXNet's model zoo, and then upload them to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import os\n",
    "import tarfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'http://data.mxnet.io/models/imagenet/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/resnet_18/synset.txt'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mx.test_utils.download(model_path+'resnet/18-layers/resnet-18-0000.params', None, 'data/resnet_18')\n",
    "mx.test_utils.download(model_path+'resnet/18-layers/resnet-18-symbol.json', None, 'data/resnet_18')\n",
    "mx.test_utils.download(model_path+'synset.txt', None, 'data/resnet_18')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/resnet_18/resnet-18-shapes.json', 'w') as file:\n",
    "    file.write('[{\"shape\": [1, 3, 224, 224], \"name\": \"data\"}]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a resnet_18.tar.gz with the content of data/resnet_18 folder\n",
    "with tarfile.open('data/resnet_18.tar.gz', 'w:gz') as tar:\n",
    "    tar.add('data/resnet_18', arcname='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/resnet_152/synset.txt'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mx.test_utils.download(model_path+'resnet/152-layers/resnet-152-0000.params', None, 'data/resnet_152')\n",
    "mx.test_utils.download(model_path+'resnet/152-layers/resnet-152-symbol.json', None, 'data/resnet_152')\n",
    "mx.test_utils.download(model_path+'synset.txt', None, 'data/resnet_152')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/resnet_152/resnet-152-shapes.json', 'w') as file:\n",
    "    file.write('[{\"shape\": [1, 3, 224, 224], \"name\": \"data\"}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create resnet_152.tar.gz with the content of data/resnet_152 folder    \n",
    "with tarfile.open('data/resnet_152.tar.gz', 'w:gz') as tar:\n",
    "    tar.add('data/resnet_152', arcname='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# upload the two model artifacts created above to our bucket\n",
    "# this takes a wile, about 300 Mb\n",
    "from botocore.client import ClientError\n",
    "import os\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "try:\n",
    "    s3.meta.client.head_bucket(Bucket=bucket)\n",
    "except ClientError:\n",
    "    s3.create_bucket(\n",
    "        Bucket=bucket,\n",
    "        CreateBucketConfiguration={\n",
    "            'LocationConstraint': region\n",
    "        })\n",
    "\n",
    "models = {'resnet_18.tar.gz', 'resnet_152.tar.gz'}\n",
    "\n",
    "for model in models:\n",
    "    key = os.path.join(prefix, model)\n",
    "    with open('data/'+model, 'rb') as file_obj:\n",
    "        s3.Bucket(bucket).Object(key).upload_fileobj(file_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a multi-model endpoint\n",
    "### Import models into hosting\n",
    "When creating the Model entity for multi-model endpoints, the container's `ModelDataUrl` is the S3 prefix where the model artifacts that are invokable by the endpoint are located. The rest of the S3 path will be specified when invoking the model.\n",
    "\n",
    "The `Mode` of container is specified as `MultiModel` to signify that the container will host multiple models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: DEMO-MultiModelModel-2020-02-09-18-11-42\n",
      "Model data Url: https://s3-us-east-2.amazonaws.com/md-ml-labs-bucket/DEMO-multimodel-endpoint/\n",
      "Container image: 868024899531.dkr.ecr.us-east-2.amazonaws.com/demo-sagemaker-multimodel:latest\n"
     ]
    }
   ],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "model_name = 'DEMO-MultiModelModel-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "model_url = 'https://s3-{}.amazonaws.com/{}/{}/'.format(region, bucket, prefix)\n",
    "container = '{}.dkr.ecr.{}.amazonaws.com/{}:latest'.format(account_id, region, 'demo-sagemaker-multimodel')\n",
    "\n",
    "print('Model name: ' + model_name)\n",
    "print('Model data Url: ' + model_url)\n",
    "print('Container image: ' + container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Image': '868024899531.dkr.ecr.us-east-2.amazonaws.com/demo-sagemaker-multimodel:latest',\n",
       " 'ModelDataUrl': 'https://s3-us-east-2.amazonaws.com/md-ml-labs-bucket/DEMO-multimodel-endpoint/',\n",
       " 'Mode': 'MultiModel'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "container_param = {\n",
    "    'Image': container,\n",
    "    'ModelDataUrl': model_url,\n",
    "    'Mode': 'MultiModel'\n",
    "}\n",
    "container_param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reference:  \n",
    "[Create Model in AWS docs](https://docs.aws.amazon.com/goto/WebAPI/sagemaker-2017-07-24/CreateModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName = model_name,\n",
    "    ExecutionRoleArn = role,\n",
    "    Containers = [container_param])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Arn: arn:aws:sagemaker:us-east-2:868024899531:model/demo-multimodelmodel-2020-02-09-18-11-42\n"
     ]
    }
   ],
   "source": [
    "print(\"Model Arn: \" + create_model_response['ModelArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create endpoint configuration\n",
    "Endpoint config creation works the same way it does as single model endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint config name: DEMO-MultiModelEndpointConfig-2020-02-09-18-14-02\n"
     ]
    }
   ],
   "source": [
    "endpoint_config_name = 'DEMO-MultiModelEndpointConfig-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print('Endpoint config name: ' + endpoint_config_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint config Arn: arn:aws:sagemaker:us-east-2:868024899531:endpoint-config/demo-multimodelendpointconfig-2020-02-09-18-14-02\n"
     ]
    }
   ],
   "source": [
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType': 'ml.m5.xlarge',\n",
    "        'InitialInstanceCount': 2,\n",
    "        'InitialVariantWeight': 1,\n",
    "        'ModelName': model_name,\n",
    "        'VariantName': 'AllTraffic'}])\n",
    "\n",
    "print(\"Endpoint config Arn: \" + create_endpoint_config_response['EndpointConfigArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create endpoint\n",
    "Similarly, endpoint creation works the same way as for single model endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint name: DEMO-MultiModelEndpoint-2020-02-09-18-16-40\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "endpoint_name = 'DEMO-MultiModelEndpoint-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print('Endpoint name: ' + endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Arn: arn:aws:sagemaker:us-east-2:868024899531:endpoint/demo-multimodelendpoint-2020-02-09-18-16-40\n"
     ]
    }
   ],
   "source": [
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name)\n",
    "print('Endpoint Arn: ' + create_endpoint_response['EndpointArn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Status: Creating\n"
     ]
    }
   ],
   "source": [
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp['EndpointStatus']\n",
    "print(\"Endpoint Status: \" + status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for DEMO-MultiModelEndpoint-2020-02-09-18-16-40 endpoint to be in service...\n"
     ]
    }
   ],
   "source": [
    "print('Waiting for {} endpoint to be in service...'.format(endpoint_name))\n",
    "waiter = sm_client.get_waiter('endpoint_in_service')\n",
    "waiter.wait(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke models\n",
    "Now we invoke the models that we uploaded to S3 previously. The first invocation of a model may be slow, since behind the scenes, SageMaker is downloading the model artifacts from S3 to the instance and loading it into the container.\n",
    "\n",
    "First we will download an image of a cat as the payload to invoke the model, then call InvokeEndpoint to invoke the ResNet 18 model. The `TargetModel` field is concatenated with the S3 prefix specified in `ModelDataUrl` when creating the model, to generate the location of the model in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = mx.test_utils.download('https://github.com/dmlc/web-data/blob/master/mxnet/doc/tutorials/python/predict_image/cat.jpg?raw=true', 'cat.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cat.jpg'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(fname, 'rb') as f:\n",
    "    payload = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability=0.244390, class=n02119022 red fox, Vulpes vulpes\n",
      "probability=0.170341, class=n02119789 kit fox, Vulpes macrotis\n",
      "probability=0.145019, class=n02113023 Pembroke, Pembroke Welsh corgi\n",
      "probability=0.059833, class=n02356798 fox squirrel, eastern fox squirrel, Sciurus niger\n",
      "probability=0.051555, class=n02123159 tiger cat\n",
      "CPU times: user 13.1 ms, sys: 1.73 ms, total: 14.8 ms\n",
      "Wall time: 4.89 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import json\n",
    "\n",
    "response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType='application/x-image',\n",
    "    TargetModel='resnet_18.tar.gz', # this is the rest of the S3 path where the model artifacts are located\n",
    "    Body=payload)\n",
    "\n",
    "print(*json.loads(response['Body'].read()), sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we invoke the same ResNet 18 model a 2nd time, it is already downloaded to the instance and loaded in the container, so inference is faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability=0.244390, class=n02119022 red fox, Vulpes vulpes\n",
      "probability=0.170341, class=n02119789 kit fox, Vulpes macrotis\n",
      "probability=0.145019, class=n02113023 Pembroke, Pembroke Welsh corgi\n",
      "probability=0.059833, class=n02356798 fox squirrel, eastern fox squirrel, Sciurus niger\n",
      "probability=0.051555, class=n02123159 tiger cat\n",
      "CPU times: user 4.12 ms, sys: 2.18 ms, total: 6.3 ms\n",
      "Wall time: 2.44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType='application/x-image',\n",
    "    TargetModel='resnet_18.tar.gz',\n",
    "    Body=payload)\n",
    "\n",
    "print(*json.loads(response['Body'].read()), sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '08ad9834-f417-43fa-b97a-9bb3cf22f7df',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '08ad9834-f417-43fa-b97a-9bb3cf22f7df',\n",
       "   'x-amzn-invoked-production-variant': 'AllTraffic',\n",
       "   'date': 'Sun, 9 Feb 2020 18:27:07 GMT',\n",
       "   'content-type': 'application/json',\n",
       "   'content-length': '356'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ContentType': 'application/json',\n",
       " 'InvokedProductionVariant': 'AllTraffic',\n",
       " 'Body': <botocore.response.StreamingBody at 0x7f94e53e9ac8>}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke another model\n",
    "Exercising the power of a multi-model endpoint, we can specify a different model (resnet_152.tar.gz) as `TargetModel` and perform inference on it using the same endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability=0.386026, class=n02119022 red fox, Vulpes vulpes\n",
      "probability=0.300927, class=n02119789 kit fox, Vulpes macrotis\n",
      "probability=0.029575, class=n02123045 tabby, tabby cat\n",
      "probability=0.026005, class=n02123159 tiger cat\n",
      "probability=0.023201, class=n02113023 Pembroke, Pembroke Welsh corgi\n",
      "CPU times: user 4.9 ms, sys: 1.73 ms, total: 6.63 ms\n",
      "Wall time: 8.71 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType='application/x-image',\n",
    "    TargetModel='resnet_152.tar.gz',\n",
    "    Body=payload)\n",
    "\n",
    "print(*json.loads(response['Body'].read()), sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add models to the endpoint\n",
    "We can add more models to the endpoint without having to update the endpoint. Below we are adding a 3rd model, `squeezenet_v1.0`. To demonstrate hosting multiple models behind the endpoint, this model is duplicated 10 times with a slightly different name in S3. In a more realistic scenario, these could be 10 new different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx.test_utils.download(model_path+'squeezenet/squeezenet_v1.0-0000.params', None, 'data/squeezenet_v1.0')\n",
    "mx.test_utils.download(model_path+'squeezenet/squeezenet_v1.0-symbol.json', None, 'data/squeezenet_v1.0')\n",
    "mx.test_utils.download(model_path+'synset.txt', None, 'data/squeezenet_v1.0')\n",
    "\n",
    "with open('data/squeezenet_v1.0/squeezenet_v1.0-shapes.json', 'w') as file:\n",
    "    file.write('[{\"shape\": [1, 3, 224, 224], \"name\": \"data\"}]')\n",
    "    \n",
    "with tarfile.open('data/squeezenet_v1.0.tar.gz', 'w:gz') as tar:\n",
    "    tar.add('data/squeezenet_v1.0', arcname='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'resnet_152.tar.gz', 'resnet_18.tar.gz'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of models: 12\n",
      "Models: {'demo-subfolder/squeezenet_v1.0_3.tar.gz', 'demo-subfolder/squeezenet_v1.0_5.tar.gz', 'demo-subfolder/squeezenet_v1.0_8.tar.gz', 'demo-subfolder/squeezenet_v1.0_2.tar.gz', 'demo-subfolder/squeezenet_v1.0_7.tar.gz', 'resnet_152.tar.gz', 'resnet_18.tar.gz', 'demo-subfolder/squeezenet_v1.0_6.tar.gz', 'demo-subfolder/squeezenet_v1.0_9.tar.gz', 'demo-subfolder/squeezenet_v1.0_0.tar.gz', 'demo-subfolder/squeezenet_v1.0_1.tar.gz', 'demo-subfolder/squeezenet_v1.0_4.tar.gz'}\n"
     ]
    }
   ],
   "source": [
    "file = 'data/squeezenet_v1.0.tar.gz'\n",
    "\n",
    "for x in range(0, 10):\n",
    "    s3_file_name = 'demo-subfolder/squeezenet_v1.0_{}.tar.gz'.format(x)\n",
    "    key = os.path.join(prefix, s3_file_name)\n",
    "    with open(file, 'rb') as file_obj:\n",
    "        s3.Bucket(bucket).Object(key).upload_fileobj(file_obj)\n",
    "    models.add(s3_file_name)\n",
    "\n",
    "print('Number of models: {}'.format(len(models)))\n",
    "print('Models: {}'.format(models))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After uploading the SqueezeNet models to S3, we will invoke the endpoint 100 times, randomly choosing from one of the 12 models behind the S3 prefix for each invocation, and keeping a count of the label with the highest probability on each invoke response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7 µs, sys: 2 µs, total: 9 µs\n",
      "Wall time: 12.6 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "results = defaultdict(int)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('probability=0.294885, class=n02326432 hare', 247)\n",
      "('probability=0.244390, class=n02119022 red fox, Vulpes vulpes', 30)\n",
      "('probability=0.386026, class=n02119022 red fox, Vulpes vulpes', 23)\n"
     ]
    }
   ],
   "source": [
    "for x in range(0, 100):\n",
    "    target_model = random.choice(tuple(models))\n",
    "    response = runtime_sm_client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        ContentType='application/x-image',\n",
    "        TargetModel=target_model,\n",
    "        Body=payload)\n",
    "\n",
    "    results[json.loads(response['Body'].read())[0]] += 1\n",
    "    \n",
    "print(*results.items(), sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating a model\n",
    "To update a model, you would follow the same approach as above and add it as a new model. For example, if you have retrained the `resnet_18.tar.gz` model and wanted to start invoking it, you would upload the updated model artifacts behind the S3 prefix with a new name such as `resnet_18_v2.tar.gz`, and then change the `TargetModel` field to invoke `resnet_18_v2.tar.gz` instead of `resnet_18.tar.gz`. You do not want to overwrite the model artifacts in Amazon S3, because the old version of the model might still be loaded in the containers or on the storage volume of the instances on the endpoint. Invocations to the new model could then invoke the old version of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Delete the hosting resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'b2ffb3e9-ec53-4d0e-8326-3f8469c5dd95',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'b2ffb3e9-ec53-4d0e-8326-3f8469c5dd95',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Sun, 09 Feb 2020 18:48:26 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "# sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "# sm_client.delete_model(ModelName=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
