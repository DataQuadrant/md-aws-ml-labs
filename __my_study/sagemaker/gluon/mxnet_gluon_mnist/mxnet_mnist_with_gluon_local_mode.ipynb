{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local MNIST Training with MXNet and Gluon\n",
    "\n",
    "### Pre-requisites\n",
    "\n",
    "This notebook shows how to use the SageMaker Python SDK to run your code in a local container before deploying to SageMaker's managed training or hosting environments.  This can speed up iterative testing and debugging while using the same familiar Python SDK interface.  Just change your estimator's `train_instance_type` to `local`.  You could also use `local_gpu` if you're using an ml.p2 or ml.p3 notebook instance, but then you'll need to set `train_instance_count=1` since distributed, local, GPU training is not yet supported.\n",
    "\n",
    "In order to use this feature you'll need to install docker-compose (and nvidia-docker if training with a GPU).  Running the setup.sh script below will handle this for you.\n",
    "\n",
    "**Note, you can only run a single local notebook at one time.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "! {sys.prefix}/bin/pip install mxnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/bin/bash /home/ec2-user/SageMaker/AWS-ML-Certification/__my_study/sagemaker/gluon/mxnet_gluon_mnist/setup.sh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! echo $PWD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "MNIST is a widely used dataset for handwritten digit classification. It consists of 70,000 labeled 28x28 pixel grayscale images of hand-written digits. The dataset is split into 60,000 training images and 10,000 test images. There are 10 classes (one for each of the 10 digits). This tutorial will show how to train and test an MNIST model on SageMaker local mode using MXNet and the Gluon API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Instance type = local\n"
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.mxnet import MXNet\n",
    "from mxnet import gluon\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "instance_type = 'local'\n",
    "\n",
    "# if subprocess.call('nvidia-smi') == 0:\n",
    "#     ## Set type to GPU if one is present\n",
    "#     instance_type = 'local_gpu'\n",
    "    \n",
    "print(\"Instance type = \" + instance_type)\n",
    "\n",
    "# role = get_execution_role()\n",
    "role = 'arn:aws:iam::868024899531:role/service-role/AmazonSageMaker-ExecutionRole-20200530T112594'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Downloading ./data/train/train-images-idx3-ubyte.gz from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/mnist/train-images-idx3-ubyte.gz...\nDownloading ./data/train/train-labels-idx1-ubyte.gz from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/mnist/train-labels-idx1-ubyte.gz...\nDownloading ./data/test/t10k-images-idx3-ubyte.gz from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/mnist/t10k-images-idx3-ubyte.gz...\nDownloading ./data/test/t10k-labels-idx1-ubyte.gz from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/mnist/t10k-labels-idx1-ubyte.gz...\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<mxnet.gluon.data.vision.datasets.MNIST at 0x7fc308d63b38>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "gluon.data.vision.MNIST('./data/train', train=True)\n",
    "gluon.data.vision.MNIST('./data/test', train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading the data\n",
    "\n",
    "We use the `sagemaker.Session.upload_data` function to upload our datasets to an S3 location. The return value `inputs` identifies the location -- we will use this later when we start the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "s3://md-labs-bucket/md-labs-mxnet-gluon/data/mnist\n"
    }
   ],
   "source": [
    "bucket = 'md-labs-bucket'\n",
    "prefix = 'md-labs-mxnet-gluon/data/mnist'\n",
    "\n",
    "inputs = sagemaker_session.upload_data(bucket=bucket, path='data', key_prefix=prefix)\n",
    "print(inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the training function\n",
    "\n",
    "We need to provide a training script that can run on the SageMaker platform. The training scripts are essentially the same as one you would write for local training, except that you need to provide a `train` function. The `train` function will check for the validation accuracy at the end of every epoch and checkpoints the best model so far, along with the optimizer state, in the folder `/opt/ml/checkpoints` if the folder path exists, else it will skip the checkpointing. When SageMaker calls your function, it will pass in arguments that describe the training environment. Check the script below to see how this works.\n",
    "\n",
    "The script here is an adaptation of the [Gluon MNIST example](https://github.com/apache/incubator-mxnet/blob/master/example/gluon/mnist.py) provided by the [Apache MXNet](https://mxnet.incubator.apache.org/) project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36m__future__\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m print_function\n\n\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\n\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n\n\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mmxnet\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mmx\u001b[39;49;00m\n\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mmxnet\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m gluon, autograd\n\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mmxnet\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mgluon\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m nn\n\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m\n\n\nlogging.basicConfig(level=logging.DEBUG)\n\n\u001b[37m# ------------------------------------------------------------ #\u001b[39;49;00m\n\u001b[37m# Training methods                                             #\u001b[39;49;00m\n\u001b[37m# ------------------------------------------------------------ #\u001b[39;49;00m\n\n\n\u001b[34mdef\u001b[39;49;00m \u001b[32mtrain\u001b[39;49;00m(args):\n    \u001b[37m# SageMaker passes num_cpus, num_gpus and other args we can use to tailor training to\u001b[39;49;00m\n    \u001b[37m# the current container environment, but here we just use simple cpu context.\u001b[39;49;00m\n    ctx = mx.cpu()\n\n    \u001b[37m# retrieve the hyperparameters we set in notebook (with some defaults)\u001b[39;49;00m\n    batch_size = args.batch_size\n    epochs = args.epochs\n    learning_rate = args.learning_rate\n    momentum = args.momentum\n    log_interval = args.log_interval\n\n    num_gpus = \u001b[36mint\u001b[39;49;00m(os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_NUM_GPUS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n    current_host = args.current_host\n    hosts = args.hosts\n    model_dir = args.model_dir\n    CHECKPOINTS_DIR = \u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/checkpoints\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n    checkpoints_enabled = os.path.exists(CHECKPOINTS_DIR)\n\n    \u001b[37m# load training and validation data\u001b[39;49;00m\n    \u001b[37m# we use the gluon.data.vision.MNIST class because of its built in mnist pre-processing logic,\u001b[39;49;00m\n    \u001b[37m# but point it at the location where SageMaker placed the data files, so it doesn't download them again.\u001b[39;49;00m\n    training_dir = args.train\n    train_data = get_train_data(training_dir + \u001b[33m'\u001b[39;49;00m\u001b[33m/train\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, batch_size)\n    val_data = get_val_data(training_dir + \u001b[33m'\u001b[39;49;00m\u001b[33m/test\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, batch_size)\n\n    \u001b[37m# define the network\u001b[39;49;00m\n    net = define_network()\n\n    \u001b[37m# Collect all parameters from net and its children, then initialize them.\u001b[39;49;00m\n    net.initialize(mx.init.Xavier(magnitude=\u001b[34m2.24\u001b[39;49;00m), ctx=ctx)\n    \u001b[37m# Trainer is for updating parameters with gradient.\u001b[39;49;00m\n\n    \u001b[34mif\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(hosts) == \u001b[34m1\u001b[39;49;00m:\n        kvstore = \u001b[33m'\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m num_gpus > \u001b[34m0\u001b[39;49;00m \u001b[34melse\u001b[39;49;00m \u001b[33m'\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n    \u001b[34melse\u001b[39;49;00m:\n        kvstore = \u001b[33m'\u001b[39;49;00m\u001b[33mdist_device_sync\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m num_gpus > \u001b[34m0\u001b[39;49;00m \u001b[34melse\u001b[39;49;00m \u001b[33m'\u001b[39;49;00m\u001b[33mdist_sync\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n\n    trainer = gluon.Trainer(net.collect_params(), \u001b[33m'\u001b[39;49;00m\u001b[33msgd\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n                            {\u001b[33m'\u001b[39;49;00m\u001b[33mlearning_rate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: learning_rate, \u001b[33m'\u001b[39;49;00m\u001b[33mmomentum\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: momentum},\n                            kvstore=kvstore)\n    metric = mx.metric.Accuracy()\n    loss = gluon.loss.SoftmaxCrossEntropyLoss()\n\n    \u001b[37m# shard the training data in case we are doing distributed training. Alternatively to splitting in memory,\u001b[39;49;00m\n    \u001b[37m# the data could be pre-split in S3 and use ShardedByS3Key to do distributed training.\u001b[39;49;00m\n    \u001b[34mif\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(hosts) > \u001b[34m1\u001b[39;49;00m:\n        train_data = [x \u001b[34mfor\u001b[39;49;00m x \u001b[35min\u001b[39;49;00m train_data]\n        shard_size = \u001b[36mlen\u001b[39;49;00m(train_data) // \u001b[36mlen\u001b[39;49;00m(hosts)\n        \u001b[34mfor\u001b[39;49;00m i, host \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(hosts):\n            \u001b[34mif\u001b[39;49;00m host == current_host:\n                start = shard_size * i\n                end = start + shard_size\n                \u001b[34mbreak\u001b[39;49;00m\n\n        train_data = train_data[start:end]\n\n    net.hybridize()\n\n    best_val_score = \u001b[34m0.0\u001b[39;49;00m\n    \u001b[34mfor\u001b[39;49;00m epoch \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(epochs):\n        \u001b[37m# reset data iterator and metric at begining of epoch.\u001b[39;49;00m\n        metric.reset()\n        btic = time.time()\n        \u001b[34mfor\u001b[39;49;00m i, (data, label) \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(train_data):\n            \u001b[37m# Copy data to ctx if necessary\u001b[39;49;00m\n            data = data.as_in_context(ctx)\n            label = label.as_in_context(ctx)\n            \u001b[37m# Start recording computation graph with record() section.\u001b[39;49;00m\n            \u001b[37m# Recorded graphs can then be differentiated with backward.\u001b[39;49;00m\n            \u001b[34mwith\u001b[39;49;00m autograd.record():\n                output = net(data)\n                L = loss(output, label)\n                L.backward()\n            \u001b[37m# take a gradient step with batch_size equal to data.shape[0]\u001b[39;49;00m\n            trainer.step(data.shape[\u001b[34m0\u001b[39;49;00m])\n            \u001b[37m# update metric at last.\u001b[39;49;00m\n            metric.update([label], [output])\n\n            \u001b[34mif\u001b[39;49;00m i % log_interval == \u001b[34m0\u001b[39;49;00m \u001b[35mand\u001b[39;49;00m i > \u001b[34m0\u001b[39;49;00m:\n                name, acc = metric.get()\n                \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m[Epoch \u001b[39;49;00m\u001b[33m%d\u001b[39;49;00m\u001b[33m Batch \u001b[39;49;00m\u001b[33m%d\u001b[39;49;00m\u001b[33m] Training: \u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m=\u001b[39;49;00m\u001b[33m%f\u001b[39;49;00m\u001b[33m, \u001b[39;49;00m\u001b[33m%f\u001b[39;49;00m\u001b[33m samples/s\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m %\n                      (epoch, i, name, acc, batch_size / (time.time() - btic)))\n\n            btic = time.time()\n\n        name, acc = metric.get()\n        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m[Epoch \u001b[39;49;00m\u001b[33m%d\u001b[39;49;00m\u001b[33m] Training: \u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m=\u001b[39;49;00m\u001b[33m%f\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m % (epoch, name, acc))\n\n        name, val_acc = test(ctx, net, val_data)\n        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m[Epoch \u001b[39;49;00m\u001b[33m%d\u001b[39;49;00m\u001b[33m] Validation: \u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m=\u001b[39;49;00m\u001b[33m%f\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m % (epoch, name, val_acc))\n        \u001b[37m# checkpoint the model, params and optimizer states in the folder /opt/ml/checkpoints\u001b[39;49;00m\n        \u001b[34mif\u001b[39;49;00m checkpoints_enabled \u001b[35mand\u001b[39;49;00m val_acc > best_val_score:\n            best_val_score = val_acc\n            logging.info(\u001b[33m'\u001b[39;49;00m\u001b[33mSaving the model, params and optimizer state.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n            net.export(CHECKPOINTS_DIR + \u001b[33m\"\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m%.4f\u001b[39;49;00m\u001b[33m-gluon_mnist\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m%(best_val_score), epoch)\n            trainer.save_states(CHECKPOINTS_DIR + \u001b[33m'\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m%.4f\u001b[39;49;00m\u001b[33m-gluon_mnist-\u001b[39;49;00m\u001b[33m%d\u001b[39;49;00m\u001b[33m.states\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m%(best_val_score, epoch))\n\n    \u001b[34mif\u001b[39;49;00m current_host == hosts[\u001b[34m0\u001b[39;49;00m]:\n        save(net, model_dir)\n\n\n\u001b[34mdef\u001b[39;49;00m \u001b[32msave\u001b[39;49;00m(net, model_dir):\n    \u001b[37m# save the model\u001b[39;49;00m\n    net.export(\u001b[33m'\u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m/model\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m% model_dir)\n\n\n\u001b[34mdef\u001b[39;49;00m \u001b[32mdefine_network\u001b[39;49;00m():\n    net = nn.HybridSequential()\n    \u001b[34mwith\u001b[39;49;00m net.name_scope():\n        net.add(nn.Dense(\u001b[34m128\u001b[39;49;00m, activation=\u001b[33m'\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n        net.add(nn.Dense(\u001b[34m64\u001b[39;49;00m, activation=\u001b[33m'\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n        net.add(nn.Dense(\u001b[34m10\u001b[39;49;00m))\n    \u001b[34mreturn\u001b[39;49;00m net\n\n\n\u001b[34mdef\u001b[39;49;00m \u001b[32minput_transformer\u001b[39;49;00m(data, label):\n    data = data.reshape((-\u001b[34m1\u001b[39;49;00m,)).astype(np.float32) / \u001b[34m255.\u001b[39;49;00m\n    \u001b[34mreturn\u001b[39;49;00m data, label\n\n\n\u001b[34mdef\u001b[39;49;00m \u001b[32mget_train_data\u001b[39;49;00m(data_dir, batch_size):\n    \u001b[34mreturn\u001b[39;49;00m gluon.data.DataLoader(\n        gluon.data.vision.MNIST(data_dir, train=\u001b[34mTrue\u001b[39;49;00m, transform=input_transformer),\n        batch_size=batch_size, shuffle=\u001b[34mTrue\u001b[39;49;00m, last_batch=\u001b[33m'\u001b[39;49;00m\u001b[33mrollover\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n\n\n\u001b[34mdef\u001b[39;49;00m \u001b[32mget_val_data\u001b[39;49;00m(data_dir, batch_size):\n    \u001b[34mreturn\u001b[39;49;00m gluon.data.DataLoader(\n        gluon.data.vision.MNIST(data_dir, train=\u001b[34mFalse\u001b[39;49;00m, transform=input_transformer),\n        batch_size=batch_size, shuffle=\u001b[34mFalse\u001b[39;49;00m)\n\n\n\u001b[34mdef\u001b[39;49;00m \u001b[32mtest\u001b[39;49;00m(ctx, net, val_data):\n    metric = mx.metric.Accuracy()\n    \u001b[34mfor\u001b[39;49;00m data, label \u001b[35min\u001b[39;49;00m val_data:\n        data = data.as_in_context(ctx)\n        label = label.as_in_context(ctx)\n        output = net(data)\n        metric.update([label], [output])\n    \u001b[34mreturn\u001b[39;49;00m metric.get()\n\n\n\u001b[37m# ------------------------------------------------------------ #\u001b[39;49;00m\n\u001b[37m# Hosting methods                                              #\u001b[39;49;00m\n\u001b[37m# ------------------------------------------------------------ #\u001b[39;49;00m\n\n\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_dir):\n    \u001b[33m\"\"\"\u001b[39;49;00m\n\u001b[33m    Load the gluon model. Called once when hosting service starts.\u001b[39;49;00m\n\u001b[33m\u001b[39;49;00m\n\u001b[33m    :param: model_dir The directory where model files are stored.\u001b[39;49;00m\n\u001b[33m    :return: a model (in this case a Gluon network)\u001b[39;49;00m\n\u001b[33m    \"\"\"\u001b[39;49;00m\n    net = gluon.SymbolBlock.imports(\n        \u001b[33m'\u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m/model-symbol.json\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m % model_dir,\n        [\u001b[33m'\u001b[39;49;00m\u001b[33mdata\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m],\n        \u001b[33m'\u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m/model-0000.params\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m % model_dir,\n    )\n    \u001b[34mreturn\u001b[39;49;00m net\n\n\n\u001b[34mdef\u001b[39;49;00m \u001b[32mtransform_fn\u001b[39;49;00m(net, data, input_content_type, output_content_type):\n    \u001b[33m\"\"\"\u001b[39;49;00m\n\u001b[33m    Transform a request using the Gluon model. Called once per request.\u001b[39;49;00m\n\u001b[33m\u001b[39;49;00m\n\u001b[33m    :param net: The Gluon model.\u001b[39;49;00m\n\u001b[33m    :param data: The request payload.\u001b[39;49;00m\n\u001b[33m    :param input_content_type: The request content type.\u001b[39;49;00m\n\u001b[33m    :param output_content_type: The (desired) response content type.\u001b[39;49;00m\n\u001b[33m    :return: response payload and content type.\u001b[39;49;00m\n\u001b[33m    \"\"\"\u001b[39;49;00m\n    \u001b[37m# we can use content types to vary input/output handling, but\u001b[39;49;00m\n    \u001b[37m# here we just assume json for both\u001b[39;49;00m\n    parsed = json.loads(data)\n    nda = mx.nd.array(parsed)\n    output = net(nda)\n    prediction = mx.nd.argmax(output, axis=\u001b[34m1\u001b[39;49;00m)\n    response_body = json.dumps(prediction.asnumpy().tolist()[\u001b[34m0\u001b[39;49;00m])\n    \u001b[34mreturn\u001b[39;49;00m response_body, output_content_type\n\n\n\u001b[37m# ------------------------------------------------------------ #\u001b[39;49;00m\n\u001b[37m# Training execution                                           #\u001b[39;49;00m\n\u001b[37m# ------------------------------------------------------------ #\u001b[39;49;00m\n\n\u001b[34mdef\u001b[39;49;00m \u001b[32mparse_args\u001b[39;49;00m():\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--batch-size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m100\u001b[39;49;00m)\n    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m10\u001b[39;49;00m)\n    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--learning-rate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.1\u001b[39;49;00m)\n    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--momentum\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.9\u001b[39;49;00m)\n    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--log-interval\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m100\u001b[39;49;00m)\n\n    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAINING\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n\n    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--current-host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CURRENT_HOST\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mlist\u001b[39;49;00m, default=json.loads(os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_HOSTS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]))\n\n    \u001b[34mreturn\u001b[39;49;00m parser.parse_args()\n\n\n\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n    args = parse_args()\n\n    train(args)\n"
    }
   ],
   "source": [
    "# !cat 'mnist.py'\n",
    "!pygmentize '/home/ec2-user/SageMaker/AWS-ML-Certification/__my_study/sagemaker/gluon/mxnet_gluon_mnist/mnist.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the training script on SageMaker\n",
    "\n",
    "The ```MXNet``` class allows us to run our training function on SageMaker local mode. We need to configure it with our training script, an IAM role, the number of training instances, and the training instance type.  This is the the only difference from [mnist_with_gluon.ipynb](./mnist_with_gluon.ipynb).  Instead of ``train_instance_type='ml.c4.xlarge'``, we set it to ``train_instance_type='local'``.  For local training with GPU, we could set this to \"local_gpu\".  In this case, `instance_type` was set above based on your whether you're running a GPU instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MXNet(\"/home/ec2-user/SageMaker/AWS-ML-Certification/__my_study/sagemaker/gluon/mxnet_gluon_mnist/mnist.py\",\n",
    "          role=role,\n",
    "          train_instance_count=1,\n",
    "          train_instance_type=instance_type,\n",
    "          framework_version=\"1.6.0\",\n",
    "          py_version=\"py3\",\n",
    "          hyperparameters={'batch-size': 100,\n",
    "                           'epochs': 20,\n",
    "                           'learning-rate': 0.1,\n",
    "                           'momentum': 0.9,\n",
    "                           'log-interval': 100})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we've constructed our `MXNet` object, we fit it using the data we uploaded to S3. Even though we're in local mode, using S3 as our data source makes sense because it maintains consistency with how SageMaker's distributed, managed training ingests data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using the short-lived AWS credentials found in session. They might expire while running.\nCommand output: \n"
    },
    {
     "output_type": "error",
     "ename": "Exception",
     "evalue": "Failed to run docker,images,-q,763104351884.dkr.ecr.us-east-2.amazonaws.com/mxnet-training:1.6.0-cpu-py3",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mException\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-b1d0de43edea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_for_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_TrainingJob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mstart_new\u001b[0;34m(cls, estimator, inputs, experiment_config)\u001b[0m\n\u001b[1;32m   1012\u001b[0m             \u001b[0mtrain_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"enable_sagemaker_metrics\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_sagemaker_metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_job_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_mode, input_config, role, job_name, output_config, resource_config, vpc_config, hyperparameters, stop_condition, tags, metric_definitions, enable_network_isolation, image, algorithm_arn, encrypt_inter_container_traffic, train_use_spot_instances, checkpoint_s3_uri, checkpoint_local_path, experiment_config, debugger_rule_configs, debugger_hook_config, tensorboard_output_config, enable_sagemaker_metrics)\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Creating training-job with name: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train request: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_training_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_request\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     def process(\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sagemaker/local/local_session.py\u001b[0m in \u001b[0;36mcreate_training_job\u001b[0;34m(self, TrainingJobName, AlgorithmSpecification, OutputDataConfig, ResourceConfig, InputDataConfig, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mtraining_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_LocalTrainingJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mhyperparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"HyperParameters\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"HyperParameters\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mtraining_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInputDataConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOutputDataConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingJobName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mLocalSagemakerClient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_training_jobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTrainingJobName\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_job\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sagemaker/local/entities.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, input_data_config, output_data_config, hyperparameters, job_name)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         self.model_artifacts = self.container.train(\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0minput_data_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_data_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         )\n\u001b[1;32m     98\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sagemaker/local/image.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_data_config, output_data_config, hyperparameters, job_name)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mcompose_command\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0m_ecr_login_if_needed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboto_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0m_pull_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sagemaker/local/image.py\u001b[0m in \u001b[0;36m_ecr_login_if_needed\u001b[0;34m(boto_session, image)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m     \u001b[0;31m# do we have the image?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0m_check_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"docker images -q %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sagemaker/local/image.py\u001b[0m in \u001b[0;36m_check_output\u001b[0;34m(cmd, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    696\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Command output: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Failed to run %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m\",\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Failed to run docker,images,-q,763104351884.dkr.ecr.us-east-2.amazonaws.com/mxnet-training:1.6.0-cpu-py3"
     ]
    }
   ],
   "source": [
    "m.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, we use the MXNet object to deploy an MXNetPredictor object. This creates a SageMaker endpoint locally that we can use to perform inference. \n",
    "\n",
    "This allows us to perform inference on json encoded multi-dimensional arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictor = m.deploy(initial_instance_count=1, instance_type=instance_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use this predictor to classify hand-written digits. Drawing into the image box loads the pixel data into a 'data' variable in this notebook, which we can then pass to the mxnet predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(open(\"input.html\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictor runs inference on our input data and returns the predicted digit (as a float value, so we convert to int for display)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response = predictor.predict(data)\n",
    "print(int(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean-up\n",
    "\n",
    "Deleting the local endpoint when you're finished is important since you can only run one local endpoint at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5-final"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}