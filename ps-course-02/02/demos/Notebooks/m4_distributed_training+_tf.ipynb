{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed training  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages \n",
    "* os -  provides a portable way of using operating system dependent functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing standard python packages\n",
    "* utils - a collection of small Python functions and classes which make common patterns shorter and easier.\n",
    "* numpy - package for scientific computing with Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing tensorflow packages\n",
    "* tensorflow - library for dataflow programming across a range of tasks.\n",
    "* tensorflow.contrib.learn.python.learn.datasets - contains dataset utilities and synthetic/reference datasets, for getting the mnist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.contrib.learn.python.learn.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing amazon packages\n",
    "* sagemaker - Python SDK for training and deploying machine learning models on Amazon SageMaker.\n",
    "* get_execution_role - Return the role ARN whose credentials are used to call the API.\n",
    "* sagemaker.tensorflow - The Amazon SageMaker custom TensorFlow code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.tensorflow import TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting and preprocessing the dataset\n",
    "* Read the mnist dataset\n",
    "* split it into three : train, validation and test\n",
    "* instantiate a sagemaker session\n",
    "* upload our datasets to an S3 location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "data_sets = mnist.read_data_sets('data', dtype=tf.uint8, reshape=False, validation_size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Writing', 'data/train.tfrecords')\n",
      "('Writing', 'data/validation.tfrecords')\n",
      "('Writing', 'data/test.tfrecords')\n"
     ]
    }
   ],
   "source": [
    "utils.convert_to(data_sets.train, 'train', 'data')\n",
    "utils.convert_to(data_sets.validation, 'validation', 'data')\n",
    "utils.convert_to(data_sets.test, 'test', 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Created S3 bucket: sagemaker-us-east-2-324118574079\n"
     ]
    }
   ],
   "source": [
    "inputs = sagemaker_session.upload_data(path='data', key_prefix='data/mnist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "* You will need the training script predefined.\n",
    "* Define a tensorflow estimator object and pass in the python script as the entry point parameter.\n",
    "* Your TensorFlow training script must be a Python 2.7 source file. It should define the following methods.\n",
    "* **model_fn**: Defines the model that will be trained.\n",
    "* **train_input_fn**: Preprocess and load training data.\n",
    "* **eval_input_fn**: Preprocess and load evaluation data.\n",
    "* **serving_input_fn**: Defines the features to be passed to the model during prediction.\n",
    "* Get the role ARN whose credentials are used to call the API.\n",
    "* To perform distributed training,the instance count is set to 2. \n",
    "* Invoke the fit method to train the model. The fit method will create a training job in two ml.c4.xlarge instances. The logs will show the instances doing training, evaluation, and incrementing the number of training steps.\n",
    "* Invoke the deploy method to create an endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import os\r\n",
      "import tensorflow as tf\r\n",
      "from tensorflow.python.estimator.model_fn import ModeKeys as Modes\r\n",
      "\r\n",
      "INPUT_TENSOR_NAME = 'inputs'\r\n",
      "SIGNATURE_NAME = 'predictions'\r\n",
      "\r\n",
      "LEARNING_RATE = 0.001\r\n",
      "\r\n",
      "\r\n",
      "def model_fn(features, labels, mode, params):\r\n",
      "    # Input Layer\r\n",
      "    input_layer = tf.reshape(features[INPUT_TENSOR_NAME], [-1, 28, 28, 1])\r\n",
      "\r\n",
      "    # Convolutional Layer #1\r\n",
      "    conv1 = tf.layers.conv2d(\r\n",
      "        inputs=input_layer,\r\n",
      "        filters=32,\r\n",
      "        kernel_size=[5, 5],\r\n",
      "        padding='same',\r\n",
      "        activation=tf.nn.relu)\r\n",
      "\r\n",
      "    # Pooling Layer #1\r\n",
      "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\r\n",
      "\r\n",
      "    # Convolutional Layer #2 and Pooling Layer #2\r\n",
      "    conv2 = tf.layers.conv2d(\r\n",
      "        inputs=pool1,\r\n",
      "        filters=64,\r\n",
      "        kernel_size=[5, 5],\r\n",
      "        padding='same',\r\n",
      "        activation=tf.nn.relu)\r\n",
      "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\r\n",
      "\r\n",
      "    # Dense Layer\r\n",
      "    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\r\n",
      "    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\r\n",
      "    dropout = tf.layers.dropout(\r\n",
      "        inputs=dense, rate=0.4, training=(mode == Modes.TRAIN))\r\n",
      "\r\n",
      "    # Logits Layer\r\n",
      "    logits = tf.layers.dense(inputs=dropout, units=10)\r\n",
      "\r\n",
      "    # Define operations\r\n",
      "    if mode in (Modes.PREDICT, Modes.EVAL):\r\n",
      "        predicted_indices = tf.argmax(input=logits, axis=1)\r\n",
      "        probabilities = tf.nn.softmax(logits, name='softmax_tensor')\r\n",
      "\r\n",
      "    if mode in (Modes.TRAIN, Modes.EVAL):\r\n",
      "        global_step = tf.train.get_or_create_global_step()\r\n",
      "        label_indices = tf.cast(labels, tf.int32)\r\n",
      "        loss = tf.losses.softmax_cross_entropy(\r\n",
      "            onehot_labels=tf.one_hot(label_indices, depth=10), logits=logits)\r\n",
      "        tf.summary.scalar('OptimizeLoss', loss)\r\n",
      "\r\n",
      "    if mode == Modes.PREDICT:\r\n",
      "        predictions = {\r\n",
      "            'classes': predicted_indices,\r\n",
      "            'probabilities': probabilities\r\n",
      "        }\r\n",
      "        export_outputs = {\r\n",
      "            SIGNATURE_NAME: tf.estimator.export.PredictOutput(predictions)\r\n",
      "        }\r\n",
      "        return tf.estimator.EstimatorSpec(\r\n",
      "            mode, predictions=predictions, export_outputs=export_outputs)\r\n",
      "\r\n",
      "    if mode == Modes.TRAIN:\r\n",
      "        optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\r\n",
      "        train_op = optimizer.minimize(loss, global_step=global_step)\r\n",
      "        return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\r\n",
      "\r\n",
      "    if mode == Modes.EVAL:\r\n",
      "        eval_metric_ops = {\r\n",
      "            'accuracy': tf.metrics.accuracy(label_indices, predicted_indices)\r\n",
      "        }\r\n",
      "        return tf.estimator.EstimatorSpec(\r\n",
      "            mode, loss=loss, eval_metric_ops=eval_metric_ops)\r\n",
      "\r\n",
      "\r\n",
      "def serving_input_fn(params):\r\n",
      "    inputs = {INPUT_TENSOR_NAME: tf.placeholder(tf.float32, [None, 784])}\r\n",
      "    return tf.estimator.export.ServingInputReceiver(inputs, inputs)\r\n",
      "\r\n",
      "\r\n",
      "def read_and_decode(filename_queue):\r\n",
      "    reader = tf.TFRecordReader()\r\n",
      "    _, serialized_example = reader.read(filename_queue)\r\n",
      "\r\n",
      "    features = tf.parse_single_example(\r\n",
      "        serialized_example,\r\n",
      "        features={\r\n",
      "            'image_raw': tf.FixedLenFeature([], tf.string),\r\n",
      "            'label': tf.FixedLenFeature([], tf.int64),\r\n",
      "        })\r\n",
      "\r\n",
      "    image = tf.decode_raw(features['image_raw'], tf.uint8)\r\n",
      "    image.set_shape([784])\r\n",
      "    image = tf.cast(image, tf.float32) * (1. / 255)\r\n",
      "    label = tf.cast(features['label'], tf.int32)\r\n",
      "\r\n",
      "    return image, label\r\n",
      "\r\n",
      "\r\n",
      "def train_input_fn(training_dir, params):\r\n",
      "    return _input_fn(training_dir, 'train.tfrecords', batch_size=100)\r\n",
      "\r\n",
      "\r\n",
      "def eval_input_fn(training_dir, params):\r\n",
      "    return _input_fn(training_dir, 'test.tfrecords', batch_size=100)\r\n",
      "\r\n",
      "\r\n",
      "def _input_fn(training_dir, training_filename, batch_size=100):\r\n",
      "    test_file = os.path.join(training_dir, training_filename)\r\n",
      "    filename_queue = tf.train.string_input_producer([test_file])\r\n",
      "\r\n",
      "    image, label = read_and_decode(filename_queue)\r\n",
      "    images, labels = tf.train.batch(\r\n",
      "        [image, label], batch_size=batch_size,\r\n",
      "        capacity=1000 + 3 * batch_size)\r\n",
      "\r\n",
      "    return {INPUT_TENSOR_NAME: images}, labels\r\n"
     ]
    }
   ],
   "source": [
    "!cat 'mnist.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:iam::324118574079:role/service-role/AmazonSageMaker-ExecutionRole-20180209T192191'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "role = get_execution_role()\n",
    "role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnist_estimator = TensorFlow(entry_point='mnist.py',\n",
    "                             role=role,\n",
    "                             training_steps=1000, \n",
    "                             evaluation_steps=100,\n",
    "                             train_instance_count=2,\n",
    "                             train_instance_type='ml.c4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sagemaker-tensorflow-py2-cpu-2018-03-10-12-19-50-341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...................................................................\n",
      "\u001b[32mexecuting startup script (first run)\u001b[0m\n",
      "\u001b[32m2018-03-10 12:25:18,122 INFO - root - running container entrypoint\u001b[0m\n",
      "\u001b[32m2018-03-10 12:25:18,122 INFO - root - starting train task\u001b[0m\n",
      "\u001b[31mexecuting startup script (first run)\u001b[0m\n",
      "\u001b[31m2018-03-10 12:25:16,712 INFO - root - running container entrypoint\u001b[0m\n",
      "\u001b[31m2018-03-10 12:25:16,713 INFO - root - starting train task\u001b[0m\n",
      "\u001b[31m2018-03-10 12:25:18,318 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTP connection (1): 169.254.170.2\u001b[0m\n",
      "\u001b[31m2018-03-10 12:25:19,219 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (1): s3.amazonaws.com\u001b[0m\n",
      "\u001b[31m2018-03-10 12:25:19,303 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (1): s3.us-east-2.amazonaws.com\u001b[0m\n",
      "\u001b[31m2018-03-10 12:25:19,391 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (1): s3.amazonaws.com\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:----------------------TF_CONFIG--------------------------\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:{\"environment\": \"cloud\", \"cluster\": {\"worker\": [\"algo-2:2222\"], \"ps\": [\"algo-1:2223\", \"algo-2:2223\"], \"master\": [\"algo-1:2222\"]}, \"task\": {\"index\": 0, \"type\": \"master\"}}\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:---------------------------------------------------------\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:going to training\u001b[0m\n",
      "\u001b[31m2018-03-10 12:25:19.469378: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\u001b[0m\n",
      "\u001b[31m2018-03-10 12:25:19,469 INFO - root - creating RunConfig:\u001b[0m\n",
      "\u001b[31m2018-03-10 12:25:19,469 INFO - root - {'save_checkpoints_secs': 300}\u001b[0m\n",
      "\u001b[31m2018-03-10 12:25:19,469 INFO - root - creating the estimator\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Using config: {'_model_dir': u's3://sagemaker-us-east-2-324118574079/sagemaker-tensorflow-py2-cpu-2018-03-10-12-19-50-341/checkpoints', '_save_checkpoints_secs': 300, '_num_ps_replicas': 2, '_keep_checkpoint_max': 5, '_session_config': None, '_tf_random_seed': None, '_task_type': u'master', '_environment': u'cloud', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f0ac8b30c50>, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\u001b[0m\n",
      "\u001b[31m}\u001b[0m\n",
      "\u001b[31m, '_num_worker_replicas': 2, '_task_id': 0, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': u'grpc://algo-1:2222', '_log_step_count_steps': 100}\u001b[0m\n",
      "\u001b[31m2018-03-10 12:25:19,470 INFO - root - creating Experiment:\u001b[0m\n",
      "\u001b[31m2018-03-10 12:25:19,470 INFO - root - {'min_eval_frequency': 1000}\u001b[0m\n",
      "\u001b[31mE0310 12:25:19.473097529      61 ev_epoll1_linux.c:1051]     grpc epoll fd: 5\u001b[0m\n",
      "\u001b[31m2018-03-10 12:25:19.480192: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job master -> {0 -> algo-1:2222}\u001b[0m\n",
      "\u001b[31m2018-03-10 12:25:19.480210: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:2223, 1 -> algo-2:2223}\u001b[0m\n",
      "\u001b[31m2018-03-10 12:25:19.480219: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> algo-2:2222}\u001b[0m\n",
      "\u001b[31m2018-03-10 12:25:19.480741: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:2223\u001b[0m\n",
      "\u001b[31mWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/monitors.py:267: __init__ (from tensorflow.contrib.learn.python.learn.monitors) is deprecated and will be removed after 2016-12-05.\u001b[0m\n",
      "\u001b[31mInstructions for updating:\u001b[0m\n",
      "\u001b[31mMonitors are deprecated. Please use tf.train.SessionRunHook.\u001b[0m\n",
      "\u001b[31m2018-03-10 12:25:19.561913: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job master -> {0 -> localhost:2222}\u001b[0m\n",
      "\u001b[31m2018-03-10 12:25:19.561941: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> algo-1:2223, 1 -> algo-2:2223}\u001b[0m\n",
      "\u001b[31m2018-03-10 12:25:19.561947: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> algo-2:2222}\u001b[0m\n",
      "\u001b[31m2018-03-10 12:25:19.562071: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:2222\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Create CheckpointSaverHook.\u001b[0m\n",
      "\u001b[32m2018-03-10 12:25:19,794 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTP connection (1): 169.254.170.2\u001b[0m\n",
      "\u001b[32m2018-03-10 12:25:20,702 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (1): s3.amazonaws.com\u001b[0m\n",
      "\u001b[32m2018-03-10 12:25:20,775 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (1): s3.us-east-2.amazonaws.com\u001b[0m\n",
      "\u001b[32m2018-03-10 12:25:20,875 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (1): s3.amazonaws.com\u001b[0m\n",
      "\u001b[32mINFO:tensorflow:----------------------TF_CONFIG--------------------------\u001b[0m\n",
      "\u001b[32mINFO:tensorflow:{\"environment\": \"cloud\", \"cluster\": {\"worker\": [\"algo-2:2222\"], \"ps\": [\"algo-1:2223\", \"algo-2:2223\"], \"master\": [\"algo-1:2222\"]}, \"task\": {\"index\": 0, \"type\": \"worker\"}}\u001b[0m\n",
      "\u001b[32mINFO:tensorflow:---------------------------------------------------------\u001b[0m\n",
      "\u001b[32mINFO:tensorflow:going to training\u001b[0m\n",
      "\u001b[32m2018-03-10 12:25:20,953 INFO - root - creating RunConfig:\u001b[0m\n",
      "\u001b[32m2018-03-10 12:25:20,954 INFO - root - {'save_checkpoints_secs': 300}\u001b[0m\n",
      "\u001b[32m2018-03-10 12:25:20.954235: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\u001b[0m\n",
      "\u001b[32m2018-03-10 12:25:20,954 INFO - root - creating the estimator\u001b[0m\n",
      "\u001b[32mINFO:tensorflow:Using config: {'_model_dir': u's3://sagemaker-us-east-2-324118574079/sagemaker-tensorflow-py2-cpu-2018-03-10-12-19-50-341/checkpoints', '_save_checkpoints_secs': 300, '_num_ps_replicas': 2, '_keep_checkpoint_max': 5, '_session_config': None, '_tf_random_seed': None, '_task_type': u'worker', '_environment': u'cloud', '_is_chief': False, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f0902d41c50>, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\u001b[0m\n",
      "\u001b[32m}\u001b[0m\n",
      "\u001b[32m, '_num_worker_replicas': 2, '_task_id': 0, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': u'grpc://algo-2:2222', '_log_step_count_steps': 100}\u001b[0m\n",
      "\u001b[32m2018-03-10 12:25:20,955 INFO - root - creating Experiment:\u001b[0m\n",
      "\u001b[32m2018-03-10 12:25:20,955 INFO - root - {'min_eval_frequency': 1000}\u001b[0m\n",
      "\u001b[32mE0310 12:25:20.957448179      60 ev_epoll1_linux.c:1051]     grpc epoll fd: 5\u001b[0m\n",
      "\u001b[32m2018-03-10 12:25:20.965164: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job master -> {0 -> algo-1:2222}\u001b[0m\n",
      "\u001b[32m2018-03-10 12:25:20.965195: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> algo-1:2223, 1 -> algo-2:2223}\u001b[0m\n",
      "\u001b[32m2018-03-10 12:25:20.965201: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2222}\u001b[0m\n",
      "\u001b[32m2018-03-10 12:25:20.965317: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job master -> {0 -> algo-1:2222}\u001b[0m\n",
      "\u001b[32m2018-03-10 12:25:20.965337: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> algo-1:2223, 1 -> localhost:2223}\u001b[0m\n",
      "\u001b[32m2018-03-10 12:25:20.965373: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> algo-2:2222}\u001b[0m\n",
      "\u001b[32m2018-03-10 12:25:20.965766: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:2223\u001b[0m\n",
      "\u001b[32m2018-03-10 12:25:20.965997: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:2222\u001b[0m\n",
      "\u001b[31m2018-03-10 12:25:21.581769: I tensorflow/core/distributed_runtime/master_session.cc:1004] Start master session 3b1c32f65755f7d6 with config: gpu_options { per_process_gpu_memory_fraction: 1 } allow_soft_placement: true\u001b[0m\n",
      "\u001b[32mINFO:tensorflow:Create CheckpointSaverHook.\u001b[0m\n",
      "\u001b[32m2018-03-10 12:25:21.427433: I tensorflow/core/distributed_runtime/master_session.cc:1004] Start master session 2c531431a64d4c81 with config: gpu_options { per_process_gpu_memory_fraction: 1 } allow_soft_placement: true\u001b[0m\n",
      "\u001b[32mINFO:tensorflow:Waiting for model to be ready.  Ready_for_local_init_op:  Variables not initialized: global_step, conv2d/kernel, conv2d/bias, conv2d_1/kernel, conv2d_1/bias, dense/kernel, dense/bias, dense_1/kernel, dense_1/bias, beta1_power, beta2_power, conv2d/kernel/Adam, conv2d/kernel/Adam_1, conv2d/bias/Adam, conv2d/bias/Adam_1, conv2d_1/kernel/Adam, conv2d_1/kernel/Adam_1, conv2d_1/bias/Adam, conv2d_1/bias/Adam_1, dense/kernel/Adam, dense/kernel/Adam_1, dense/bias/Adam, dense/bias/Adam_1, dense_1/kernel/Adam, dense_1/kernel/Adam_1, dense_1/bias/Adam, dense_1/bias/Adam_1, ready: None\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Saving checkpoints for 1 into s3://sagemaker-us-east-2-324118574079/sagemaker-tensorflow-py2-cpu-2018-03-10-12-19-50-341/checkpoints/model.ckpt.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mINFO:tensorflow:loss = 2.3031068, step = 0\u001b[0m\n",
      "\u001b[32m2018-03-10 12:25:51.531754: I tensorflow/core/distributed_runtime/master_session.cc:1004] Start master session 9f44844d8f8b80a0 with config: gpu_options { per_process_gpu_memory_fraction: 1 } allow_soft_placement: true\u001b[0m\n",
      "\u001b[32mINFO:tensorflow:loss = 0.14459099, step = 88\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 3.62032\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.06488793, step = 110 (29.435 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 6.96771\u001b[0m\n",
      "\u001b[32mINFO:tensorflow:loss = 0.040450353, step = 281 (28.099 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 6.71306\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.04918912, step = 318 (30.389 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 6.77391\u001b[0m\n",
      "\u001b[32mINFO:tensorflow:loss = 0.05305976, step = 477 (29.033 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 6.73164\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.06597349, step = 522 (30.229 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 6.86542\u001b[0m\n",
      "\u001b[32mINFO:tensorflow:loss = 0.06363681, step = 673 (28.786 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 6.77029\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.022910194, step = 729 (30.415 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 6.68315\u001b[0m\n",
      "\u001b[32mINFO:tensorflow:loss = 0.04944187, step = 866 (28.531 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 6.79526\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.029839283, step = 933 (30.272 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Saving checkpoints for 1002 into s3://sagemaker-us-east-2-324118574079/sagemaker-tensorflow-py2-cpu-2018-03-10-12-19-50-341/checkpoints/model.ckpt.\u001b[0m\n",
      "\u001b[32mINFO:tensorflow:Loss for final step: 0.015442661.\u001b[0m\n",
      "\u001b[32mINFO:tensorflow:writing success training\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Loss for final step: 0.006732045.\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Starting evaluation at 2018-03-10-12:28:10\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Restoring parameters from s3://sagemaker-us-east-2-324118574079/sagemaker-tensorflow-py2-cpu-2018-03-10-12-19-50-341/checkpoints/model.ckpt-1002\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [1/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [2/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [3/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [4/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [5/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [6/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [7/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [8/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [9/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [10/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [11/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [12/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [13/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [14/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [15/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [16/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [17/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [18/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [19/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [20/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [21/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [22/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [23/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [24/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [25/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [26/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [27/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [28/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [29/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [30/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [31/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [32/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [33/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [34/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [35/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [36/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [37/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [38/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [39/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [40/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [41/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [42/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [43/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [44/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [45/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [46/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [47/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [48/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [49/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [50/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [51/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [52/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [53/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [54/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [55/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [56/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [57/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [58/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [59/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [60/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [61/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [62/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [63/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [64/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [65/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [66/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [67/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [68/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [69/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [70/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [71/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [72/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [73/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [74/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [75/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [76/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [77/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [78/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [79/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [80/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [81/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [82/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [83/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [84/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [85/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [86/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [87/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [88/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [89/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [90/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [91/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [92/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [93/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [94/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [95/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [96/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [97/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [98/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [99/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [100/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Finished evaluation at 2018-03-10-12:28:16\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Saving dict for global step 1002: accuracy = 0.9884, global_step = 1002, loss = 0.033372782\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Restoring parameters from s3://sagemaker-us-east-2-324118574079/sagemaker-tensorflow-py2-cpu-2018-03-10-12-19-50-341/checkpoints/model.ckpt-1002\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Assets added to graph.\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:No assets to write.\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:SavedModel written to: s3://sagemaker-us-east-2-324118574079/sagemaker-tensorflow-py2-cpu-2018-03-10-12-19-50-341/checkpoints/export/Servo/temp-1520684897/saved_model.pb\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:writing success training\u001b[0m\n",
      "\u001b[31m2018-03-10 12:28:22,089 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (1): s3.amazonaws.com\u001b[0m\n",
      "\u001b[31m2018-03-10 12:28:22,147 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (1): s3.us-east-2.amazonaws.com\u001b[0m\n",
      "\u001b[31m2018-03-10 12:28:22,213 INFO - tf_container.serve - Downloaded saved model at /opt/ml/model/export/Servo/1520684897/saved_model.pb\u001b[0m\n",
      "\u001b[31m2018-03-10 12:28:22,227 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (2): s3.us-east-2.amazonaws.com\u001b[0m\n",
      "\u001b[32mINFO:tensorflow:master algo-1 is down, stopping parameter server\u001b[0m\n",
      "===== Job Complete =====\n"
     ]
    }
   ],
   "source": [
    "mnist_estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: sagemaker-tensorflow-py2-cpu-2018-03-08-11-07-26-995\n",
      "INFO:sagemaker:Creating endpoint with name sagemaker-tensorflow-py2-cpu-2018-03-08-11-07-26-995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "mnist_predictor = mnist_estimator.deploy(initial_instance_count=1,\n",
    "                                             instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating the model\n",
    "* get some data for testing\n",
    "* call the predictor to compare the labels from test data and the predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Datasets(train=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x7fd9faa09e50>, validation=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x7fd9e3fa5c10>, test=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x7fd9e3fa5510>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "label is 7\n",
      "prediction is 7\n",
      "========================================\n",
      "label is 2\n",
      "prediction is 2\n",
      "========================================\n",
      "label is 1\n",
      "prediction is 1\n",
      "========================================\n",
      "label is 0\n",
      "prediction is 0\n",
      "========================================\n",
      "label is 4\n",
      "prediction is 4\n",
      "========================================\n",
      "label is 1\n",
      "prediction is 1\n",
      "========================================\n",
      "label is 4\n",
      "prediction is 4\n",
      "========================================\n",
      "label is 9\n",
      "prediction is 9\n",
      "========================================\n",
      "label is 5\n",
      "prediction is 5\n",
      "========================================\n",
      "label is 9\n",
      "prediction is 9\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    data = mnist.test.images[i].tolist()\n",
    "    tensor_proto = tf.make_tensor_proto(values=np.asarray(data), shape=[1, len(data)], dtype=tf.float32)\n",
    "    predict_response = mnist_predictor.predict(tensor_proto)\n",
    "    \n",
    "    print(\"========================================\")\n",
    "    label = np.argmax(mnist.test.labels[i])\n",
    "    print(\"label is {}\".format(label))\n",
    "    prediction = predict_response['outputs']['classes']['int64Val'][0]\n",
    "    print(\"prediction is {}\".format(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint with name: sagemaker-tensorflow-py2-cpu-2018-03-08-11-07-26-995\n"
     ]
    }
   ],
   "source": [
    "sagemaker_session.delete_endpoint(mnist_predictor.endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p27",
   "language": "python",
   "name": "conda_tensorflow_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
